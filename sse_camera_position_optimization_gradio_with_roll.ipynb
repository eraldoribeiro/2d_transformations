{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgXQ62E7IruA"
      },
      "source": [
        "# SSE pose fitting using Pytorch3D differential rendering (Render-and-Compare)\n",
        "\n",
        "---\n",
        "## Overview\n",
        "\n",
        "This program calculates the pose of the object with respect to the camera using PyTorch3D differentiable rendering. The method uses the difference between silhouettes as a cost function. It is a simple version of a render-and-compare approach.\n",
        "\n",
        "We assume that the SSE mesh file in `.ply` (or `.obj`) format are present in the directory `assets/`. This directory is created by the code and its files are downloaded from a Dropbox shared link.\n",
        "\n",
        "\n",
        "The output of the demo is a `.gif` showing the (poses) iterations of the optimization.\n",
        "\n",
        "This program was adapted from the example provided here: https://github.com/facebookresearch/pytorch3d/blob/main/docs/tutorials/camera_position_optimization_with_differentiable_rendering.ipynb\n",
        "\n",
        "In this program, we will learn the $[x, y, z]$ position of a camera given a reference image using differentiable rendering.\n",
        "\n",
        "We will first initialize a renderer with a starting position for the camera. We will then use this to generate an image, compute a loss with the reference image, and finally backpropagate through the entire pipeline to update the position of the camera.\n",
        "\n",
        "## PyTorch3D\n",
        "\n",
        "Modules `torch` and `torchvision` are required. If `pytorch3d` is not installed, install it using the following cell. Here, I modified to install PyTorch3D from my own pre-built wheel. Using my own pytorch3d wheel allows for faster installation. Installing from source takes a few minutes to complete.\n",
        "\n",
        "The pre-built PyTorch3D wheel is downloaded from my Dropbox (shared link). Another copy of the wheel is also stored in my Google Drive, and is located at: `/content/drive/MyDrive/research/projects/slosh_project/slosh_project_team_files/Colab_wheels/pytorch3d-0.7.8-cp311-cp311-linux_x86_64.whl`\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "## Main steps\n",
        "\n",
        "\n",
        "### Load the CAD model file\n",
        "\n",
        "We will load a CAD model (i.e., `ply` format) file and create a **Meshes** object. **Meshes** is a unique datastructure provided in PyTorch3D for working with **batches of meshes of different sizes**. It has several useful class methods which are used in the rendering pipeline.\n",
        "\n",
        "### Create a renderer\n",
        "\n",
        "A **renderer** in PyTorch3D is composed of a **rasterizer** and a **shader** which each have a number of subcomponents such as a **camera** (orthographic/perspective). Here, we initialize some of these components and use default values for the rest.\n",
        "\n",
        "For optimizing the camera position we will use a renderer which produces a **silhouette** of the object only and does not apply any **lighting** or **shading**. We will also initialize another renderer which applies full **Phong shading** and use this for visualizing the outputs.\n",
        "\n",
        "### Create a reference image\n",
        "\n",
        "We will first position the teapot and generate an image. We use helper functions to rotate the teapot to a desired viewpoint. Then we can use the renderers to produce an image. Here we will use both renderers and visualize the silhouette and full shaded image.\n",
        "\n",
        "The world coordinate system is defined as +Y up, +X left and +Z in.\n",
        "\n",
        "We defined a camera which is positioned on the positive z axis hence sees the spout to the right.\n",
        "\n",
        "### Initialize the model and optimizer\n",
        "\n",
        "Create an instance of the **model** above and set up an **optimizer** for the camera position parameter.\n",
        "\n",
        "### Run the optimization\n",
        "\n",
        "We run several iterations of the forward and backward pass and save outputs every 10 iterations. When this has finished take a look at `./optimization_sequence.gif` for a cool gif of the optimization process!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHJzmJ7x84FI"
      },
      "source": [
        "# Installation and environment setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to install libraries and dependencies"
      ],
      "metadata": {
        "id": "SneIp1P5LYD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Platform Handling --------------------\n",
        "class PlatformManager:\n",
        "    def __init__(self):\n",
        "        self.platform, self.local_path = self.detect_platform()\n",
        "\n",
        "    @staticmethod\n",
        "    def mount_gdrive():\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_platform() -> tuple[str, str]:\n",
        "        \"\"\"Detect platform and return its name and the local path\"\"\"\n",
        "        import os\n",
        "\n",
        "        computing_platform = 'LocalPC'\n",
        "\n",
        "        if os.getenv('RUNPOD_POD_ID'):\n",
        "            computing_platform = \"RunPod\"\n",
        "            print(\"Running on RunPod.\")\n",
        "            local_path = \"/workspace/\"\n",
        "        elif 'content' in str(os.getcwd()):\n",
        "            computing_platform = \"Colab\"\n",
        "            print(\"Running on Colab.\")\n",
        "            local_path = \"/content/\"\n",
        "        elif os.getenv(\"LIGHTNING_ARTIFACTS_DIR\"):\n",
        "            computing_platform = \"LightningAI\"\n",
        "            print(\"Running on Lightning AI Studio\")\n",
        "            local_path = os.getenv(\"LIGHTNING_ARTIFACTS_DIR\") + '/'\n",
        "        else:\n",
        "            local_path = os.getcwd() + '/'\n",
        "\n",
        "        return computing_platform, local_path\n",
        "\n",
        "\n",
        "# -------------------- Installation Helpers --------------------\n",
        "class DependencyInstaller:\n",
        "    @staticmethod\n",
        "    def install_glut(computing_platform):\n",
        "        !pip install --upgrade pip\n",
        "        if computing_platform == \"LightningAI\":\n",
        "            !sudo apt -qq update\n",
        "            !sudo apt install -y freeglut3-dev libglew-dev libsdl2-dev\n",
        "        elif computing_platform == \"RunPod\":\n",
        "            !apt -qq update\n",
        "            !apt install -y freeglut3-dev libglew-dev libsdl2-dev\n",
        "\n",
        "    @staticmethod\n",
        "    def install_opengl():\n",
        "        import subprocess\n",
        "        import sys\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"PyOpenGL\"])\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"PyOpenGL_accelerate\"])\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Failed to install OpenGL: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def get_pytorch3d_version_string():\n",
        "        import torch, sys\n",
        "        pyt_version_str = torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        version_str = \"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\", \"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        return version_str\n",
        "\n",
        "\n",
        "# -------------------- PyTorch3D Installer --------------------\n",
        "class PyTorch3DInstaller:\n",
        "    def __init__(self, computing_platform, local_path):\n",
        "        self.platform = computing_platform\n",
        "        self.local_path = local_path\n",
        "\n",
        "    def install(self):\n",
        "        import os, sys, torch\n",
        "        need_pytorch3d = False\n",
        "\n",
        "        !pip install --upgrade pip\n",
        "\n",
        "        DependencyInstaller.install_glut(self.platform)\n",
        "        DependencyInstaller.install_opengl()\n",
        "\n",
        "        version_str = DependencyInstaller.get_pytorch3d_version_string()\n",
        "        print(f\"\\nPyTorch3D to be installed: {version_str}\\n\")\n",
        "\n",
        "        %pip install iopath\n",
        "\n",
        "        if sys.platform.startswith(\"linux\"):\n",
        "            print(f\"Trying to install wheel for PyTorch3D. Running on a {self.platform} instance.\")\n",
        "\n",
        "            if self.platform in {\"RunPod\", \"LightningAI\"}:\n",
        "                %pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "\n",
        "            elif self.platform == \"Colab\":\n",
        "                os.chdir(self.local_path)\n",
        "                print(\"Downloading my pre-built PyTorch3D Wheel from Dropbox\")\n",
        "                !wget -O pytorch3d-0.7.8-cp311-cp311-linux_x86_64.whl https://www.dropbox.com/scl/fi/qfv89iszrtwrbkkkvfzzp/pytorch3d-0.7.8-cp311-cp311-linux_x86_64.whl?rlkey=hr1tqcsczgblln2zvtscq4wy0&dl=0\n",
        "                my_pytorch3D_path = self.local_path + \"pytorch3d-0.7.8-cp311-cp311-linux_x86_64.whl\"\n",
        "                %pip install pytorch3d -f $my_pytorch3D_path\n",
        "                print(\"Deleting the wheel file to save space.\")\n",
        "                !rm $my_pytorch3D_path\n",
        "\n",
        "        # Check installation\n",
        "        try:\n",
        "            import pytorch3d\n",
        "        except ImportError:\n",
        "            need_pytorch3d = True\n",
        "\n",
        "        if need_pytorch3d:\n",
        "            print(f\"failed to find/install wheel for {version_str}\")\n",
        "            print(\"Installing PyTorch3D from source\")\n",
        "            %pip install ninja --root-user-action ignore\n",
        "            %pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\n",
        "        else:\n",
        "            try:\n",
        "                import pytorch3d\n",
        "            except:\n",
        "                print(f\"âŒ PyTorch3D failed to install.\")\n",
        "                print(\"ðŸ¤· I don't know what happened.\")\n",
        "            else:\n",
        "                print(f\"âœ… PyTorch3D successfully installed!\")\n"
      ],
      "metadata": {
        "id": "fs66ju2DvOCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required libraries and setup environment"
      ],
      "metadata": {
        "id": "KAsw4yAszzZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect platform\n",
        "platform_mgr = PlatformManager()\n",
        "platform = platform_mgr.platform\n",
        "local_path = platform_mgr.local_path\n",
        "\n",
        "# # Optional: Mount GDrive if on Colab\n",
        "# if platform == \"Colab\":\n",
        "#     platform_mgr.mount_gdrive()\n",
        "\n",
        "# Install PyTorch3D\n",
        "installer = PyTorch3DInstaller(platform, local_path)\n",
        "installer.install()\n",
        "\n",
        "# Image grid vizualization (Meta)\n",
        "import os\n",
        "filename = \"plot_image_grid.py\"\n",
        "url = \"https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\"\n",
        "if not os.path.exists(filename):\n",
        "    !wget {url}\n",
        "\n",
        "\n",
        "!pip -q install trimesh pyrender opencv-python matplotlib pytorch-lightning #==1.8.1\n"
      ],
      "metadata": {
        "id": "Wfi5kbotvS-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main imports"
      ],
      "metadata": {
        "id": "2rpo2u8YEtuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###-------------------------------------------------------------------###\n",
        "#                                Imports\n",
        "###-------------------------------------------------------------------###\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import imageio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import img_as_ubyte\n",
        "import requests\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "\n",
        "# io utils\n",
        "from pytorch3d.io import load_obj, load_ply\n",
        "\n",
        "# Util function for loading meshes\n",
        "from pytorch3d.io import load_objs_as_meshes\n",
        "\n",
        "\n",
        "# datastructures\n",
        "from pytorch3d.structures import Meshes\n",
        "\n",
        "# 3D transformations functions\n",
        "from pytorch3d.transforms import Rotate, Translate\n",
        "\n",
        "from pytorch3d.renderer.cameras import look_at_rotation\n",
        "from pytorch3d.transforms import RotateAxisAngle\n",
        "\n",
        "# rendering components\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras, look_at_view_transform, look_at_rotation,\n",
        "    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
        "    SoftSilhouetteShader, HardPhongShader, PointLights, TexturesVertex,\n",
        "    PerspectiveCameras, camera_position_from_spherical_angles, SoftPhongShader\n",
        ")\n",
        "\n",
        "\n",
        "from plot_image_grid import image_grid\n",
        "\n",
        "\n",
        "from typing import Optional, Literal, Tuple, Dict, Any\n",
        "\n",
        "from PIL import Image, ImageOps"
      ],
      "metadata": {
        "id": "mUP51ZPBDT_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility class"
      ],
      "metadata": {
        "id": "Cf24jtjGmaaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Old utilities"
      ],
      "metadata": {
        "id": "qgrIhxi9FTp0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8dXDbJ284FJ"
      },
      "outputs": [],
      "source": [
        "# #---------------------------- IMPORTS -----------------------------------------\n",
        "# import os\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from tqdm.notebook import tqdm\n",
        "# import imageio\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import matplotlib.pyplot as plt\n",
        "# from skimage import img_as_ubyte\n",
        "# import requests\n",
        "# import shutil\n",
        "# from pathlib import Path\n",
        "\n",
        "# import cv2\n",
        "\n",
        "# # io utils\n",
        "# from pytorch3d.io import load_obj, load_ply\n",
        "\n",
        "# # Util function for loading meshes\n",
        "# from pytorch3d.io import load_objs_as_meshes\n",
        "\n",
        "\n",
        "# # datastructures\n",
        "# from pytorch3d.structures import Meshes\n",
        "\n",
        "# # 3D transformations functions\n",
        "# from pytorch3d.transforms import Rotate, Translate\n",
        "\n",
        "# from pytorch3d.renderer.cameras import look_at_rotation\n",
        "# from pytorch3d.transforms import RotateAxisAngle\n",
        "\n",
        "# # rendering components\n",
        "# from pytorch3d.renderer import (\n",
        "#     FoVPerspectiveCameras, look_at_view_transform, look_at_rotation,\n",
        "#     RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
        "#     SoftSilhouetteShader, HardPhongShader, PointLights, TexturesVertex,\n",
        "#     PerspectiveCameras, camera_position_from_spherical_angles, SoftPhongShader\n",
        "# )\n",
        "\n",
        "\n",
        "# from plot_image_grid import image_grid\n",
        "\n",
        "\n",
        "# from typing import Optional, Literal, Tuple, Dict, Any\n",
        "\n",
        "# from PIL import Image, ImageOps\n",
        "\n",
        "# # #------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# # class SSE_Util:\n",
        "# #     def __init__(self, local_path: str):\n",
        "# #         self.local_path = local_path\n",
        "# #         self.assets_dir = os.path.join(self.local_path, \"assets/\")\n",
        "\n",
        "\n",
        "# #     def create_gif_writer(self, filepath, duration=0.5):\n",
        "# #         return imageio.get_writer(filepath, mode='I', duration=duration)\n",
        "\n",
        "\n",
        "# #     def get_camera_position(self, distance, elevation, azimuth, degrees=True, device=\"cpu\"):\n",
        "# #         camera_pos = camera_position_from_spherical_angles(\n",
        "# #             distance=distance,\n",
        "# #             elevation=elevation,\n",
        "# #             azimuth=azimuth,\n",
        "# #             degrees=degrees\n",
        "# #         )\n",
        "# #         return camera_pos.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# #     def camera_center_to_dist_elev_azim(self, C: torch.Tensor):\n",
        "# #         \"\"\"\n",
        "# #         Convert camera center C (â€¦,3) to (dist, elev_deg, azim_deg) as PyTorch3D expects.\n",
        "# #         Convention: Y is up; azim=0 points along +Z; positive azim rotates toward +X.\n",
        "# #         Returns tensors shaped like C[...,0].\n",
        "# #         \"\"\"\n",
        "# #         if C.ndim == 1:\n",
        "# #             C = C[None, :]  # (1,3)\n",
        "# #             squeeze = True\n",
        "# #         else:\n",
        "# #             squeeze = False\n",
        "\n",
        "# #         x, y, z = C[..., 0], C[..., 1], C[..., 2]\n",
        "# #         dist = torch.linalg.norm(C, dim=-1)\n",
        "\n",
        "# #         # avoid divide-by-zero at the poles/origin\n",
        "# #         rho = torch.sqrt(torch.clamp(x*x + z*z, min=1e-12))\n",
        "\n",
        "# #         elev = torch.rad2deg(torch.atan2(y, rho))     # [-90, 90]\n",
        "# #         azim = torch.rad2deg(torch.atan2(x, z))       # (-180, 180]\n",
        "\n",
        "# #         if squeeze:\n",
        "# #             dist, elev, azim = dist[0], elev[0], azim[0]\n",
        "# #         return dist, elev, azim\n",
        "\n",
        "# #     # Possible fix\n",
        "# #     # def add_camera_roll_to_RT(self, R, T, roll_deg, device=None, mode=\"world\"):\n",
        "# #     #     \"\"\"\n",
        "# #     #     Compose a Z-axis roll into (R, T) and keep the same camera center C.\n",
        "# #     #     Grad-safe: preserves autograd paths from both R/T and roll_deg.\n",
        "# #     #     \"\"\"\n",
        "# #     #     import torch\n",
        "\n",
        "# #     #     # tensors / shapes (preserve graph)\n",
        "# #     #     if not torch.is_tensor(R): R = torch.as_tensor(R)\n",
        "# #     #     if not torch.is_tensor(T): T = torch.as_tensor(T)\n",
        "# #     #     dev   = device or R.device\n",
        "# #     #     dtype = torch.float32\n",
        "\n",
        "# #     #     R = R.to(dev, dtype)\n",
        "# #     #     T = T.to(dev, dtype)\n",
        "\n",
        "# #     #     unbatched = (R.ndim == 2)\n",
        "# #     #     if unbatched:\n",
        "# #     #         R = R[None, ...]   # (1,3,3)\n",
        "# #     #         T = T[None, ...]   # (1,3)\n",
        "\n",
        "# #     #     # camera center C from (R,T):  T = -R^T C  =>  C = -R @ T\n",
        "# #     #     C = -torch.matmul(R, T[..., None]).squeeze(-1)  # (B,3)\n",
        "\n",
        "# #     #     # roll matrix about camera Z  (NO float(...), NO torch.tensor(...))\n",
        "# #     #     theta = torch.as_tensor(roll_deg, dtype=dtype, device=dev).reshape(1)  # keeps requires_grad if Parameter\n",
        "# #     #     c = torch.cos(torch.deg2rad(theta))\n",
        "# #     #     s = torch.sin(torch.deg2rad(theta))\n",
        "\n",
        "# #     #     # constants tied to same dtype/device\n",
        "# #     #     z = torch.zeros_like(c)\n",
        "# #     #     o = torch.ones_like(c)\n",
        "\n",
        "# #     #     Rz = torch.stack([\n",
        "# #     #         torch.stack([ c, -s, z], dim=-1),\n",
        "# #     #         torch.stack([ s,  c, z], dim=-1),\n",
        "# #     #         torch.stack([ z,  z,  o], dim=-1),\n",
        "# #     #     ], dim=1)  # (1,3,3)\n",
        "\n",
        "# #     #     # compose\n",
        "# #     #     if mode == \"camera\":\n",
        "# #     #         R_new = torch.matmul(Rz, R)      # roll in camera coords\n",
        "# #     #     elif mode == \"world\":\n",
        "# #     #         R_new = torch.matmul(R, Rz)      # roll in world coords\n",
        "# #     #     else:\n",
        "# #     #         raise ValueError(\"mode must be 'camera' or 'world'\")\n",
        "\n",
        "# #     #     # keep the same camera center\n",
        "# #     #     T_new = -torch.matmul(R_new.transpose(1, 2), C[..., None]).squeeze(-1)  # (B,3)\n",
        "\n",
        "# #     #     if unbatched:\n",
        "# #     #         R_new, T_new = R_new[0], T_new[0]\n",
        "# #     #     return R_new, T_new\n",
        "\n",
        "\n",
        "\n",
        "# #     def add_camera_roll_to_RT(self, R, T, roll_deg, device=None, mode=\"camera\"):\n",
        "# #         \"\"\"\n",
        "# #         Compose a Z-axis roll into (R, T) and keep the same camera center C.\n",
        "\n",
        "# #         Args\n",
        "# #           R: (3,3) or (1,3,3) world->cam rotation\n",
        "# #           T: (3,)  or (1,3)   world->cam translation (PyTorch3D: X_cam = R @ X_world + T)\n",
        "# #           roll_deg: float degrees\n",
        "# #           mode: \"camera\" -> R' = Rz @ R   (roll in camera frame)\n",
        "# #                 \"world\"  -> R' = R  @ Rz  (roll in world frame)\n",
        "\n",
        "# #         Returns\n",
        "# #           R_new, T_new with the same rank as inputs\n",
        "# #         \"\"\"\n",
        "# #         # tensors / shapes\n",
        "# #         if not torch.is_tensor(R): R = torch.as_tensor(R)\n",
        "# #         if not torch.is_tensor(T): T = torch.as_tensor(T)\n",
        "# #         dev   = device or R.device\n",
        "# #         dtype = torch.float32\n",
        "# #         R = R.to(dev, dtype)\n",
        "# #         T = T.to(dev, dtype)\n",
        "\n",
        "# #         unbatched = (R.ndim == 2)\n",
        "# #         if unbatched:\n",
        "# #             R = R[None, ...]   # (1,3,3)\n",
        "# #             T = T[None, ...]   # (1,3)\n",
        "\n",
        "# #         # camera center C from (R,T):  T = -R^T C  =>  C = -R @ T\n",
        "# #         C = -torch.matmul(R, T[..., None]).squeeze(-1)  # (B,3)\n",
        "\n",
        "# #         # roll matrix about camera Z\n",
        "# #         theta = torch.tensor(float(roll_deg), dtype=dtype, device=dev)\n",
        "# #         c, s = torch.cos(torch.deg2rad(theta)), torch.sin(torch.deg2rad(theta))\n",
        "# #         z = torch.tensor(0.0, dtype=dtype, device=dev)\n",
        "# #         o = torch.tensor(1.0, dtype=dtype, device=dev)\n",
        "# #         Rz = torch.stack([\n",
        "# #             torch.stack([ c, -s, z]),\n",
        "# #             torch.stack([ s,  c, z]),\n",
        "# #             torch.stack([ z,  z,  o]),\n",
        "# #         ], dim=0).expand(R.shape[0], -1, -1)  # (B,3,3)\n",
        "\n",
        "# #         # compose\n",
        "# #         if mode == \"camera\":\n",
        "# #             R_new = torch.matmul(Rz, R)      # roll in camera coords\n",
        "# #         elif mode == \"world\":\n",
        "# #             R_new = torch.matmul(R, Rz)      # roll in world coords\n",
        "# #         else:\n",
        "# #             raise ValueError(\"mode must be 'camera' or 'world'\")\n",
        "\n",
        "# #         # keep the same camera center\n",
        "# #         T_new = -torch.matmul(R_new.transpose(1, 2), C[..., None]).squeeze(-1)  # (B,3)\n",
        "\n",
        "# #         if unbatched:\n",
        "# #             R_new, T_new = R_new[0], T_new[0]\n",
        "# #         return R_new, T_new\n",
        "\n",
        "\n",
        "# #     def read_rgb_cutout_black_bg(self, path: str) -> np.ndarray:\n",
        "# #         \"\"\"\n",
        "# #         Read an image file as an RGB cutout composited on a BLACK background.\n",
        "\n",
        "# #         Args:\n",
        "# #             path: Path to the image (PNG/JPEG/etc.). Alpha is respected if present.\n",
        "\n",
        "# #         Returns:\n",
        "# #             np.ndarray of shape (H, W, 3).\n",
        "# #         \"\"\"\n",
        "# #         # Load and fix EXIF orientation, ensure RGBA so we have an alpha channel to composite\n",
        "# #         img = Image.open(path)\n",
        "# #         img = ImageOps.exif_transpose(img).convert(\"RGBA\")\n",
        "\n",
        "# #         rgba = np.asarray(img).astype(np.float32)  # (H,W,4), 0..255\n",
        "# #         rgb  = rgba[..., :3]\n",
        "# #         a    = rgba[..., 3:4] / 255.0             # (H,W,1) in [0,1]\n",
        "\n",
        "# #         # Composite on BLACK: out = rgb * alpha + black * (1 - alpha) == rgb * alpha\n",
        "# #         rgb_black = rgb * a                        # still 0..255 range (float)\n",
        "\n",
        "# #         return np.clip(rgb_black, 0, 255).astype(np.uint8)  # (H,W,3), uint8\n",
        "\n",
        "\n",
        "# #     def center_cutout_rgb_uint8(self, img: np.ndarray, black_thresh: int = 5) -> np.ndarray:\n",
        "# #         \"\"\"\n",
        "# #         Center an RGB cutout (black background) on a same-size black canvas.\n",
        "# #         Input / Output: (H, W, 3) uint8 [0..255]. No scaling.\n",
        "\n",
        "# #         black_thresh: pixels with any channel > black_thresh are treated as foreground.\n",
        "# #         \"\"\"\n",
        "# #         if img.ndim != 3 or img.shape[2] != 3 or img.dtype != np.uint8:\n",
        "# #             raise ValueError(\"Expected (H, W, 3) uint8 input.\")\n",
        "\n",
        "# #         H, W, _ = img.shape\n",
        "# #         # Foreground mask: \"non-black\"\n",
        "# #         mask = (img.max(axis=2) > black_thresh)\n",
        "# #         if not mask.any():\n",
        "# #             return img.copy()  # nothing to center\n",
        "\n",
        "# #         ys, xs = np.where(mask)\n",
        "# #         y0, y1 = int(ys.min()), int(ys.max()) + 1\n",
        "# #         x0, x1 = int(xs.min()), int(xs.max()) + 1\n",
        "\n",
        "# #         roi = img[y0:y1, x0:x1, :]        # (h, w, 3)\n",
        "# #         h, w = roi.shape[:2]\n",
        "\n",
        "# #         # Top-left to center\n",
        "# #         sy = (H - h) // 2\n",
        "# #         sx = (W - w) // 2\n",
        "\n",
        "# #         out = np.zeros_like(img)          # black canvas\n",
        "# #         out[sy:sy+h, sx:sx+w, :] = roi\n",
        "# #         return out\n",
        "\n",
        "# #     import numpy as np\n",
        "\n",
        "# #     def crop_center_to_size_uint8(self, img: np.ndarray, out_size: tuple[int, int], fill_color=(0,0,0)) -> np.ndarray:\n",
        "# #         \"\"\"\n",
        "# #         Crop a centered window of size (W_out, H_out) from an RGB cutout (black background),\n",
        "# #         preserving scale & centering. No resizing. Pads with black if the crop exceeds bounds.\n",
        "\n",
        "# #         Args:\n",
        "# #             img: (H, W, 3) uint8.\n",
        "# #             out_size: (W_out, H_out) desired output size.\n",
        "# #             fill_color: RGB tuple for padding if needed (default black).\n",
        "\n",
        "# #         Returns:\n",
        "# #             (H_out, W_out, 3) uint8.\n",
        "# #         \"\"\"\n",
        "# #         if img.ndim != 3 or img.shape[2] != 3 or img.dtype != np.uint8:\n",
        "# #             raise ValueError(\"Expected (H, W, 3) uint8 input.\")\n",
        "\n",
        "# #         W_out, H_out = map(int, out_size)\n",
        "# #         H, W = img.shape[:2]\n",
        "\n",
        "# #         # Centered crop box in source image\n",
        "# #         left   = (W - W_out) // 2\n",
        "# #         top    = (H - H_out) // 2\n",
        "# #         right  = left + W_out\n",
        "# #         bottom = top + H_out\n",
        "\n",
        "# #         # Compute overlap with source (clip to bounds)\n",
        "# #         src_x0 = max(0, left)\n",
        "# #         src_y0 = max(0, top)\n",
        "# #         src_x1 = min(W, right)\n",
        "# #         src_y1 = min(H, bottom)\n",
        "\n",
        "# #         # Destination positions (where to paste inside output canvas)\n",
        "# #         dst_x0 = max(0, -left)\n",
        "# #         dst_y0 = max(0, -top)\n",
        "# #         dst_x1 = dst_x0 + (src_x1 - src_x0)\n",
        "# #         dst_y1 = dst_y0 + (src_y1 - src_y0)\n",
        "\n",
        "# #         # Prepare output canvas\n",
        "# #         out = np.empty((H_out, W_out, 3), dtype=np.uint8)\n",
        "# #         out[...] = np.asarray(fill_color, dtype=np.uint8)\n",
        "\n",
        "# #         # Paste the overlapping region\n",
        "# #         if src_x1 > src_x0 and src_y1 > src_y0:\n",
        "# #             out[dst_y0:dst_y1, dst_x0:dst_x1] = img[src_y0:src_y1, src_x0:src_x1]\n",
        "\n",
        "# #         return out\n",
        "\n",
        "\n",
        "# #     import numpy as np\n",
        "# #     import cv2\n",
        "\n",
        "# #     def add_alpha_from_black_bg_uint8(self,\n",
        "# #         img_rgb: np.ndarray,\n",
        "# #         *,\n",
        "# #         black_thresh: int = 5,     # pixel is FG if any channel > black_thresh\n",
        "# #         mode: str = \"binary\",      # \"binary\" or \"soft\"\n",
        "# #         close_kernel: int = 3      # 0/1 to disable; else use 3,5,7...\n",
        "# #     ) -> np.ndarray:\n",
        "# #         \"\"\"\n",
        "# #         Add alpha to an RGB cutout (black background), closing holes in the mask.\n",
        "\n",
        "# #         Input:  img_rgb (H, W, 3) uint8\n",
        "# #         Output: rgba     (H, W, 4) uint8\n",
        "# #         \"\"\"\n",
        "# #         if img_rgb.ndim != 3 or img_rgb.shape[2] != 3 or img_rgb.dtype != np.uint8:\n",
        "# #             raise ValueError(\"Expected (H, W, 3) uint8.\")\n",
        "\n",
        "# #         # 1) Initial mask (0/255)\n",
        "# #         mask = (img_rgb.max(axis=2) > black_thresh).astype(np.uint8) * 255  # (H,W)\n",
        "\n",
        "# #         # 2) Fill holes (classic OpenCV recipe)\n",
        "# #         im_flood = mask.copy()\n",
        "# #         h, w = mask.shape\n",
        "# #         ff_mask = np.zeros((h + 2, w + 2), np.uint8)\n",
        "# #         cv2.floodFill(im_flood, ff_mask, (0, 0), 255)       # fill background from (0,0)\n",
        "# #         holes = cv2.bitwise_not(im_flood)                   # holes are 255\n",
        "# #         mask_filled = cv2.bitwise_or(mask, holes)           # FG + holes\n",
        "\n",
        "# #         # 3) Optional morphological closing to seal tiny gaps\n",
        "# #         if close_kernel and close_kernel > 1:\n",
        "# #             k = np.ones((close_kernel, close_kernel), np.uint8)\n",
        "# #             mask_filled = cv2.morphologyEx(mask_filled, cv2.MORPH_CLOSE, k)\n",
        "\n",
        "# #         # 4) Alpha channel\n",
        "# #         if mode == \"binary\":\n",
        "# #             alpha = mask_filled\n",
        "# #         elif mode == \"soft\":\n",
        "# #             # Keep soft edges from brightness, but force filled mask as minimum\n",
        "# #             alpha_soft = img_rgb.max(axis=2)                # 0..255\n",
        "# #             alpha = np.maximum(alpha_soft, mask_filled).astype(np.uint8)\n",
        "# #         else:\n",
        "# #             raise ValueError(\"mode must be 'binary' or 'soft'\")\n",
        "\n",
        "# #         # 5) Assemble RGBA\n",
        "# #         rgba = np.concatenate([img_rgb, alpha[..., None]], axis=2)\n",
        "# #         return rgba\n",
        "\n",
        "# #     import numpy as np\n",
        "\n",
        "# #     def to_float_batched_rgba_white_bg_preserve_alpha(self,\n",
        "# #         img: np.ndarray,\n",
        "# #         *,\n",
        "# #         derive_alpha: str = \"soft\",  # \"soft\" (max channel) or \"binary\"\n",
        "# #         black_thresh: int = 5        # for \"binary\" when deriving alpha from RGB uint8\n",
        "# #     ) -> np.ndarray:\n",
        "# #         \"\"\"\n",
        "# #         Convert (H,W,3/4) or (1,H,W,3/4) to (1,H,W,4) float32 in [0,1].\n",
        "# #         - RGB is composited over WHITE: rgb' = rgb * a + (1 - a) * 1\n",
        "# #         - Alpha channel 'a' is PRESERVED (or derived if missing).\n",
        "\n",
        "# #         If input has no alpha:\n",
        "# #           - soft: a = max(rgb) (normalized to [0,1])\n",
        "# #           - binary: a = 1 where any channel > black_thresh (uint8) else 0\n",
        "# #         \"\"\"\n",
        "# #         arr = np.asarray(img)\n",
        "\n",
        "# #         # Accept optional batch dim of 1\n",
        "# #         if arr.ndim == 4 and arr.shape[0] == 1:\n",
        "# #             arr = arr[0]\n",
        "# #         if arr.ndim != 3 or arr.shape[-1] not in (3, 4):\n",
        "# #             raise ValueError(\"Expected (H,W,3) or (H,W,4) (optionally with leading batch of 1).\")\n",
        "\n",
        "# #         # Normalize to float32 in [0,1]\n",
        "# #         arr = arr.astype(np.float32, copy=False)\n",
        "# #         if np.nanmax(arr) > 1.0 + 1e-6:\n",
        "# #             arr = np.clip(arr, 0, 255) / 255.0\n",
        "\n",
        "# #         H, W, C = arr.shape\n",
        "# #         if C == 4:\n",
        "# #             rgb = arr[..., :3]\n",
        "# #             a   = np.clip(arr[..., 3], 0.0, 1.0)\n",
        "# #         else:\n",
        "# #             rgb = arr\n",
        "# #             if derive_alpha == \"soft\":\n",
        "# #                 a = np.clip(rgb.max(axis=-1), 0.0, 1.0)            # soft mask from brightness\n",
        "# #             elif derive_alpha == \"binary\":\n",
        "# #                 # original likely uint8->float; emulate threshold in 0..1\n",
        "# #                 thr = black_thresh / 255.0\n",
        "# #                 a = (rgb.max(axis=-1) > thr).astype(np.float32)\n",
        "# #             else:\n",
        "# #                 raise ValueError(\"derive_alpha must be 'soft' or 'binary'\")\n",
        "\n",
        "# #         # Composite RGB over WHITE but keep alpha as the mask\n",
        "# #         a3 = a[..., None]                          # (H,W,1)\n",
        "# #         rgb_on_white = rgb * a3 + (1.0 - a3) * 1.0 # white background appearance\n",
        "\n",
        "# #         rgba_out = np.concatenate([rgb_on_white, a3], axis=-1).astype(np.float32)  # (H,W,4)\n",
        "# #         return rgba_out[None, ...]  # (1,H,W,4)\n",
        "\n",
        "\n",
        "# #     def center_mask_keep_scale(self,\n",
        "# #         mask_image: np.ndarray,\n",
        "# #         output_size=None,          # (W, H); if None, keep original canvas size\n",
        "# #         pad: int = 0,              # padding inside the canvas (center within the inner area)\n",
        "# #         threshold: int = 127,      # binarization threshold (0..255)\n",
        "# #         overflow: str = \"crop\",    # 'crop' | 'error' | 'shrink'\n",
        "# #         return_transform: bool = False\n",
        "# #     ):\n",
        "# #         \"\"\"\n",
        "# #         Center the largest region of a binary mask on a canvas, preserving original scale.\n",
        "\n",
        "# #         Args:\n",
        "# #             mask_image: 2D numpy array (grayscale/binary).\n",
        "# #             output_size: (W, H) of output canvas; if None, use input image size.\n",
        "# #             pad: pixels of padding from each border; the mask is centered in the inner area.\n",
        "# #             threshold: threshold to binarize if needed.\n",
        "# #             overflow:\n",
        "# #                 - 'crop'  : keep scale and crop parts that would fall outside canvas\n",
        "# #                 - 'error' : raise if the mask doesn't fit\n",
        "# #                 - 'shrink': ONLY if needed, uniformly shrink to fit (keeps aspect ratio)\n",
        "# #             return_transform: if True, returns a dict with offset/scale/bbox info.\n",
        "\n",
        "# #         Returns:\n",
        "# #             binary_mask: (H,W) uint8 {0,255}\n",
        "# #             centered   : (H_out,W_out) uint8 {0,255}\n",
        "# #             (optional) info: {'scale', 'bbox', 'offset', 'pasted_box', 'source_box'}\n",
        "# #         \"\"\"\n",
        "# #         if mask_image is None or mask_image.ndim != 2:\n",
        "# #             raise ValueError(\"Input must be a 2D mask image.\")\n",
        "\n",
        "# #         # Normalize to uint8\n",
        "# #         img = mask_image\n",
        "# #         if img.dtype != np.uint8:\n",
        "# #             if np.nanmax(img) <= 1.0:\n",
        "# #                 img = (np.clip(img, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
        "# #             else:\n",
        "# #                 img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "# #         # Binarize\n",
        "# #         _, binary_mask = cv2.threshold(img, threshold, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# #         # Find largest contour\n",
        "# #         contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "# #         if not contours:\n",
        "# #             # No foregroundâ€”return empty canvas\n",
        "# #             if output_size is None:\n",
        "# #                 out_w, out_h = binary_mask.shape[1], binary_mask.shape[0]\n",
        "# #             else:\n",
        "# #                 out_w, out_h = int(output_size[0]), int(output_size[1])\n",
        "# #             centered = np.zeros((out_h, out_w), dtype=np.uint8)\n",
        "# #             info = {'scale': 1.0, 'bbox': (0,0,0,0), 'offset': (0,0),\n",
        "# #                     'pasted_box': (0,0,0,0), 'source_box': (0,0,0,0)}\n",
        "# #             return (binary_mask, centered, info) if return_transform else (binary_mask, centered)\n",
        "\n",
        "# #         # Bounding box of largest region (original scale)\n",
        "# #         largest = max(contours, key=cv2.contourArea)\n",
        "# #         x, y, w, h = cv2.boundingRect(largest)\n",
        "# #         roi = binary_mask[y:y+h, x:x+w]  # 0/255\n",
        "\n",
        "# #         # Output canvas\n",
        "# #         if output_size is None:\n",
        "# #             out_w, out_h = binary_mask.shape[1], binary_mask.shape[0]\n",
        "# #         else:\n",
        "# #             out_w, out_h = int(output_size[0]), int(output_size[1])\n",
        "\n",
        "# #         pad = max(0, int(pad))\n",
        "# #         inner_w = max(1, out_w - 2*pad)\n",
        "# #         inner_h = max(1, out_h - 2*pad)\n",
        "\n",
        "# #         # Decide if it fits at original scale\n",
        "# #         fits = (w <= inner_w) and (h <= inner_h)\n",
        "\n",
        "# #         scale = 1.0\n",
        "# #         roi_to_paste = roi\n",
        "\n",
        "# #         if not fits:\n",
        "# #             if overflow == \"error\":\n",
        "# #                 raise ValueError(\n",
        "# #                     f\"Mask bbox ({w}x{h}) does not fit into inner area ({inner_w}x{inner_h}).\"\n",
        "# #                 )\n",
        "# #             elif overflow == \"shrink\":\n",
        "# #                 # Shrink uniformly just enough to fit (still \"preserve\" aspect ratio, but not the exact size)\n",
        "# #                 scale = min(inner_w / w, inner_h / h)\n",
        "# #                 new_w = max(1, int(round(w * scale)))\n",
        "# #                 new_h = max(1, int(round(h * scale)))\n",
        "# #                 roi_to_paste = cv2.resize(roi, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
        "# #                 w, h = new_w, new_h\n",
        "# #             elif overflow == \"crop\":\n",
        "# #                 # Keep original scale; we'll crop at paste time\n",
        "# #                 pass\n",
        "# #             else:\n",
        "# #                 raise ValueError(\"overflow must be one of: 'crop', 'error', 'shrink'\")\n",
        "\n",
        "# #         # Center within inner area\n",
        "# #         start_x = pad + (inner_w - w) // 2\n",
        "# #         start_y = pad + (inner_h - h) // 2\n",
        "\n",
        "# #         centered = np.zeros((out_h, out_w), dtype=np.uint8)\n",
        "\n",
        "# #         # Compute destination box and crop if needed (for 'crop' overflow)\n",
        "# #         dst_x0 = max(0, start_x)\n",
        "# #         dst_y0 = max(0, start_y)\n",
        "# #         dst_x1 = min(out_w, start_x + w)\n",
        "# #         dst_y1 = min(out_h, start_y + h)\n",
        "\n",
        "# #         # Corresponding source crop\n",
        "# #         src_x0 = max(0, -start_x)\n",
        "# #         src_y0 = max(0, -start_y)\n",
        "# #         src_x1 = src_x0 + max(0, dst_x1 - dst_x0)\n",
        "# #         src_y1 = src_y0 + max(0, dst_y1 - dst_y0)\n",
        "\n",
        "# #         # Paste if there is an overlap\n",
        "# #         if dst_x1 > dst_x0 and dst_y1 > dst_y0:\n",
        "# #             centered[dst_y0:dst_y1, dst_x0:dst_x1] = roi_to_paste[src_y0:src_y1, src_x0:src_x1]\n",
        "\n",
        "# #         if return_transform:\n",
        "# #             info = {\n",
        "# #                 'scale': scale,           # 1.0 if not shrunk\n",
        "# #                 'bbox': (x, y, roi.shape[1], roi.shape[0]),  # original bbox size (w,h) before any shrink\n",
        "# #                 'offset': (start_x, start_y),                # where top-left would be without cropping\n",
        "# #                 'pasted_box': (dst_x0, dst_y0, dst_x1 - dst_x0, dst_y1 - dst_y0),\n",
        "# #                 'source_box': (src_x0, src_y0, src_x1 - src_x0, src_y1 - src_y0),\n",
        "# #             }\n",
        "# #             return binary_mask, centered, info\n",
        "# #         else:\n",
        "# #             return binary_mask, centered\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #     def center_mask_preserve_ratio(self,\n",
        "# #         mask_image: np.ndarray,\n",
        "# #         output_size=(256, 256),      # (width, height); if None, use original size\n",
        "# #         pad: int = 0,                # optional padding (pixels) around the mask on the canvas\n",
        "# #         allow_upscale: bool = True,  # if False, only shrink; don't enlarge small masks\n",
        "# #         threshold: int = 127,        # binarization threshold (0..255)\n",
        "# #         return_transform: bool = False\n",
        "# #     ):\n",
        "# #         \"\"\"\n",
        "# #         Center the main (largest) region of a binary mask onto a new canvas,\n",
        "# #         scaling uniformly to fit while preserving aspect ratio.\n",
        "\n",
        "# #         Args:\n",
        "# #             mask_image: 2D numpy array (grayscale or binary).\n",
        "# #             output_size: (W, H). If None, uses original image size.\n",
        "# #             pad: padding (pixels) to keep around the scaled mask (applied on all sides).\n",
        "# #             allow_upscale: if False, the mask won't be enlarged beyond its original size.\n",
        "# #             threshold: threshold for binarization if input isn't already {0,255}.\n",
        "# #             return_transform: if True, also returns a dict with scale/offset/bbox.\n",
        "\n",
        "# #         Returns:\n",
        "# #             binary_mask: (H,W) uint8 in {0,255} â€” thresholded version of the input.\n",
        "# #             centered_image: (H_out,W_out) uint8 in {0,255} â€” centered on the new canvas.\n",
        "# #             (optional) info: dict with 'scale', 'bbox'=(x,y,w,h), 'offset'=(start_x,start_y)\n",
        "# #         \"\"\"\n",
        "# #         if mask_image is None or mask_image.ndim != 2:\n",
        "# #             raise ValueError(\"Input must be a 2D mask image.\")\n",
        "\n",
        "# #         # Normalize dtype/range to uint8\n",
        "# #         img = mask_image\n",
        "# #         if img.dtype != np.uint8:\n",
        "# #             if np.nanmax(img) <= 1.0:\n",
        "# #                 img = (np.clip(img, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
        "# #             else:\n",
        "# #                 img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "# #         # Binarize (ensure {0,255})\n",
        "# #         _, binary_mask = cv2.threshold(img, threshold, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# #         # Find largest contour\n",
        "# #         contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "# #         if not contours:\n",
        "# #             # No foreground; return empty canvas\n",
        "# #             if output_size is None:\n",
        "# #                 output_size = (binary_mask.shape[1], binary_mask.shape[0])\n",
        "# #             centered = np.zeros((output_size[1], output_size[0]), dtype=np.uint8)\n",
        "# #             return (binary_mask, centered, {'scale': 0.0, 'bbox': (0,0,0,0), 'offset': (0,0)}) if return_transform else (binary_mask, centered)\n",
        "\n",
        "# #         largest = max(contours, key=cv2.contourArea)\n",
        "# #         x, y, w, h = cv2.boundingRect(largest)\n",
        "# #         roi = binary_mask[y:y+h, x:x+w]  # values are 0 or 255\n",
        "\n",
        "# #         # Output canvas size\n",
        "# #         if output_size is None:\n",
        "# #             out_w, out_h = binary_mask.shape[1], binary_mask.shape[0]\n",
        "# #         else:\n",
        "# #             out_w, out_h = int(output_size[0]), int(output_size[1])\n",
        "\n",
        "# #         # Effective area after padding\n",
        "# #         pad = max(0, int(pad))\n",
        "# #         eff_w = max(1, out_w - 2*pad)\n",
        "# #         eff_h = max(1, out_h - 2*pad)\n",
        "\n",
        "# #         # Uniform scale to fit while preserving aspect ratio\n",
        "# #         scale = min(eff_w / max(1, w), eff_h / max(1, h))\n",
        "# #         if not allow_upscale:\n",
        "# #             scale = min(1.0, scale)\n",
        "\n",
        "# #         new_w = max(1, int(round(w * scale)))\n",
        "# #         new_h = max(1, int(round(h * scale)))\n",
        "\n",
        "# #         # Resize with nearest to keep binary values intact\n",
        "# #         roi_resized = cv2.resize(roi, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "# #         # Create centered canvas\n",
        "# #         centered = np.zeros((out_h, out_w), dtype=np.uint8)\n",
        "# #         start_x = (out_w - new_w) // 2\n",
        "# #         start_y = (out_h - new_h) // 2\n",
        "\n",
        "# #         # Paste\n",
        "# #         centered[start_y:start_y+new_h, start_x:start_x+new_w] = roi_resized\n",
        "\n",
        "# #         if return_transform:\n",
        "# #             info = {'scale': scale, 'bbox': (x, y, w, h), 'offset': (start_x, start_y)}\n",
        "# #             return 255 - binary_mask, 255 - centered, info\n",
        "# #         else:\n",
        "# #             return 255 - binary_mask, 255 - centered\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #     def center_mask_in_image_from_array(self, mask_image: np.ndarray, output_size=None):\n",
        "# #         \"\"\"\n",
        "# #         Centers the masked region of a binary mask image.\n",
        "\n",
        "# #         Args:\n",
        "# #             mask_image (np.ndarray): Input binary mask as a NumPy array (single-channel).\n",
        "# #             output_size (tuple or None): Desired output size as (width, height).\n",
        "# #                                         If None, uses original image size.\n",
        "\n",
        "# #         Returns:\n",
        "# #             original_mask (np.ndarray): Thresholded binary mask.\n",
        "# #             centered_image (np.ndarray): New image with mask centered.\n",
        "# #         \"\"\"\n",
        "# #         if mask_image is None or len(mask_image.shape) != 2:\n",
        "# #             raise ValueError(\"Input must be a 2D binary mask image (grayscale).\")\n",
        "\n",
        "# #         if mask_image.dtype != np.uint8:\n",
        "# #             mask_image = (mask_image * 255 if mask_image.max() <= 1.0 else mask_image).astype(np.uint8)\n",
        "\n",
        "# #         # Ensure it's binary\n",
        "# #         _, binary_mask = cv2.threshold(mask_image, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# #         # Find contours and bounding box\n",
        "# #         contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "# #         if not contours:\n",
        "# #             raise ValueError(\"No masked region found in the image.\")\n",
        "\n",
        "# #         # Bounding box of the largest contour\n",
        "# #         largest_contour = max(contours, key=cv2.contourArea)\n",
        "# #         x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "\n",
        "# #         # Crop the region of interest\n",
        "# #         cropped = binary_mask[y:y+h, x:x+w]\n",
        "\n",
        "# #         # Define output canvas\n",
        "# #         if output_size is None:\n",
        "# #             output_size = binary_mask.shape[::-1]  # (width, height)\n",
        "\n",
        "# #         centered_image = np.zeros((output_size[1], output_size[0]), dtype=np.uint8)\n",
        "\n",
        "# #         # Center coordinates\n",
        "# #         center_x, center_y = output_size[0] // 2, output_size[1] // 2\n",
        "# #         start_x, start_y = center_x - w // 2, center_y - h // 2\n",
        "\n",
        "# #         # Place cropped mask into center of canvas\n",
        "# #         centered_image[start_y:start_y+h, start_x:start_x+w] = cropped\n",
        "\n",
        "# #         return binary_mask, centered_image\n",
        "\n",
        "\n",
        "# #     # import numpy as np\n",
        "# #     # import cv2\n",
        "# #     # from typing import Optional, Tuple, Literal, Dict, Any\n",
        "\n",
        "# #     def center_cutout_keep_scale_2(\n",
        "# #         self,\n",
        "# #         cutout_rgba: np.ndarray,\n",
        "# #         output_size: Optional[Tuple[int, int]] = None,  # (W_out, H_out). If None, keep original size\n",
        "# #         pad: int = 0,\n",
        "# #         overflow: Literal[\"crop\", \"error\", \"shrink\"] = \"crop\",\n",
        "# #         alpha_threshold: int = 10,\n",
        "# #         white_thresh: int = 245,\n",
        "# #         return_transform: bool = False\n",
        "# #     ) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:\n",
        "# #         \"\"\"\n",
        "# #         Re-center an RGBA cutout on a WHITE canvas, preserving scale.\n",
        "# #         Also creates a centered RGBA silhouette (white FG, alpha=mask, transparent BG).\n",
        "# #         Returns (centered_rgba, centered_silhouette_rgba, [info]) with shapes (1,H,W,4).\n",
        "# #         \"\"\"\n",
        "# #         if cutout_rgba is None:\n",
        "# #             raise ValueError(\"cutout_rgba is None\")\n",
        "\n",
        "# #         # Accept (H,W,4) or (1,H,W,4); ensure uint8 0..255\n",
        "# #         img = cutout_rgba[0] if (cutout_rgba.ndim == 4 and cutout_rgba.shape[0] == 1) else cutout_rgba\n",
        "# #         if img.ndim != 3 or img.shape[2] != 4:\n",
        "# #             raise ValueError(\"cutout_rgba must have shape (H,W,4) or (1,H,W,4).\")\n",
        "\n",
        "# #         if img.dtype != np.uint8:\n",
        "# #             maxv = float(np.nanmax(img))\n",
        "# #             img = ((np.clip(img, 0.0, 1.0) * 255.0) if maxv <= 1.0 else np.clip(img, 0, 255)).astype(np.uint8)\n",
        "\n",
        "# #         H, W, _ = img.shape\n",
        "# #         if output_size is None:\n",
        "# #             out_w, out_h = W, H\n",
        "# #         else:\n",
        "# #             out_w, out_h = int(output_size[0]), int(output_size[1])\n",
        "\n",
        "# #         # ---- Foreground mask (use alpha if informative; fallback to non-white RGB)\n",
        "# #         alpha = img[..., 3]\n",
        "# #         use_alpha = not (np.all(alpha <= alpha_threshold) or np.all(alpha >= 255))\n",
        "# #         if use_alpha:\n",
        "# #             hard_mask = (alpha > alpha_threshold).astype(np.uint8) * 1       # for bbox finding\n",
        "# #         else:\n",
        "# #             rgb = img[..., :3]\n",
        "# #             non_white = (rgb < white_thresh).any(axis=2)\n",
        "# #             hard_mask = (non_white.astype(np.uint8)) * 1\n",
        "\n",
        "# #         cnts, _ = cv2.findContours(hard_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "# #         if not cnts:\n",
        "# #             centered = np.full((out_h, out_w, 4), 255, dtype=np.uint8)         # white, opaque\n",
        "# #             sil_centered = np.zeros((out_h, out_w, 4), dtype=np.uint8)         # transparent\n",
        "# #             centered, sil_centered = centered[None, ...], sil_centered[None, ...]\n",
        "# #             info = {'scale': 1.0, 'bbox': (0,0,0,0), 'offset': (0,0),\n",
        "# #                     'pasted_box': (0,0,0,0), 'source_box': (0,0,0,0)}\n",
        "# #             return (centered, sil_centered, info) if return_transform else (centered, sil_centered)\n",
        "\n",
        "# #         # ---- Crop ROI for image and for mask/alpha\n",
        "# #         x, y, w, h = cv2.boundingRect(max(cnts, key=cv2.contourArea))\n",
        "# #         roi_img  = img[y:y+h, x:x+w, :]                         # (h,w,4)\n",
        "# #         roi_alpha_soft = roi_img[..., 3].astype(np.uint8) if use_alpha else hard_mask[y:y+h, x:x+w]  # (h,w)\n",
        "\n",
        "# #         # ---- Prepare canvases\n",
        "# #         centered       = np.full((out_h, out_w, 4), 255, dtype=np.uint8)       # white RGB, alpha=255\n",
        "# #         sil_centered   = np.zeros((out_h, out_w, 4), dtype=np.uint8)           # transparent\n",
        "\n",
        "# #         # ---- Fit into inner area with optional shrink\n",
        "# #         pad = max(0, int(pad))\n",
        "# #         inner_w, inner_h = max(1, out_w - 2*pad), max(1, out_h - 2*pad)\n",
        "# #         fits = (w <= inner_w) and (h <= inner_h)\n",
        "# #         scale = 1.0\n",
        "# #         roi_img_paste   = roi_img\n",
        "# #         roi_alpha_paste = roi_alpha_soft\n",
        "\n",
        "# #         if not fits:\n",
        "# #             if overflow == \"error\":\n",
        "# #                 raise ValueError(\"Cutout too large to fit canvas.\")\n",
        "# #             elif overflow == \"shrink\":\n",
        "# #                 scale = min(inner_w / max(1, w), inner_h / max(1, h))\n",
        "# #                 new_w = max(1, int(round(w * scale)))\n",
        "# #                 new_h = max(1, int(round(h * scale)))\n",
        "# #                 interp_img  = cv2.INTER_AREA if scale < 1.0 else cv2.INTER_LINEAR\n",
        "# #                 roi_img_paste   = cv2.resize(roi_img, (new_w, new_h), interpolation=interp_img)\n",
        "# #                 # For alpha/mask -> keep edges crisp\n",
        "# #                 roi_alpha_paste = cv2.resize(roi_alpha_soft, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
        "# #                 w, h = new_w, new_h\n",
        "# #             # overflow == \"crop\": keep size; paste region will be clipped\n",
        "\n",
        "# #         # ---- Center positions\n",
        "# #         start_x = pad + (inner_w - w) // 2\n",
        "# #         start_y = pad + (inner_h - h) // 2\n",
        "\n",
        "# #         # Destination & source windows (clip to canvas)\n",
        "# #         dx0, dy0 = max(0, start_x), max(0, start_y)\n",
        "# #         dx1, dy1 = min(out_w, start_x + w), min(out_h, start_y + h)\n",
        "# #         sx0, sy0 = max(0, -start_x), max(0, -start_y)\n",
        "# #         sx1, sy1 = sx0 + max(0, dx1 - dx0), sy0 + max(0, dy1 - dy0)\n",
        "\n",
        "# #         if dx1 > dx0 and dy1 > dy0:\n",
        "# #             src_rgba  = roi_img_paste[sy0:sy1, sx0:sx1, :]           # (ph,pw,4)\n",
        "# #             src_alpha = roi_alpha_paste[sy0:sy1, sx0:sx1]            # (ph,pw) 0..255\n",
        "\n",
        "# #             # ---- Paste onto centered (white) with alpha compositing for RGB; alpha stays 255\n",
        "# #             a = (src_alpha.astype(np.float32) / 255.0)[..., None]    # (ph,pw,1)\n",
        "# #             dst = centered[dy0:dy1, dx0:dx1, :].astype(np.float32)\n",
        "# #             dst[..., :3] = a * src_rgba[..., :3].astype(np.float32) + (1.0 - a) * dst[..., :3]\n",
        "# #             centered[dy0:dy1, dx0:dx1, :3] = dst[..., :3].astype(np.uint8)\n",
        "# #             centered[dy0:dy1, dx0:dx1, 3]  = 255\n",
        "\n",
        "# #             # ---- Build silhouette RGBA: white FG, transparent BG; alpha = src_alpha\n",
        "# #             sil_patch = sil_centered[dy0:dy1, dx0:dx1, :]\n",
        "# #             sil_patch[..., 0:3] = (src_alpha > 0).astype(np.uint8)[..., None] * 255\n",
        "# #             sil_patch[..., 3]   = src_alpha\n",
        "# #             sil_centered[dy0:dy1, dx0:dx1, :] = sil_patch\n",
        "\n",
        "\n",
        "\n",
        "# #         # Add batch dimension\n",
        "# #         centered     = centered[None, ...]       # (1,H,W,4)\n",
        "# #         sil_centered = sil_centered[None, ...]   # (1,H,W,4)\n",
        "\n",
        "# #         return centered, sil_centered\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #     def center_cutout_keep_scale(self,\n",
        "# #         cutout_rgba: np.ndarray,\n",
        "# #         output_size: Optional[Tuple[int, int]] = None,  # (W_out, H_out). If None, keep original size\n",
        "# #         pad: int = 0,\n",
        "# #         overflow: Literal[\"crop\", \"error\", \"shrink\"] = \"crop\",\n",
        "# #         alpha_threshold: int = 10,\n",
        "# #         white_thresh: int = 245,\n",
        "# #         return_transform: bool = False\n",
        "# #     ) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
        "# #         \"\"\"\n",
        "# #         Re-center an RGBA cutout on a white canvas, preserving scale.\n",
        "# #         Returns (1,H,W,4) batch format.\n",
        "# #         \"\"\"\n",
        "# #         # --- [same preprocessing as before] ---\n",
        "# #         if cutout_rgba is None:\n",
        "# #             raise ValueError(\"cutout_rgba is None\")\n",
        "# #         img = cutout_rgba\n",
        "# #         if img.ndim == 4 and img.shape[0] == 1:\n",
        "# #             img = img[0]\n",
        "# #         if img.ndim != 3 or img.shape[2] != 4:\n",
        "# #             raise ValueError(\"cutout_rgba must have shape (H,W,4) or (1,H,W,4).\")\n",
        "\n",
        "# #         if img.dtype != np.uint8:\n",
        "# #             maxv = np.nanmax(img)\n",
        "# #             if maxv <= 1.0:\n",
        "# #                 img = (np.clip(img, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
        "# #             else:\n",
        "# #                 img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "# #         H, W, _ = img.shape\n",
        "# #         if output_size is None:\n",
        "# #             out_w, out_h = W, H\n",
        "# #         else:\n",
        "# #             out_w, out_h = int(output_size[0]), int(output_size[1])\n",
        "\n",
        "# #         # Foreground mask detection (alpha or RGB fallback)\n",
        "# #         alpha = img[..., 3]\n",
        "# #         use_alpha = not (np.all(alpha <= alpha_threshold) or np.all(alpha >= 255))\n",
        "# #         if use_alpha:\n",
        "# #             mask = (alpha > alpha_threshold).astype(np.uint8) * 255\n",
        "# #         else:\n",
        "# #             rgb = img[..., :3]\n",
        "# #             non_white = (rgb < white_thresh).any(axis=2)\n",
        "# #             mask = (non_white.astype(np.uint8)) * 255\n",
        "\n",
        "# #         contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "# #         if not contours:\n",
        "# #             centered = np.full((out_h, out_w, 4), 255, dtype=np.uint8)\n",
        "# #             centered = centered[None, ...]   # add batch dim\n",
        "# #             info = {'scale': 1.0, 'bbox': (0,0,0,0), 'offset': (0,0),\n",
        "# #                     'pasted_box': (0,0,0,0), 'source_box': (0,0,0,0)}\n",
        "# #             return (centered, info) if return_transform else (centered,)\n",
        "\n",
        "# #         # Bounding box of largest contour\n",
        "# #         largest = max(contours, key=cv2.contourArea)\n",
        "# #         x, y, w, h = cv2.boundingRect(largest)\n",
        "# #         roi = img[y:y+h, x:x+w, :]\n",
        "\n",
        "# #         centered = np.full((out_h, out_w, 4), 255, dtype=np.uint8)\n",
        "\n",
        "# #         pad = max(0, int(pad))\n",
        "# #         inner_w, inner_h = max(1, out_w - 2*pad), max(1, out_h - 2*pad)\n",
        "# #         fits = (w <= inner_w) and (h <= inner_h)\n",
        "# #         scale, roi_to_paste = 1.0, roi\n",
        "\n",
        "# #         if not fits:\n",
        "# #             if overflow == \"error\":\n",
        "# #                 raise ValueError(\"Cutout too large to fit canvas.\")\n",
        "# #             elif overflow == \"shrink\":\n",
        "# #                 scale = min(inner_w / max(1, w), inner_h / max(1, h))\n",
        "# #                 new_w = max(1, int(round(w * scale)))\n",
        "# #                 new_h = max(1, int(round(h * scale)))\n",
        "# #                 interp = cv2.INTER_AREA if scale < 1.0 else cv2.INTER_LINEAR\n",
        "# #                 roi_to_paste = cv2.resize(roi, (new_w, new_h), interpolation=interp)\n",
        "# #                 w, h = new_w, new_h\n",
        "\n",
        "# #         start_x = pad + (inner_w - w) // 2\n",
        "# #         start_y = pad + (inner_h - h) // 2\n",
        "\n",
        "# #         dst_x0, dst_y0 = max(0, start_x), max(0, start_y)\n",
        "# #         dst_x1, dst_y1 = min(out_w, start_x + w), min(out_h, start_y + h)\n",
        "# #         src_x0, src_y0 = max(0, -start_x), max(0, -start_y)\n",
        "# #         src_x1, src_y1 = src_x0 + max(0, dst_x1 - dst_x0), src_y0 + max(0, dst_y1 - dst_y0)\n",
        "\n",
        "# #         if dst_x1 > dst_x0 and dst_y1 > dst_y0:\n",
        "# #             src = roi_to_paste[src_y0:src_y1, src_x0:src_x1, :]\n",
        "# #             dst = centered[dst_y0:dst_y1, dst_x0:dst_x1, :]\n",
        "# #             a = (src[..., 3:4].astype(np.float32)) / 255.0\n",
        "# #             dst[..., :3] = (a * src[..., :3].astype(np.float32) +\n",
        "# #                             (1.0 - a) * dst[..., :3].astype(np.float32)).astype(np.uint8)\n",
        "# #             centered[dst_y0:dst_y1, dst_x0:dst_x1, :3] = dst[..., :3]\n",
        "\n",
        "# #         info = {\n",
        "# #             'scale': scale,\n",
        "# #             'bbox': (x, y, roi.shape[1], roi.shape[0]),\n",
        "# #             'offset': (start_x, start_y),\n",
        "# #             'pasted_box': (dst_x0, dst_y0, dst_x1 - dst_x0, dst_y1 - dst_y0),\n",
        "# #             'source_box': (src_x0, src_y0, src_x1 - src_x0, src_y1 - src_y0),\n",
        "# #         }\n",
        "\n",
        "# #         # âœ… Add batch dimension at the end\n",
        "# #         centered = centered[None, ...]   # shape (1,H,W,4)\n",
        "# #         return (centered, info) if return_transform else (centered,)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #     # def get_camera_pose(self, distance=3, elevation=40, azimuth=40,\n",
        "# #     #                     device='cpu', eye_override=None):\n",
        "# #     #     \"\"\"\n",
        "# #     #     Computes the camera rotation and translation matrices using\n",
        "# #     #     spherical coordinates or a fixed eye location.\n",
        "\n",
        "# #     #     Args:\n",
        "# #     #         distance (float): Distance from camera to the origin (default: 3)\n",
        "# #     #         elevation (float): Elevation angle in degrees (default: 40)\n",
        "# #     #         azimuth (float): Azimuth angle in degrees (default: 40)\n",
        "# #     #         device (str): PyTorch device ('cpu' or 'cuda') for tensor operations\n",
        "# #     #         eye_override (list/tuple): Optional fixed camera position [x, y, z]\n",
        "# #     #                                 If provided, overrides spherical coordinates\n",
        "\n",
        "# #     #     Returns:\n",
        "# #     #         tuple: (R, T) where R is rotation matrix and T is translation vector\n",
        "# #     #     \"\"\"\n",
        "\n",
        "# #     #     # Check if a specific camera position is provided\n",
        "# #     #     if eye_override is not None:\n",
        "# #     #         # Convert eye position to PyTorch tensor and add batch dimension\n",
        "# #     #         eye = torch.tensor(eye_override, device=device).unsqueeze(0)  # Shape: (1, 3)\n",
        "\n",
        "# #     #         # Compute view transform using fixed eye position\n",
        "# #     #         # Camera looks at origin (0,0,0) by default\n",
        "# #     #         R, T = look_at_view_transform(eye=eye, device=device)\n",
        "# #     #     else:\n",
        "# #     #         # Use spherical coordinates to position camera\n",
        "# #     #         # dist: radial distance from origin\n",
        "# #     #         # elev: elevation angle (up/down rotation)\n",
        "# #     #         # azim: azimuth angle (left/right rotation)\n",
        "# #     #         R, T = look_at_view_transform(dist=distance, elev=elevation,\n",
        "# #     #                                       azim=azimuth, degrees=True,\n",
        "# #     #                                       device=device)\n",
        "\n",
        "# #     #     # # Debug output - print the computed matrices\n",
        "# #     #     # print(f\"Rotation R:\\n{R}\\n\")      # 3x3 rotation matrix\n",
        "# #     #     # print(f\"Translation T:\\n{T}\\n\")   # 3D translation vector\n",
        "\n",
        "# #     #     # Return the camera extrinsic parameters\n",
        "# #     #     return R, T\n",
        "\n",
        "# #     def get_camera_pose(self, distance=3, elevation=40, azimuth=40,\n",
        "# #                         device=\"cpu\", eye_override=None):\n",
        "# #         \"\"\"\n",
        "# #         Returns (R, T) with shapes (1,3,3) and (1,3).\n",
        "# #         - If eye_override is provided (3,) or (1,3), gradients flow through it.\n",
        "# #         - Otherwise uses spherical (dist, elev, azim).\n",
        "# #         \"\"\"\n",
        "# #         dev = device  # str or torch.device both work with .to(...)\n",
        "\n",
        "# #         if eye_override is not None:\n",
        "# #             # âœ… Preserve autograd: do NOT use torch.tensor(...) or float(...)\n",
        "# #             if isinstance(eye_override, torch.Tensor):\n",
        "# #                 eye = eye_override.to(device=dev, dtype=torch.float32).reshape(1, 3)\n",
        "# #             else:\n",
        "# #                 eye = torch.as_tensor(eye_override, dtype=torch.float32, device=dev).reshape(1, 3)\n",
        "\n",
        "# #             # Differentiable pose from camera center:\n",
        "# #             # X_cam = R @ X_world + T, with T = -R^T @ eye\n",
        "# #             R = look_at_rotation(eye, device=dev)                               # (1,3,3)\n",
        "# #             T = -torch.bmm(R.transpose(1, 2), eye[..., None])[:, :, 0]          # (1,3)\n",
        "# #             return R, T\n",
        "\n",
        "# #         # Spherical path (angles â†’ pose). 'degrees=True' only matters in this branch.\n",
        "# #         R, T = look_at_view_transform(dist=distance, elev=elevation,\n",
        "# #                                       azim=azimuth, degrees=True, device=dev)\n",
        "# #         return R, T\n",
        "\n",
        "\n",
        "# #     def render_mesh(self, mesh, silhouette_renderer, phong_renderer, R, T):\n",
        "# #         \"\"\"\n",
        "# #         Render a mesh using both silhouette and Phong renderers.\n",
        "# #         Returns both render outputs.\n",
        "# #         \"\"\"\n",
        "# #         silhouette = silhouette_renderer(meshes_world=mesh, R=R, T=T)\n",
        "# #         image_ref = phong_renderer(meshes_world=mesh, R=R, T=T)\n",
        "# #         return silhouette, image_ref\n",
        "\n",
        "\n",
        "\n",
        "# #     def get_camera_intrinsics(self, scale_factor = 1):\n",
        "# #         \"\"\"\n",
        "# #         Returns the camera intrinsics matrix K.\n",
        "# #         \"\"\"\n",
        "# #         # if cuda_available:\n",
        "# #         # K = np.array([\n",
        "# #         #     [672.*scale_factor,                  0.,   358.*scale_factor],\n",
        "# #         #     [               0.,   770.*scale_factor,   232.*scale_factor],\n",
        "# #         #     [               0.,                  0.,                  1.]\n",
        "# #         # ], dtype=\"double\")\n",
        "\n",
        "# #         # Principle point adjusted proportionally\n",
        "# #         K = np.array([\n",
        "# #             [1400.*scale_factor,                  0.,   128],\n",
        "# #             [               0.,   1800.*scale_factor,   128],\n",
        "# #             [               0.,                  0.,                  1.]\n",
        "# #         ], dtype=\"double\")\n",
        "\n",
        "# #         return K\n",
        "\n",
        "\n",
        "# #     def create_perspective_camera(self, K, image_size, device):\n",
        "# #         \"\"\"\n",
        "# #         Create a PerspectiveCameras object from intrinsics.\n",
        "# #         \"\"\"\n",
        "# #         fcl_screen = ((K[0, 0], K[1, 1]),)  # fx, fy\n",
        "# #         prp_screen = ((K[0, 2], K[1, 2]),)  # cx, cy\n",
        "# #         return PerspectiveCameras(\n",
        "# #             focal_length=fcl_screen,\n",
        "# #             principal_point=prp_screen,\n",
        "# #             in_ndc=False,\n",
        "# #             image_size=image_size,\n",
        "# #             device=device\n",
        "# #         )\n",
        "\n",
        "\n",
        "# #     def create_silhouette_renderer(self, cameras, image_size, device):\n",
        "# #         \"\"\"\n",
        "# #         Create a mesh renderer that outputs silhouettes.\n",
        "# #         \"\"\"\n",
        "# #         blend_params = BlendParams(sigma=1e-5, gamma=1e-4)\n",
        "\n",
        "# #         raster_settings = RasterizationSettings(\n",
        "# #             image_size=image_size,\n",
        "# #             blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma,\n",
        "# #             faces_per_pixel=100,\n",
        "# #             max_faces_per_bin=200000\n",
        "# #         )\n",
        "\n",
        "# #         renderer = MeshRenderer(\n",
        "# #             rasterizer=MeshRasterizer(\n",
        "# #                 cameras=cameras,\n",
        "# #                 raster_settings=raster_settings\n",
        "# #             ),\n",
        "# #             shader=SoftSilhouetteShader(blend_params=blend_params)\n",
        "# #         )\n",
        "# #         return renderer\n",
        "\n",
        "\n",
        "# #     def create_phong_renderer(self, cameras, image_size, device):\n",
        "# #         \"\"\"\n",
        "# #         Create a mesh renderer that outputs shaded RGB images using Phong lighting.\n",
        "# #         \"\"\"\n",
        "# #         raster_settings = RasterizationSettings(\n",
        "# #             image_size=image_size,\n",
        "# #             blur_radius=0.0,\n",
        "# #             faces_per_pixel=10,\n",
        "# #             max_faces_per_bin=200000\n",
        "# #         )\n",
        "\n",
        "# #         lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))\n",
        "\n",
        "\n",
        "# #         # Tune sigma/gamma for edge softness; background_color only affects RGB where alpha==0\n",
        "# #         blend_params = BlendParams(\n",
        "# #             sigma=1e-4,\n",
        "# #             gamma=1e-4,\n",
        "# #             background_color=(1.0, 1.0, 1.0)  # choose any; alpha controls transparency\n",
        "# #         )\n",
        "\n",
        "\n",
        "# #         renderer = MeshRenderer(\n",
        "# #             rasterizer=MeshRasterizer(\n",
        "# #                 cameras=cameras,\n",
        "# #                 raster_settings=raster_settings\n",
        "# #             ),\n",
        "# #             shader=SoftPhongShader(device=device, cameras=cameras, lights=lights, blend_params=blend_params)\n",
        "# #             # shader=HardPhongShader(device=device, cameras=cameras, lights=lights)\n",
        "# #         )\n",
        "# #         return renderer\n",
        "\n",
        "# #     def get_cad_model_files(self):\n",
        "\n",
        "# #         # Create the assets directory if it doesn't already exist.\n",
        "# #         os.makedirs(self.assets_dir, exist_ok=True)\n",
        "\n",
        "# #         # These are the location of the CAD model and Texture file for the object (e.g., dropbox shared link or similar)\n",
        "# #         cad_url = \"https://www.dropbox.com/scl/fi/y55irvvd58yw4ucu3igup/sse_only_jun9_scaled100.ply?rlkey=6io3u20qpupumdfp18i1ssmr9&dl=1\"\n",
        "# #         tex_url = \"https://www.dropbox.com/scl/fi/rj1thjtu16o2344vh5mjn/sse_texture.png?rlkey=503sm6dwzv1f8gyy1jgg6tnxl&dl=1\"\n",
        "\n",
        "# #         # Simplified CAD model (no texture, yet)\n",
        "# #         cad_small_url = \"https://www.dropbox.com/scl/fi/kg5zutt2deamc4ksi17f7/final_model.obj?rlkey=3uenujpffgqk1t0edx5plj5oe&dl=1\"\n",
        "# #         cad_small_path = os.path.join(self.assets_dir, \"final_model.obj\")\n",
        "# #         print(\"ðŸ“¥ Downloading simplified CAD model...\")\n",
        "# #         self._download_file(cad_small_url, cad_small_path)\n",
        "\n",
        "# #         # Detailed model\n",
        "# #         cad_detailed_url = \"https://www.dropbox.com/scl/fi/ubit9vw23fphcwpu0w88v/obj_000001.ply?rlkey=hx9f12a9ka8160mhxq8vnz7yk&dl=1\"\n",
        "# #         cad_detailed_path = os.path.join(self.assets_dir, \"obj_000001.ply\")\n",
        "# #         print(\"ðŸ“¥ Downloading detailed CAD model...\")\n",
        "# #         self._download_file(cad_detailed_url, cad_detailed_path)\n",
        "\n",
        "# #         # Test image\n",
        "# #         print(\"ðŸ“¥ Downloading a test image...\")\n",
        "# #         image_detailed_path = os.path.join(self.assets_dir, \"00002.png\")\n",
        "# #         self._download_file(\"https://www.dropbox.com/scl/fi/b4bjgg4w6n6c6ufx1krhg/00002.png?rlkey=2h6yikdk1liz4b0ojpw4dcz50&dl=1\", image_detailed_path)\n",
        "\n",
        "# #         # Test image\n",
        "# #         print(\"ðŸ“¥ Downloading a test image...\")\n",
        "# #         image_detailed_path = os.path.join(self.assets_dir, \"00016.png\")\n",
        "# #         self._download_file(\"https://www.dropbox.com/scl/fi/0nf86esq8723iirfz35uj/00016.png?rlkey=9yac94rwxj7uppz4oorgntpqz&dl=1\", image_detailed_path)\n",
        "\n",
        "\n",
        "# #         # Download the files into the assets directory\n",
        "# #         cad_path = os.path.join(self.assets_dir, \"sse_only_jun9_scaled100.ply\")\n",
        "# #         tex_path = os.path.join(self.assets_dir, \"sse_texture.png\")\n",
        "# #         print(\"ðŸ“¥ Downloading CAD model and texture...\")\n",
        "# #         self._download_file(cad_url, cad_path)\n",
        "# #         self._download_file(tex_url, tex_path)\n",
        "\n",
        "\n",
        "# #         # SSE model in .obj format\n",
        "# #         print(\"ðŸ“¥ Downloading SSE model in .obj format...\")\n",
        "# #         image_detailed_path = os.path.join(self.assets_dir, \"sse_only.obj\")\n",
        "# #         self._download_file(\"https://www.dropbox.com/scl/fi/nwfswb9yqaox5guzxedyp/sse_only.obj?rlkey=ub48myfosw690btzg7gfoxert&dl=1\", image_detailed_path)\n",
        "\n",
        "# #         # SSE model .mtl texture file\n",
        "# #         print(\"ðŸ“¥ Downloading SSE model .mtl texture file...\")\n",
        "# #         image_detailed_path = os.path.join(self.assets_dir, \"sse_only.mtl\")\n",
        "# #         self._download_file(\"https://www.dropbox.com/scl/fi/3mz2zgppszxj6cvv2mgqm/sse_only.mtl?rlkey=ckrw1ko30dk9d8unvsl659vmr&dl=1\", image_detailed_path)\n",
        "\n",
        "\n",
        "\n",
        "# #         # SSE model in .obj format\n",
        "# #         print(\"ðŸ“¥ Downloading new SSE model with real texture in .obj format...\")\n",
        "# #         image_detailed_path = os.path.join(self.assets_dir, \"sse_only_real_texture.obj\")\n",
        "# #         self._download_file(\"https://www.dropbox.com/scl/fi/8oo2ofzz6gtxn3ve2ue6r/sse_only_real_texture.obj?rlkey=a13utsbxbzib2c61j5gcasblt&dl=1\", image_detailed_path)\n",
        "\n",
        "# #         # SSE model .mtl texture file\n",
        "# #         print(\"ðŸ“¥ Downloading new SSE model with real texture .mtl texture file...\")\n",
        "# #         image_detailed_path = os.path.join(self.assets_dir, \"sse_only_real_texture.mtl\")\n",
        "# #         self._download_file(\"https://www.dropbox.com/scl/fi/e9ld55c4cyq4e6bguwti6/sse_only_real_texture.mtl?rlkey=0lvgesvk9yoi245gndcw0arqm&dl=1\", image_detailed_path)\n",
        "\n",
        "\n",
        "\n",
        "# #         print(f\"âœ… CAD and texture files placed in directory: {self.assets_dir}\")\n",
        "\n",
        "\n",
        "# #     def _download_file(self, url, dest_path):\n",
        "# #         with requests.get(url, stream=True) as r:\n",
        "# #             with open(dest_path, \"wb\") as f:\n",
        "# #                 shutil.copyfileobj(r.raw, f)\n",
        "\n",
        "# #     def get_device(self):\n",
        "# #         \"\"\"Returns the appropriate torch.device and sets CUDA device if available.\"\"\"\n",
        "# #         if torch.cuda.is_available():\n",
        "# #             torch.cuda.set_device(0)\n",
        "# #             device = torch.device(\"cuda:0\")\n",
        "# #             print(f\"[INFO]: Using CUDA device: {device}.\")\n",
        "# #         else:\n",
        "# #             device = torch.device(\"cpu\")\n",
        "# #             print(f\"[INFO]: Using CPU device.\")\n",
        "\n",
        "# #         return device\n",
        "\n",
        "\n",
        "# #     def load_cad_mesh(self, cad_path: str, device: torch.device = torch.device(\"cuda:0\")) -> Meshes:\n",
        "# #         \"\"\"\n",
        "# #         Load a CAD mesh (.obj or .ply) and return it as a PyTorch3D Meshes object with vertex colors.\n",
        "\n",
        "# #         Args:\n",
        "# #             cad_path (str): Full path to the CAD mesh file.\n",
        "# #             device (torch.device): The device to load the mesh onto.\n",
        "\n",
        "# #         Returns:\n",
        "# #             Meshes: A PyTorch3D Meshes object with yellow vertex color.\n",
        "# #         \"\"\"\n",
        "# #         cad_path = Path(cad_path)\n",
        "# #         ext = cad_path.suffix.lower()\n",
        "\n",
        "# #         if ext == \".obj\":\n",
        "# #             verts, faces, _ = load_obj(str(cad_path), load_textures=False)\n",
        "# #             verts = verts.to(device)\n",
        "# #             faces = faces.verts_idx.to(device)\n",
        "# #         elif ext == \".ply\":\n",
        "# #             verts, faces = load_ply(str(cad_path))\n",
        "# #             verts = (verts / 1000.0).to(device)  # Convert mm to meters\n",
        "# #             faces = faces.to(device)\n",
        "# #         else:\n",
        "# #             raise ValueError(f\"Unsupported CAD file format: {ext}\")\n",
        "\n",
        "# #         # Set vertex color to yellow\n",
        "# #         verts_rgb = torch.tensor([1.0, 1.0, 0.0], device=device).repeat(verts.shape[0], 1)  # (V, 3)\n",
        "# #         textures = TexturesVertex(verts_features=verts_rgb[None])  # (1, V, 3)\n",
        "\n",
        "# #         # Create and return the mesh\n",
        "# #         mesh = Meshes(\n",
        "# #             verts=[verts],\n",
        "# #             faces=[faces],\n",
        "# #             textures=textures\n",
        "# #         )\n",
        "# #         return mesh\n",
        "\n",
        "\n",
        "\n",
        "# #     def load_cad_mesh(self, cad_path: str, device: torch.device = torch.device(\"cuda:0\")) -> Meshes:\n",
        "# #         \"\"\"\n",
        "# #         Load a CAD mesh (.obj or .ply) and return it as a PyTorch3D Meshes object with vertex colors.\n",
        "\n",
        "# #         Args:\n",
        "# #             cad_path (str): Full path to the CAD mesh file.\n",
        "# #             device (torch.device): The device to load the mesh onto.\n",
        "\n",
        "# #         Returns:\n",
        "# #             Meshes: A PyTorch3D Meshes object with yellow vertex color.\n",
        "# #         \"\"\"\n",
        "# #         cad_path = Path(cad_path)\n",
        "# #         ext = cad_path.suffix.lower()\n",
        "\n",
        "# #         if ext == \".obj\":\n",
        "# #             verts, faces, _ = load_obj(str(cad_path), load_textures=False)\n",
        "# #             verts = verts.to(device)\n",
        "# #             faces = faces.verts_idx.to(device)\n",
        "# #         elif ext == \".ply\":\n",
        "# #             verts, faces = load_ply(str(cad_path))\n",
        "# #             verts = (verts / 1000.0).to(device)  # Convert mm to meters\n",
        "# #             faces = faces.to(device)\n",
        "# #         else:\n",
        "# #             raise ValueError(f\"Unsupported CAD file format: {ext}\")\n",
        "\n",
        "# #         # Set vertex color to yellow\n",
        "# #         verts_rgb = torch.tensor([1.0, 1.0, 0.0], device=device).repeat(verts.shape[0], 1)  # (V, 3)\n",
        "# #         textures = TexturesVertex(verts_features=verts_rgb[None])  # (1, V, 3)\n",
        "\n",
        "# #         # Create and return the mesh\n",
        "# #         mesh = Meshes(\n",
        "# #             verts=[verts],\n",
        "# #             faces=[faces],\n",
        "# #             textures=textures\n",
        "# #         )\n",
        "# #         return mesh\n",
        "\n",
        "\n",
        "# #     def load_cad_mesh_with_texture(self, cad_path: str, device: torch.device = torch.device(\"cuda:0\")) -> Meshes:\n",
        "# #         \"\"\"\n",
        "# #         Load a CAD mesh (.obj or .ply) and return it as a PyTorch3D Meshes object with vertex colors.\n",
        "\n",
        "# #         Args:\n",
        "# #             cad_path (str): Full path to the CAD mesh file.\n",
        "# #             device (torch.device): The device to load the mesh onto.\n",
        "\n",
        "# #         Returns:\n",
        "# #             Meshes: A PyTorch3D Meshes object with yellow vertex color.\n",
        "# #         \"\"\"\n",
        "# #         cad_path = Path(cad_path)\n",
        "\n",
        "# #         # Load obj file\n",
        "# #         mesh = load_objs_as_meshes([str(cad_path)], device=device)\n",
        "\n",
        "\n",
        "# #         return mesh\n",
        "\n",
        "\n",
        "\n",
        "# #     def compute_camera_pose(self, camera_position, device):\n",
        "# #         \"\"\"\n",
        "# #         Compute camera rotation R and translation T from a given camera position.\n",
        "# #         \"\"\"\n",
        "# #         R = look_at_rotation(camera_position[None, :], device=device)\n",
        "# #         T = -torch.bmm(R.transpose(1, 2), camera_position[None, :, None])[:, :, 0]\n",
        "# #         return R, T\n",
        "\n",
        "\n",
        "# #     def create_background_from_mask(self, mask):\n",
        "# #         \"\"\"\n",
        "# #         Convert a single-channel silhouette mask into an RGB background.\n",
        "# #         \"\"\"\n",
        "# #         return np.stack([mask] * 3, axis=-1)  # (H, W, 3)\n",
        "\n",
        "\n",
        "# #     def alpha_blend(self, rgb_rendered, alpha_rendered, background_img):\n",
        "# #         \"\"\"\n",
        "# #         Alpha-blend the rendered RGB image over the background.\n",
        "# #         \"\"\"\n",
        "# #         return rgb_rendered * alpha_rendered + background_img * (1.0 - alpha_rendered)\n",
        "\n",
        "\n",
        "\n",
        "# #     def render_overlay(self, model, phong_renderer, mask_background):\n",
        "# #         \"\"\"\n",
        "# #         Renders an overlay image of the current model pose and background.\n",
        "\n",
        "# #         Args:\n",
        "# #             model: The pose model with .camera_position, .device, and .meshes.\n",
        "# #             phong_renderer: A renderer used for generating the RGB + alpha image.\n",
        "# #             mask_background: (H, W, 3) background image to blend with.\n",
        "\n",
        "# #         Returns:\n",
        "# #             image_uint8: Blended uint8 image with the rendered object overlaid on the background.\n",
        "# #         \"\"\"\n",
        "\n",
        "# #         R, T = self.compute_camera_pose(model.camera_position, model.device)\n",
        "\n",
        "\n",
        "# #         # print(\"R, T before roll\")\n",
        "# #         # print(R)\n",
        "# #         # print(T)\n",
        "\n",
        "# #         # print(\"self.roll_deg = \", model.roll_deg)\n",
        "\n",
        "# #         # Add roll (try mode=\"camera\" first; if no visible spin, try mode=\"world\")\n",
        "# #         R, T = self.add_camera_roll_to_RT(R, T, roll_deg=model.roll_deg, device=model.device, mode=\"world\")\n",
        "\n",
        "# #         # print(\"R, T after roll\")\n",
        "# #         # print(R)\n",
        "# #         # print(T)\n",
        "\n",
        "\n",
        "\n",
        "# #         rendered = phong_renderer(\n",
        "# #             meshes_world=model.meshes.clone(), R=R, T=T\n",
        "# #         )[0]  # (H, W, 4)\n",
        "\n",
        "# #         rgb_rendered = rendered[..., :3].detach().cpu().numpy()\n",
        "# #         alpha_rendered = rendered[..., 3].detach().cpu().numpy()[..., None]\n",
        "\n",
        "# #         overlay = self.alpha_blend(rgb_rendered, alpha_rendered, mask_background)\n",
        "# #         image_uint8 = img_as_ubyte(overlay)\n",
        "\n",
        "# #         return image_uint8\n",
        "\n",
        "\n",
        "# #     def display_cad_trimesh(self, mesh_path):\n",
        "# #         # Load a mesh from file (format is inferred from extension)\n",
        "# #         mesh = trimesh.load(mesh_file_path)\n",
        "\n",
        "# #         # Print basic info\n",
        "# #         print(\"Number of vertices:\", len(mesh.vertices))\n",
        "# #         print(\"Number of faces:\", len(mesh.faces))\n",
        "\n",
        "\n",
        "# #         # Visualize the mesh\n",
        "# #         mesh.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #     def optimize_camera_pose_best_loss(self,\n",
        "# #         model,\n",
        "# #         optimizer,\n",
        "# #         mask_background,\n",
        "# #         phong_renderer,\n",
        "# #         writer,\n",
        "# #         num_iter=100,\n",
        "# #         log_every=10,\n",
        "# #         patience=10,\n",
        "# #         tolerance=10.0\n",
        "# #     ):\n",
        "# #         \"\"\"\n",
        "# #         Runs pose optimization and saves rendered frames to a GIF.\n",
        "\n",
        "# #         Args:\n",
        "# #             model: A torch.nn.Module representing the pose model.\n",
        "# #             optimizer: Optimizer for model parameters.\n",
        "# #             mask_background: (H, W, 3) RGB numpy array of the background.\n",
        "# #             phong_renderer: Renderer used for rendering the mesh.\n",
        "# #             writer: An imageio writer to save rendered frames.\n",
        "# #             num_iter: Maximum number of iterations.\n",
        "# #             log_every: Interval to render and save frames.\n",
        "# #             patience: Early stopping patience.\n",
        "# #             tolerance: Minimum loss change to reset patience.\n",
        "# #         Returns:\n",
        "# #             Tuple (R, T) of the final camera rotation and translation.\n",
        "# #         \"\"\"\n",
        "# #         best_loss = float(\"inf\")\n",
        "# #         wait = 0\n",
        "# #         final_R, final_T = None, None\n",
        "\n",
        "# #         loop = tqdm(range(num_iter))\n",
        "# #         for i in loop:\n",
        "# #             torch.cuda.empty_cache()\n",
        "# #             optimizer.zero_grad()\n",
        "# #             loss, im, _, _ = model()\n",
        "# #             loss.backward()\n",
        "# #             optimizer.step()\n",
        "\n",
        "# #             current_loss = loss.item()\n",
        "# #             loop.set_description(f\"Optimizing (loss {current_loss:.4f})\")\n",
        "\n",
        "# #             # Early stopping with best pose tracking\n",
        "# #             if current_loss < best_loss:\n",
        "# #                 best_loss = current_loss\n",
        "# #                 wait = 0\n",
        "# #                 final_R, final_T = self.compute_camera_pose(model.camera_position, model.device)\n",
        "# #             else:\n",
        "# #                 wait += 1\n",
        "# #                 if wait >= patience:\n",
        "# #                     print(f\"[INFO] Early stopping at step {i}: loss plateaued.\")\n",
        "# #                     break\n",
        "\n",
        "\n",
        "# #             # if abs(best_loss - current_loss) < tolerance:\n",
        "# #             #     wait += 1\n",
        "# #             #     if wait >= patience:\n",
        "# #             #         print(f\"[INFO] Early stopping at step {i}: loss plateaued.\")\n",
        "# #             #         break\n",
        "# #             # else:\n",
        "# #             #     best_loss = current_loss\n",
        "# #             #     wait = 0\n",
        "\n",
        "# #             #     # Save current best pose when new best loss is found\n",
        "# #             #     final_R, final_T = self.compute_camera_pose(model.camera_position, model.device)\n",
        "\n",
        "# #             # Periodically render and save for visualization\n",
        "# #             if i % log_every == 0:\n",
        "# #                 image_uint8 = self.render_overlay(model, phong_renderer, mask_background)\n",
        "# #                 writer.append_data(image_uint8)\n",
        "\n",
        "# #                 print(f\"Image from model: {im.shape}\")\n",
        "\n",
        "\n",
        "# #         writer.close()\n",
        "\n",
        "# #         print(f\"[RESULT] Best loss: {best_loss:.6f}\")\n",
        "# #         return final_R, final_T\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #     def optimize_camera_pose(self,\n",
        "# #         model,\n",
        "# #         optimizer,\n",
        "# #         mask_background,\n",
        "# #         phong_renderer,\n",
        "# #         writer,\n",
        "# #         num_iter=100,\n",
        "# #         log_every=10,\n",
        "# #         patience=10,\n",
        "# #         tolerance=10.0\n",
        "# #     ):\n",
        "# #         \"\"\"\n",
        "# #         Runs pose optimization and saves rendered frames to a GIF.\n",
        "\n",
        "# #         Args:\n",
        "# #             model: A torch.nn.Module representing the pose model.\n",
        "# #             optimizer: Optimizer for model parameters.\n",
        "# #             mask_background: (H, W, 3) RGB numpy array of the background.\n",
        "# #             phong_renderer: Renderer used for rendering the mesh.\n",
        "# #             writer: An imageio writer to save rendered frames.\n",
        "# #             num_iter: Maximum number of iterations.\n",
        "# #             log_every: Interval to render and save frames.\n",
        "# #             patience: Early stopping patience.\n",
        "# #             tolerance: Minimum loss change to reset patience.\n",
        "# #         Returns:\n",
        "# #             Tuple (R, T) of the final camera rotation and translation.\n",
        "# #         \"\"\"\n",
        "# #         best_loss = float(\"inf\")\n",
        "# #         wait = 0\n",
        "# #         final_R, final_T = None, None\n",
        "\n",
        "# #         loop = tqdm(range(num_iter))\n",
        "# #         for i in loop:\n",
        "# #             torch.cuda.empty_cache()\n",
        "# #             optimizer.zero_grad()\n",
        "# #             loss, im, _, _= model()\n",
        "# #             loss.backward()\n",
        "# #             optimizer.step()\n",
        "\n",
        "# #             current_loss = loss.item()\n",
        "# #             loop.set_description(f\"Optimizing (loss {current_loss:.4f})\")\n",
        "# #             # print(f\"Step {i}, Loss = {current_loss}\")\n",
        "\n",
        "# #             # Early stopping\n",
        "# #             if abs(best_loss - current_loss) < tolerance:\n",
        "# #                 wait += 1\n",
        "# #                 if wait >= patience:\n",
        "# #                     print(f\"[INFO] Early stopping at step {i}: loss plateaued.\")\n",
        "# #                     break\n",
        "# #             else:\n",
        "# #                 best_loss = current_loss\n",
        "# #                 wait = 0\n",
        "\n",
        "# #             # Periodically render and save\n",
        "# #             if i % log_every == 0:\n",
        "\n",
        "\n",
        "# #                 R, T = self.compute_camera_pose(model.camera_position, model.device)\n",
        "\n",
        "# #                 # Create an overlay image of the rendered model in its current\n",
        "# #                 # pose and the reference silhouette.\n",
        "# #                 image_uint8 = self.render_overlay(model, phong_renderer, mask_background)\n",
        "\n",
        "# #                 print(f\"Image from model: {im.shape}\")\n",
        "# #                 writer.append_data(image_uint8)\n",
        "\n",
        "# #                 final_R, final_T = R, T\n",
        "\n",
        "# #                 # Consider returning the best loss as the final one (not the one after plato is reached)\n",
        "\n",
        "# #         writer.close()\n",
        "\n",
        "# #         return final_R, final_T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New utilities using namespaces and stateless classes"
      ],
      "metadata": {
        "id": "qMucAxOYFayO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Namespaces (pure helpers live here) =================================\n",
        "\n",
        "class Cam:\n",
        "    \"\"\"Camera math (stateless).\"\"\"\n",
        "    @staticmethod\n",
        "    def get_camera_position(distance, elevation, azimuth, *, degrees=True, device=\"cpu\"):\n",
        "        from pytorch3d.renderer import camera_position_from_spherical_angles\n",
        "        return camera_position_from_spherical_angles(distance=distance,\n",
        "                                                     elevation=elevation,\n",
        "                                                     azimuth=azimuth,\n",
        "                                                     degrees=degrees).to(device)\n",
        "\n",
        "    @staticmethod\n",
        "    def camera_center_to_dist_elev_azim(C: torch.Tensor):\n",
        "        \"\"\"C (...,3) -> (dist,elev,azim) (PyTorch3D conv: Y-up; azim=0 -> +Z; +azim toward +X).\"\"\"\n",
        "        squeeze = False\n",
        "        if C.ndim == 1:\n",
        "            C = C[None, :]\n",
        "            squeeze = True\n",
        "        x, y, z = C[..., 0], C[..., 1], C[..., 2]\n",
        "        dist = torch.linalg.norm(C, dim=-1)\n",
        "        rho  = torch.sqrt(torch.clamp(x*x + z*z, min=1e-12))\n",
        "        elev = torch.rad2deg(torch.atan2(y, rho))\n",
        "        azim = torch.rad2deg(torch.atan2(x, z))\n",
        "        if squeeze:\n",
        "            dist, elev, azim = dist[0], elev[0], azim[0]\n",
        "        return dist, elev, azim\n",
        "\n",
        "    @staticmethod\n",
        "    def get_camera_pose(distance=3, elevation=40, azimuth=40, *, device=\"cpu\", eye_override=None):\n",
        "        \"\"\"\n",
        "        Returns (R,T) with shapes (1,3,3),(1,3). If eye_override is provided (3,) or (1,3),\n",
        "        autograd paths are preserved.\n",
        "        \"\"\"\n",
        "        from pytorch3d.renderer import look_at_view_transform\n",
        "        from pytorch3d.renderer.cameras import look_at_rotation\n",
        "\n",
        "        dev = device\n",
        "        if eye_override is not None:\n",
        "            if isinstance(eye_override, torch.Tensor):\n",
        "                eye = eye_override.to(device=dev, dtype=torch.float32).reshape(1, 3)\n",
        "            else:\n",
        "                eye = torch.as_tensor(eye_override, dtype=torch.float32, device=dev).reshape(1, 3)\n",
        "            R = look_at_rotation(eye, device=dev)\n",
        "            T = -torch.bmm(R.transpose(1, 2), eye[..., None])[:, :, 0]\n",
        "            return R, T\n",
        "\n",
        "        R, T = look_at_view_transform(dist=distance, elev=elevation, azim=azimuth,\n",
        "                                      degrees=True, device=dev)\n",
        "        return R, T\n",
        "\n",
        "    @staticmethod\n",
        "    def add_camera_roll_to_RT(R, T, roll_deg, *, device=None, mode=\"camera\"):\n",
        "        \"\"\"\n",
        "        Compose a Z-axis roll into (R,T), keeping the same camera center C.\n",
        "        Grad-safe: roll_deg can be a Tensor/Parameter.\n",
        "        mode: \"camera\" -> R' = Rz @ R ; \"world\" -> R' = R @ Rz\n",
        "        \"\"\"\n",
        "        if not torch.is_tensor(R): R = torch.as_tensor(R)\n",
        "        if not torch.is_tensor(T): T = torch.as_tensor(T)\n",
        "        dev   = device or R.device\n",
        "        dtype = torch.float32\n",
        "        R = R.to(dev, dtype)\n",
        "        T = T.to(dev, dtype)\n",
        "\n",
        "        unbatched = (R.ndim == 2)\n",
        "        if unbatched:\n",
        "            R = R[None, ...]\n",
        "            T = T[None, ...]\n",
        "\n",
        "        # C from T = -R^T C  =>  C = -R T\n",
        "        C = -torch.matmul(R, T[..., None]).squeeze(-1)\n",
        "\n",
        "        theta = torch.as_tensor(roll_deg, dtype=dtype, device=dev).reshape(1)  # keep grad\n",
        "        c, s = torch.cos(torch.deg2rad(theta)), torch.sin(torch.deg2rad(theta))\n",
        "        z = torch.zeros_like(c); o = torch.ones_like(c)\n",
        "        Rz = torch.stack([\n",
        "            torch.stack([ c, -s, z], dim=-1),\n",
        "            torch.stack([ s,  c, z], dim=-1),\n",
        "            torch.stack([ z,  z,  o], dim=-1),\n",
        "        ], dim=1)  # (1,3,3)\n",
        "\n",
        "        R_new = torch.matmul(Rz, R) if mode == \"camera\" else torch.matmul(R, Rz)\n",
        "        T_new = -torch.matmul(R_new.transpose(1, 2), C[..., None]).squeeze(-1)\n",
        "\n",
        "        if unbatched:\n",
        "            R_new, T_new = R_new[0], T_new[0]\n",
        "        return R_new, T_new\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_camera_pose_from_center(C: torch.Tensor, *, device):\n",
        "        \"\"\"C (3,) -> (R,T).\"\"\"\n",
        "        from pytorch3d.renderer.cameras import look_at_rotation\n",
        "        R = look_at_rotation(C[None, :], device=device)\n",
        "        T = -torch.bmm(R.transpose(1, 2), C[None, :, None])[:, :, 0]\n",
        "        return R, T\n",
        "\n",
        "\n",
        "class Img:\n",
        "    \"\"\"Cutout/mask utilities (stateless).\"\"\"\n",
        "    @staticmethod\n",
        "    def read_rgb_cutout_black_bg(path: str) -> np.ndarray:\n",
        "        img = Image.open(path)\n",
        "        img = ImageOps.exif_transpose(img).convert(\"RGBA\")\n",
        "        rgba = np.asarray(img).astype(np.float32)\n",
        "        rgb  = rgba[..., :3]\n",
        "        a    = rgba[..., 3:4] / 255.0\n",
        "        rgb_black = rgb * a\n",
        "        return np.clip(rgb_black, 0, 255).astype(np.uint8)\n",
        "\n",
        "    @staticmethod\n",
        "    def center_cutout_rgb_uint8(img: np.ndarray, black_thresh: int = 5) -> np.ndarray:\n",
        "        if img.ndim != 3 or img.shape[2] != 3 or img.dtype != np.uint8:\n",
        "            raise ValueError(\"Expected (H,W,3) uint8.\")\n",
        "        H, W, _ = img.shape\n",
        "        mask = (img.max(axis=2) > black_thresh)\n",
        "        if not mask.any():\n",
        "            return img.copy()\n",
        "        ys, xs = np.where(mask)\n",
        "        y0, y1 = int(ys.min()), int(ys.max()) + 1\n",
        "        x0, x1 = int(xs.min()), int(xs.max()) + 1\n",
        "        roi = img[y0:y1, x0:x1, :]\n",
        "        h, w = roi.shape[:2]\n",
        "        sy, sx = (H - h) // 2, (W - w) // 2\n",
        "        out = np.zeros_like(img)\n",
        "        out[sy:sy+h, sx:sx+w] = roi\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def crop_center_to_size_uint8(img: np.ndarray, out_size: tuple[int, int], fill_color=(0,0,0)) -> np.ndarray:\n",
        "        if img.ndim != 3 or img.shape[2] != 3 or img.dtype != np.uint8:\n",
        "            raise ValueError(\"Expected (H,W,3) uint8.\")\n",
        "        W_out, H_out = map(int, out_size)\n",
        "        H, W = img.shape[:2]\n",
        "        left = (W - W_out) // 2; top = (H - H_out) // 2\n",
        "        right, bottom = left + W_out, top + H_out\n",
        "        src_x0, src_y0 = max(0, left), max(0, top)\n",
        "        src_x1, src_y1 = min(W, right), min(H, bottom)\n",
        "        dst_x0, dst_y0 = max(0, -left), max(0, -top)\n",
        "        dst_x1, dst_y1 = dst_x0 + (src_x1 - src_x0), dst_y0 + (src_y1 - src_y0)\n",
        "        out = np.empty((H_out, W_out, 3), dtype=np.uint8); out[...] = np.asarray(fill_color, np.uint8)\n",
        "        if src_x1 > src_x0 and src_y1 > src_y0:\n",
        "            out[dst_y0:dst_y1, dst_x0:dst_x1] = img[src_y0:src_y1, src_x0:src_x1]\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def add_alpha_from_black_bg_uint8(img_rgb: np.ndarray, *, black_thresh: int = 5,\n",
        "                                      mode: str = \"binary\", close_kernel: int = 3) -> np.ndarray:\n",
        "        if img_rgb.ndim != 3 or img_rgb.shape[2] != 3 or img_rgb.dtype != np.uint8:\n",
        "            raise ValueError(\"Expected (H,W,3) uint8.\")\n",
        "        mask = (img_rgb.max(axis=2) > black_thresh).astype(np.uint8) * 255\n",
        "        im_flood = mask.copy()\n",
        "        h, w = mask.shape\n",
        "        ff_mask = np.zeros((h+2, w+2), np.uint8)\n",
        "        cv2.floodFill(im_flood, ff_mask, (0,0), 255)\n",
        "        holes = cv2.bitwise_not(im_flood)\n",
        "        mask_filled = cv2.bitwise_or(mask, holes)\n",
        "        if close_kernel and close_kernel > 1:\n",
        "            k = np.ones((close_kernel, close_kernel), np.uint8)\n",
        "            mask_filled = cv2.morphologyEx(mask_filled, cv2.MORPH_CLOSE, k)\n",
        "        if mode == \"binary\":\n",
        "            alpha = mask_filled\n",
        "        elif mode == \"soft\":\n",
        "            alpha = np.maximum(img_rgb.max(axis=2), mask_filled).astype(np.uint8)\n",
        "        else:\n",
        "            raise ValueError(\"mode must be 'binary' or 'soft'\")\n",
        "        return np.concatenate([img_rgb, alpha[..., None]], axis=2)\n",
        "\n",
        "    @staticmethod\n",
        "    def to_float_batched_rgba_white_bg_preserve_alpha(img: np.ndarray, *,\n",
        "                                                      derive_alpha: str = \"soft\",\n",
        "                                                      black_thresh: int = 5) -> np.ndarray:\n",
        "        arr = np.asarray(img)\n",
        "        if arr.ndim == 4 and arr.shape[0] == 1:\n",
        "            arr = arr[0]\n",
        "        if arr.ndim != 3 or arr.shape[-1] not in (3,4):\n",
        "            raise ValueError(\"Expected (H,W,3/4) or (1,H,W,3/4).\")\n",
        "        arr = arr.astype(np.float32, copy=False)\n",
        "        if np.nanmax(arr) > 1.0 + 1e-6:\n",
        "            arr = np.clip(arr, 0, 255) / 255.0\n",
        "        if arr.shape[-1] == 4:\n",
        "            rgb, a = arr[..., :3], np.clip(arr[..., 3], 0, 1)\n",
        "        else:\n",
        "            rgb = arr\n",
        "            if derive_alpha == \"soft\":\n",
        "                a = np.clip(rgb.max(axis=-1), 0, 1)\n",
        "            elif derive_alpha == \"binary\":\n",
        "                a = (rgb.max(axis=-1) > (black_thresh/255.0)).astype(np.float32)\n",
        "            else:\n",
        "                raise ValueError(\"derive_alpha must be 'soft' or 'binary'\")\n",
        "        a3 = a[..., None]\n",
        "        rgb_on_white = rgb * a3 + (1.0 - a3) * 1.0\n",
        "        return np.concatenate([rgb_on_white, a3], axis=-1)[None, ...].astype(np.float32)\n",
        "\n",
        "    @staticmethod\n",
        "    def center_cutout_keep_scale_2(cutout_rgba: np.ndarray,\n",
        "                                   output_size: Optional[Tuple[int,int]] = None,\n",
        "                                   pad: int = 0,\n",
        "                                   overflow: Literal[\"crop\",\"error\",\"shrink\"] = \"crop\",\n",
        "                                   alpha_threshold: int = 10,\n",
        "                                   white_thresh: int = 245,\n",
        "                                   return_transform: bool = False):\n",
        "        \"\"\"Re-center RGBA on white; ALSO produce centered silhouette RGBA. Returns (1,H,W,4) each.\"\"\"\n",
        "        if cutout_rgba.ndim == 4 and cutout_rgba.shape[0] == 1:\n",
        "            img = cutout_rgba[0]\n",
        "        else:\n",
        "            img = cutout_rgba\n",
        "        if img.ndim != 3 or img.shape[2] != 4:\n",
        "            raise ValueError(\"Expected (H,W,4) or (1,H,W,4).\")\n",
        "        if img.dtype != np.uint8:\n",
        "            img = (np.clip(img, 0, 1)*255.0 if np.nanmax(img) <= 1.0 else np.clip(img,0,255)).astype(np.uint8)\n",
        "        H, W, _ = img.shape\n",
        "        out_w, out_h = (W, H) if output_size is None else (int(output_size[0]), int(output_size[1]))\n",
        "        alpha = img[..., 3]\n",
        "        use_alpha = not (np.all(alpha <= alpha_threshold) or np.all(alpha >= 255))\n",
        "        if use_alpha:\n",
        "            hard_mask = (alpha > alpha_threshold).astype(np.uint8)\n",
        "        else:\n",
        "            hard_mask = (img[..., :3] < white_thresh).any(axis=2).astype(np.uint8)\n",
        "        cnts, _ = cv2.findContours(hard_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if not cnts:\n",
        "            centered = np.full((out_h, out_w, 4), 255, np.uint8)[None, ...]\n",
        "            sil      = np.zeros((out_h, out_w, 4), np.uint8)[None, ...]\n",
        "            info = {'scale':1.0,'bbox':(0,0,0,0),'offset':(0,0),\n",
        "                    'pasted_box':(0,0,0,0),'source_box':(0,0,0,0)}\n",
        "            return (centered, sil, info) if return_transform else (centered, sil)\n",
        "\n",
        "        x, y, w, h = cv2.boundingRect(max(cnts, key=cv2.contourArea))\n",
        "        roi = img[y:y+h, x:x+w, :]\n",
        "        roi_alpha = roi[..., 3] if use_alpha else (hard_mask[y:y+h, x:x+w]*255).astype(np.uint8)\n",
        "\n",
        "        centered = np.full((out_h, out_w, 4), 255, np.uint8)\n",
        "        sil      = np.zeros((out_h, out_w, 4), np.uint8)\n",
        "\n",
        "        pad = max(0, int(pad))\n",
        "        inner_w, inner_h = max(1, out_w-2*pad), max(1, out_h-2*pad)\n",
        "        scale = 1.0\n",
        "        if not (w <= inner_w and h <= inner_h):\n",
        "            if overflow == \"error\":\n",
        "                raise ValueError(\"Cutout too large.\")\n",
        "            if overflow == \"shrink\":\n",
        "                scale = min(inner_w/max(1,w), inner_h/max(1,h))\n",
        "                new_w, new_h = max(1,int(round(w*scale))), max(1,int(round(h*scale)))\n",
        "                roi = cv2.resize(roi, (new_w,new_h), interpolation=cv2.INTER_AREA if scale<1 else cv2.INTER_LINEAR)\n",
        "                roi_alpha = cv2.resize(roi_alpha, (new_w,new_h), interpolation=cv2.INTER_NEAREST)\n",
        "                w, h = new_w, new_h\n",
        "        start_x = pad + (inner_w - w)//2\n",
        "        start_y = pad + (inner_h - h)//2\n",
        "\n",
        "        dx0, dy0 = max(0,start_x), max(0,start_y)\n",
        "        dx1, dy1 = min(out_w, start_x+w), min(out_h, start_y+h)\n",
        "        sx0, sy0 = max(0,-start_x), max(0,-start_y)\n",
        "        sx1, sy1 = sx0 + max(0,dx1-dx0), sy0 + max(0,dy1-dy0)\n",
        "\n",
        "        if dx1>dx0 and dy1>dy0:\n",
        "            src_rgba  = roi[sy0:sy1, sx0:sx1]\n",
        "            src_alpha = roi_alpha[sy0:sy1, sx0:sx1]\n",
        "            a = (src_alpha.astype(np.float32)/255.0)[..., None]\n",
        "            dst = centered[dy0:dy1, dx0:dx1].astype(np.float32)\n",
        "            dst[..., :3] = a*src_rgba[..., :3].astype(np.float32) + (1.0-a)*dst[..., :3]\n",
        "            centered[dy0:dy1, dx0:dx1, :3] = dst[..., :3].astype(np.uint8)\n",
        "            centered[dy0:dy1, dx0:dx1, 3]  = 255\n",
        "            patch = sil[dy0:dy1, dx0:dx1]\n",
        "            patch[..., 0:3] = (src_alpha>0).astype(np.uint8)[..., None]*255\n",
        "            patch[..., 3]   = src_alpha\n",
        "            sil[dy0:dy1, dx0:dx1] = patch\n",
        "\n",
        "        centered = centered[None, ...]\n",
        "        sil      = sil[None, ...]\n",
        "        info = {'scale': scale, 'bbox': (x,y,w,h), 'offset': (start_x,start_y),\n",
        "                'pasted_box': (dx0,dy0,dx1-dx0,dy1-dy0), 'source_box': (sx0,sy0,sx1-sx0,sy1-sy0)}\n",
        "        return (centered, sil, info) if return_transform else (centered, sil)\n",
        "\n",
        "\n",
        "class Render:\n",
        "    \"\"\"Render setup & overlay (stateless).\"\"\"\n",
        "    @staticmethod\n",
        "    def get_camera_intrinsics(scale_factor=1):\n",
        "        K = np.array([\n",
        "            [1400.*scale_factor, 0., 128],\n",
        "            [0., 1800.*scale_factor, 128],\n",
        "            [0., 0., 1.]\n",
        "        ], dtype=\"double\")\n",
        "        return K\n",
        "\n",
        "    @staticmethod\n",
        "    def create_perspective_camera(K, image_size, device):\n",
        "        from pytorch3d.renderer import PerspectiveCameras\n",
        "        fcl = ((K[0,0], K[1,1]),)\n",
        "        prp = ((K[0,2], K[1,2]),)\n",
        "        return PerspectiveCameras(focal_length=fcl, principal_point=prp,\n",
        "                                  in_ndc=False, image_size=image_size, device=device)\n",
        "\n",
        "    @staticmethod\n",
        "    def create_silhouette_renderer(cameras, image_size, device):\n",
        "        from pytorch3d.renderer import (RasterizationSettings, MeshRenderer, MeshRasterizer,\n",
        "                                        BlendParams, SoftSilhouetteShader)\n",
        "        blend = BlendParams(sigma=1e-5, gamma=1e-4)\n",
        "        rast  = RasterizationSettings(image_size=image_size,\n",
        "                                      blur_radius=np.log(1./1e-4 - 1.)*blend.sigma,\n",
        "                                      faces_per_pixel=100,\n",
        "                                      max_faces_per_bin=200000)\n",
        "        return MeshRenderer(\n",
        "            rasterizer=MeshRasterizer(cameras=cameras, raster_settings=rast),\n",
        "            shader=SoftSilhouetteShader(blend_params=blend)\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def create_phong_renderer(cameras, image_size, device):\n",
        "        from pytorch3d.renderer import (RasterizationSettings, MeshRenderer, MeshRasterizer,\n",
        "                                        BlendParams, SoftPhongShader, PointLights)\n",
        "        rast = RasterizationSettings(image_size=image_size, blur_radius=0.0,\n",
        "                                     faces_per_pixel=10, max_faces_per_bin=200000)\n",
        "        lights = PointLights(device=device, location=((2.0,2.0,-2.0),))\n",
        "        blend  = BlendParams(sigma=1e-4, gamma=1e-4, background_color=(1.0,1.0,1.0))\n",
        "        return MeshRenderer(\n",
        "            rasterizer=MeshRasterizer(cameras=cameras, raster_settings=rast),\n",
        "            shader=SoftPhongShader(device=device, cameras=cameras, lights=lights, blend_params=blend)\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def render_mesh(mesh, silhouette_renderer, phong_renderer, R, T):\n",
        "        sil = silhouette_renderer(meshes_world=mesh, R=R, T=T)\n",
        "        img = phong_renderer(meshes_world=mesh, R=R, T=T)\n",
        "        return sil, img\n",
        "\n",
        "    @staticmethod\n",
        "    def alpha_blend(rgb_rendered, alpha_rendered, background_img):\n",
        "        return rgb_rendered * alpha_rendered + background_img * (1.0 - alpha_rendered)\n",
        "\n",
        "    @staticmethod\n",
        "    def render_overlay(model, phong_renderer, mask_background):\n",
        "        R, T = Cam.compute_camera_pose_from_center(model.camera_position, device=model.device)\n",
        "        R, T = Cam.add_camera_roll_to_RT(R, T, roll_deg=model.roll_deg, device=model.device, mode=\"world\")\n",
        "        rgba = phong_renderer(meshes_world=model.meshes.clone(), R=R, T=T)[0]\n",
        "        rgb  = rgba[..., :3].detach().cpu().numpy()\n",
        "        a    = rgba[..., 3].detach().cpu().numpy()[..., None]\n",
        "        overlay = Render.alpha_blend(rgb, a, mask_background)\n",
        "        return img_as_ubyte(overlay)\n",
        "\n",
        "\n",
        "class IO:\n",
        "    \"\"\"I/O & assets (stateful paths are passed in).\"\"\"\n",
        "    @staticmethod\n",
        "    def get_device():\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.set_device(0)\n",
        "            dev = torch.device(\"cuda:0\")\n",
        "            print(f\"[INFO] Using CUDA: {dev}.\")\n",
        "        else:\n",
        "            dev = torch.device(\"cpu\")\n",
        "            print(\"[INFO] Using CPU.\")\n",
        "        return dev\n",
        "\n",
        "    @staticmethod\n",
        "    def _download_file(url, dest_path):\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            with open(dest_path, \"wb\") as f:\n",
        "                shutil.copyfileobj(r.raw, f)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_cad_model_files(assets_dir: str):\n",
        "        os.makedirs(assets_dir, exist_ok=True)\n",
        "        # (unchanged URLs)\n",
        "        files = {\n",
        "            \"final_obj\": (\"https://www.dropbox.com/scl/fi/kg5zutt2deamc4ksi17f7/final_model.obj?rlkey=3uenujpffgqk1t0edx5plj5oe&dl=1\",\n",
        "                          os.path.join(assets_dir, \"final_model.obj\")),\n",
        "            \"detailed_ply\": (\"https://www.dropbox.com/scl/fi/ubit9vw23fphcwpu0w88v/obj_000001.ply?rlkey=hx9f12a9ka8160mhxq8vnz7yk&dl=1\",\n",
        "                             os.path.join(assets_dir, \"obj_000001.ply\")),\n",
        "            \"cad_ply\": (\"https://www.dropbox.com/scl/fi/y55irvvd58yw4ucu3igup/sse_only_jun9_scaled100.ply?rlkey=6io3u20qpupumdfp18i1ssmr9&dl=1\",\n",
        "                        os.path.join(assets_dir, \"sse_only_jun9_scaled100.ply\")),\n",
        "            \"cad_tex\": (\"https://www.dropbox.com/scl/fi/rj1thjtu16o2344vh5mjn/sse_texture.png?rlkey=503sm6dwzv1f8gyy1jgg6tnxl&dl=1\",\n",
        "                        os.path.join(assets_dir, \"sse_texture.png\")),\n",
        "            \"sse_obj\": (\"https://www.dropbox.com/scl/fi/nwfswb9yqaox5guzxedyp/sse_only.obj?rlkey=ub48myfosw690btzg7gfoxert&dl=1\",\n",
        "                        os.path.join(assets_dir, \"sse_only.obj\")),\n",
        "            \"sse_mtl\": (\"https://www.dropbox.com/scl/fi/3mz2zgppszxj6cvv2mgqm/sse_only.mtl?rlkey=ckrw1ko30dk9d8unvsl659vmr&dl=1\",\n",
        "                        os.path.join(assets_dir, \"sse_only.mtl\")),\n",
        "            \"sse_rt_obj\": (\"https://www.dropbox.com/scl/fi/8oo2ofzz6gtxn3ve2ue6r/sse_only_real_texture.obj?rlkey=a13utsbxbzib2c61j5gcasblt&dl=1\",\n",
        "                           os.path.join(assets_dir, \"sse_only_real_texture.obj\")),\n",
        "            \"sse_rt_mtl\": (\"https://www.dropbox.com/scl/fi/e9ld55c4cyq4e6bguwti6/sse_only_real_texture.mtl?rlkey=0lvgesvk9yoi245gndcw0arqm&dl=1\",\n",
        "                           os.path.join(assets_dir, \"sse_only_real_texture.mtl\")),\n",
        "            \"img_00002\": (\"https://www.dropbox.com/scl/fi/b4bjgg4w6n6c6ufx1krhg/00002.png?rlkey=2h6yikdk1liz4b0ojpw4dcz50&dl=1\",\n",
        "                          os.path.join(assets_dir, \"00002.png\")),\n",
        "            \"img_00016\": (\"https://www.dropbox.com/scl/fi/0nf86esq8723iirfz35uj/00016.png?rlkey=9yac94rwxj7uppz4oorgntpqz&dl=1\",\n",
        "                          os.path.join(assets_dir, \"00016.png\")),\n",
        "        }\n",
        "        for url, path in files.values():\n",
        "            print(f\"ðŸ“¥ Downloading -> {path}\")\n",
        "            IO._download_file(url, path)\n",
        "        print(f\"âœ… Files in: {assets_dir}\")\n",
        "        return {k: p for k, (_, p) in files.items()}\n",
        "\n",
        "    @staticmethod\n",
        "    def load_cad_mesh(cad_path: str, device: torch.device) -> Meshes:\n",
        "        cad_path = Path(cad_path); ext = cad_path.suffix.lower()\n",
        "        if ext == \".obj\":\n",
        "            verts, faces, _ = load_obj(str(cad_path), load_textures=False)\n",
        "            verts = verts.to(device); faces = faces.verts_idx.to(device)\n",
        "        elif ext == \".ply\":\n",
        "            verts, faces = load_ply(str(cad_path))\n",
        "            verts = (verts / 1000.0).to(device); faces = faces.to(device)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported CAD file: {ext}\")\n",
        "        verts_rgb = torch.tensor([1.0, 1.0, 0.0], device=device).repeat(verts.shape[0], 1)\n",
        "        textures = TexturesVertex(verts_features=verts_rgb[None])\n",
        "        return Meshes(verts=[verts], faces=[faces], textures=textures)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_cad_mesh_with_texture(cad_path: str, device: torch.device) -> Meshes:\n",
        "        return load_objs_as_meshes([str(cad_path)], device=device)\n",
        "\n",
        "\n",
        "class Optim:\n",
        "    \"\"\"Optimization loops (stateless).\"\"\"\n",
        "    @staticmethod\n",
        "    def optimize_camera_pose_best_loss(model, optimizer, mask_background, phong_renderer, writer,\n",
        "                                       num_iter=100, log_every=10, patience=10):\n",
        "        best_loss, wait = float(\"inf\"), 0\n",
        "        final_R, final_T = None, None\n",
        "        loop = tqdm(range(num_iter))\n",
        "        for i in loop:\n",
        "            torch.cuda.empty_cache()\n",
        "            optimizer.zero_grad()\n",
        "            loss, _, _, _ = model()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            cur = float(loss.item())\n",
        "            loop.set_description(f\"Optimizing (loss {cur:.4f})\")\n",
        "            if cur < best_loss:\n",
        "                best_loss, wait = cur, 0\n",
        "                final_R, final_T = Cam.compute_camera_pose_from_center(model.camera_position, device=model.device)\n",
        "            else:\n",
        "                wait += 1\n",
        "                if wait >= patience:\n",
        "                    print(f\"[INFO] Early stopping at step {i}.\")\n",
        "                    break\n",
        "            if i % log_every == 0:\n",
        "                frame = Render.render_overlay(model, phong_renderer, mask_background)\n",
        "                writer.append_data(frame)\n",
        "        writer.close()\n",
        "        print(f\"[RESULT] Best loss: {best_loss:.6f}\")\n",
        "        return final_R, final_T\n",
        "\n",
        "    @staticmethod\n",
        "    def optimize_camera_pose(model, optimizer, mask_background, phong_renderer, writer,\n",
        "                             num_iter=100, log_every=10, patience=10, tolerance=10.0):\n",
        "        best_loss, wait = float(\"inf\"), 0\n",
        "        final_R, final_T = None, None\n",
        "        loop = tqdm(range(num_iter))\n",
        "        for i in loop:\n",
        "            torch.cuda.empty_cache()\n",
        "            optimizer.zero_grad()\n",
        "            loss, im, _, _ = model()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            cur = float(loss.item())\n",
        "            loop.set_description(f\"Optimizing (loss {cur:.4f})\")\n",
        "            if abs(best_loss - cur) < tolerance:\n",
        "                wait += 1\n",
        "                if wait >= patience:\n",
        "                    print(f\"[INFO] Early stopping at step {i}.\")\n",
        "                    break\n",
        "            else:\n",
        "                best_loss, wait = cur, 0\n",
        "            if i % log_every == 0:\n",
        "                R, T = Cam.compute_camera_pose_from_center(model.camera_position, device=model.device)\n",
        "                frame = Render.render_overlay(model, phong_renderer, mask_background)\n",
        "                writer.append_data(frame)\n",
        "                final_R, final_T = R, T\n",
        "        writer.close()\n",
        "        return final_R, final_T\n",
        "\n",
        "\n",
        "# ========= Facade (keeps your current calls working) ==========================\n",
        "\n",
        "class SSE_Util:\n",
        "    \"\"\"\n",
        "    Thin facade that keeps your existing calls intact.\n",
        "    Only path state lives here; everything else delegates to namespaces above.\n",
        "    \"\"\"\n",
        "    def __init__(self, local_path: str):\n",
        "        self.local_path = local_path\n",
        "        self.assets_dir = os.path.join(self.local_path, \"assets/\")\n",
        "\n",
        "    # --- convenience / backwards-compat wrappers ---\n",
        "    create_gif_writer = staticmethod(lambda filepath, duration=0.5: imageio.get_writer(filepath, mode=\"I\", duration=duration))\n",
        "\n",
        "    # Camera\n",
        "    get_camera_position = staticmethod(Cam.get_camera_position)\n",
        "    camera_center_to_dist_elev_azim = staticmethod(Cam.camera_center_to_dist_elev_azim)\n",
        "    get_camera_pose = staticmethod(Cam.get_camera_pose)\n",
        "    add_camera_roll_to_RT = staticmethod(Cam.add_camera_roll_to_RT)\n",
        "    compute_camera_pose = staticmethod(Cam.compute_camera_pose_from_center)\n",
        "\n",
        "    # Image\n",
        "    read_rgb_cutout_black_bg = staticmethod(Img.read_rgb_cutout_black_bg)\n",
        "    center_cutout_rgb_uint8 = staticmethod(Img.center_cutout_rgb_uint8)\n",
        "    crop_center_to_size_uint8 = staticmethod(Img.crop_center_to_size_uint8)\n",
        "    add_alpha_from_black_bg_uint8 = staticmethod(Img.add_alpha_from_black_bg_uint8)\n",
        "    to_float_batched_rgba_white_bg_preserve_alpha = staticmethod(Img.to_float_batched_rgba_white_bg_preserve_alpha)\n",
        "    center_cutout_keep_scale_2 = staticmethod(Img.center_cutout_keep_scale_2)\n",
        "\n",
        "    # Render\n",
        "    get_camera_intrinsics = staticmethod(Render.get_camera_intrinsics)\n",
        "    create_perspective_camera = staticmethod(Render.create_perspective_camera)\n",
        "    create_silhouette_renderer = staticmethod(Render.create_silhouette_renderer)\n",
        "    create_phong_renderer = staticmethod(Render.create_phong_renderer)\n",
        "    render_mesh = staticmethod(Render.render_mesh)\n",
        "    alpha_blend = staticmethod(Render.alpha_blend)\n",
        "    render_overlay = staticmethod(Render.render_overlay)\n",
        "\n",
        "    # IO / assets\n",
        "    get_device = staticmethod(IO.get_device)\n",
        "    _download_file = staticmethod(IO._download_file)\n",
        "    get_cad_model_files = lambda self: IO.get_cad_model_files(self.assets_dir)\n",
        "    load_cad_mesh = staticmethod(IO.load_cad_mesh)\n",
        "    load_cad_mesh_with_texture = staticmethod(IO.load_cad_mesh_with_texture)\n",
        "\n",
        "    # Optimization\n",
        "    optimize_camera_pose_best_loss = staticmethod(Optim.optimize_camera_pose_best_loss)\n",
        "    optimize_camera_pose = staticmethod(Optim.optimize_camera_pose)\n"
      ],
      "metadata": {
        "id": "dlAIIagF_zHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to prepare video data for processing\n",
        "import os, shutil, zipfile\n",
        "\n",
        "def prepare_video_data(local_path: str,\n",
        "                       video_name: str,\n",
        "                       *,\n",
        "                       delete_zips: bool = True,\n",
        "                       overwrite: bool = True,\n",
        "                       verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Prepare video data under <local_path>/video_data.\n",
        "\n",
        "    Looks for these ZIPs in local_path:\n",
        "      - {video_name}.zip           -> extracted to 'images'\n",
        "      - masks_{video_name}.zip     -> extracted to 'masks'\n",
        "      - cutouts_{video_name}.zip   -> extracted to 'cutouts'\n",
        "\n",
        "    Moves found ZIPs to <local_path>/video_data, extracts them there,\n",
        "    renames extracted content to the target dir names, cleans __MACOSX,\n",
        "    and optionally deletes the ZIPs.\n",
        "\n",
        "    Returns:\n",
        "        dict with keys: created (mapping), missing (list),\n",
        "        zips_moved (list), zips_used (list)\n",
        "    \"\"\"\n",
        "    base = os.path.abspath(local_path)\n",
        "    video_data = os.path.join(base, \"video_data\")\n",
        "    os.makedirs(video_data, exist_ok=True)\n",
        "\n",
        "    def vprint(*a, **k):\n",
        "        if verbose:\n",
        "            print(*a, **k)\n",
        "\n",
        "    zip_specs = [\n",
        "        (f\"{video_name}.zip\",          \"images\"),\n",
        "        (f\"masks_{video_name}.zip\",    \"masks\"),\n",
        "        (f\"cutouts_{video_name}.zip\",  \"cutouts\"),\n",
        "    ]\n",
        "\n",
        "    results = {\"created\": {}, \"missing\": [], \"zips_moved\": [], \"zips_used\": []}\n",
        "\n",
        "    for zip_name, target_dirname in zip_specs:\n",
        "        src_zip = os.path.join(base, zip_name)\n",
        "        vd_zip  = os.path.join(video_data, zip_name)\n",
        "\n",
        "        # Pick which ZIP path to use, moving if necessary\n",
        "        if os.path.exists(src_zip):\n",
        "            if os.path.exists(vd_zip) and overwrite:\n",
        "                os.remove(vd_zip)\n",
        "            shutil.move(src_zip, video_data)  # keeps filename\n",
        "            zip_path = vd_zip\n",
        "            results[\"zips_moved\"].append(zip_name)\n",
        "            vprint(f\"Moved: {zip_name} -> video_data/\")\n",
        "        elif os.path.exists(vd_zip):\n",
        "            zip_path = vd_zip\n",
        "            results[\"zips_used\"].append(zip_name)\n",
        "            vprint(f\"Using existing in video_data/: {zip_name}\")\n",
        "        else:\n",
        "            results[\"missing\"].append(zip_name)\n",
        "            vprint(f\"Missing: {zip_name} (skipping)\")\n",
        "            continue\n",
        "\n",
        "        # Track directory contents before extraction\n",
        "        before = set(os.listdir(video_data))\n",
        "\n",
        "        # Extract\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "            zf.extractall(video_data)\n",
        "\n",
        "        # Identify what was created\n",
        "        after = set(os.listdir(video_data))\n",
        "        new_entries = sorted(list(after - before))\n",
        "\n",
        "        # Clean macOS metadata\n",
        "        mac_dir = os.path.join(video_data, \"__MACOSX\")\n",
        "        if os.path.isdir(mac_dir):\n",
        "            shutil.rmtree(mac_dir, ignore_errors=True)\n",
        "            if \"__MACOSX\" in new_entries:\n",
        "                new_entries.remove(\"__MACOSX\")\n",
        "\n",
        "        target_dir = os.path.join(video_data, target_dirname)\n",
        "\n",
        "        # Handle existing target\n",
        "        if os.path.exists(target_dir):\n",
        "            if overwrite:\n",
        "                shutil.rmtree(target_dir)\n",
        "            else:\n",
        "                raise FileExistsError(\n",
        "                    f\"Target directory already exists: {target_dir}. \"\n",
        "                    \"Set overwrite=True to replace.\"\n",
        "                )\n",
        "\n",
        "        # Decide how to move the extracted content into target_dir\n",
        "        new_dirs  = [e for e in new_entries if os.path.isdir(os.path.join(video_data, e))]\n",
        "        new_files = [e for e in new_entries if os.path.isfile(os.path.join(video_data, e))]\n",
        "\n",
        "        if len(new_dirs) == 1 and not new_files:\n",
        "            src_dir = os.path.join(video_data, new_dirs[0])\n",
        "            # If the extracted dir is already named like the target, nothing to move\n",
        "            if os.path.abspath(src_dir) != os.path.abspath(target_dir):\n",
        "                shutil.move(src_dir, target_dir)\n",
        "            else:\n",
        "                vprint(f\"Extracted folder already named '{target_dirname}'\")\n",
        "        else:\n",
        "            # Mixed content or flat files: create target and move all new entries into it\n",
        "            os.makedirs(target_dir, exist_ok=True)\n",
        "            for name in new_entries:\n",
        "                p = os.path.join(video_data, name)\n",
        "                # Skip if it was already moved (defensive)\n",
        "                if not os.path.exists(p):\n",
        "                    continue\n",
        "                shutil.move(p, os.path.join(target_dir, os.path.basename(p)))\n",
        "\n",
        "        # Optional: delete the ZIP after successful extraction\n",
        "        if delete_zips:\n",
        "            try:\n",
        "                os.remove(zip_path)\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "\n",
        "        results[\"created\"][target_dirname] = target_dir\n",
        "        vprint(f\"Prepared: {target_dirname} -> {target_dir}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "Q5nPNC7Fntpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "def show_mask_grid(local_path: str,\n",
        "                   pattern: str = r'^\\d{5}\\.png$',\n",
        "                   max_images: int = 20,\n",
        "                   cols: int = 5,\n",
        "                   figsize_cell: float = 2.0,\n",
        "                   show: bool = True):\n",
        "    \"\"\"\n",
        "    Read masks from <local_path>/video_data/masks/ and display as a grid.\n",
        "\n",
        "    Args:\n",
        "        local_path: Base directory (e.g., \"/content/\").\n",
        "        pattern: Regex for mask filenames. Default '00000.png' style.\n",
        "        max_images: Max number of masks to display.\n",
        "        cols: Number of columns in the grid.\n",
        "        figsize_cell: Figure size per cell (inches).\n",
        "        show: If True, calls plt.show().\n",
        "\n",
        "    Returns:\n",
        "        (fig, grid, files_used, masks_np)\n",
        "        - fig: Matplotlib Figure (or None if nothing to show)\n",
        "        - grid: ImageGrid instance (or None)\n",
        "        - files_used: list of filenames shown\n",
        "        - masks_np: list of numpy arrays (grayscale)\n",
        "    \"\"\"\n",
        "    mask_dir = os.path.join(local_path, \"video_data\", \"masks\")\n",
        "    if not os.path.isdir(mask_dir):\n",
        "        print(f\"[show_mask_grid] Directory not found: {mask_dir}\")\n",
        "        return None, None, [], []\n",
        "\n",
        "    rx = re.compile(pattern)\n",
        "    files = [f for f in os.listdir(mask_dir) if rx.match(f)]\n",
        "\n",
        "    # Sort numerically if filenames are like '00012.png'\n",
        "    def _num_key(name):\n",
        "        stem = os.path.splitext(name)[0]\n",
        "        try:\n",
        "            return int(stem)\n",
        "        except ValueError:\n",
        "            return stem  # fallback to string\n",
        "    files = sorted(files, key=_num_key)\n",
        "\n",
        "    if not files:\n",
        "        print(f\"[show_mask_grid] No files matching pattern {pattern} in {mask_dir}\")\n",
        "        return None, None, [], []\n",
        "\n",
        "    # Load grayscale masks\n",
        "    files = files[:max_images]\n",
        "    masks = []\n",
        "    used = []\n",
        "    for f in files:\n",
        "        img = cv2.imread(os.path.join(mask_dir, f), cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            continue\n",
        "        masks.append(img)\n",
        "        used.append(f)\n",
        "\n",
        "    if not masks:\n",
        "        print(\"[show_mask_grid] No masks could be read.\")\n",
        "        return None, None, [], []\n",
        "\n",
        "    num_images = len(masks)\n",
        "    cols = max(1, cols)\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "\n",
        "    fig = plt.figure(figsize=(cols * figsize_cell, rows * figsize_cell))\n",
        "    grid = ImageGrid(fig, 111,\n",
        "                     nrows_ncols=(rows, cols),\n",
        "                     axes_pad=0.0,\n",
        "                     share_all=True)\n",
        "\n",
        "    for ax, img in zip(grid, masks):\n",
        "        ax.imshow(img, cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Turn off any extra axes if grid > num_images\n",
        "    for ax in grid[len(masks):]:\n",
        "        ax.axis('off')\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "    return fig, grid, used, masks"
      ],
      "metadata": {
        "id": "J1ep0EzHtebo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "def show_cutout_grid(local_path: str,\n",
        "                     pattern: str = r'^\\d{5}\\.png$',\n",
        "                     max_images: int = 20,\n",
        "                     cols: int = 5,\n",
        "                     figsize_cell: float = 2.0,\n",
        "                     show: bool = True):\n",
        "    \"\"\"\n",
        "    Read cutout images from <local_path>/video_data/cutouts/ and display as a grid.\n",
        "\n",
        "    Args:\n",
        "        local_path: Base directory (e.g., \"/content/\").\n",
        "        pattern: Regex for filenames (default: '00000.png' style).\n",
        "        max_images: Maximum number of images to display.\n",
        "        cols: Number of columns in the grid.\n",
        "        figsize_cell: Figure size per cell (inches).\n",
        "        show: If True, calls plt.show().\n",
        "\n",
        "    Returns:\n",
        "        (fig, grid, files_used, images_rgb)\n",
        "          - fig: Matplotlib Figure (or None if nothing to show)\n",
        "          - grid: ImageGrid instance (or None)\n",
        "          - files_used: list of filenames shown\n",
        "          - images_rgb: list of numpy arrays in RGB (H, W, 3, uint8)\n",
        "    \"\"\"\n",
        "    cutout_dir = os.path.join(local_path, \"video_data\", \"cutouts\")\n",
        "    if not os.path.isdir(cutout_dir):\n",
        "        print(f\"[show_cutout_grid] Directory not found: {cutout_dir}\")\n",
        "        return None, None, [], []\n",
        "\n",
        "    rx = re.compile(pattern)\n",
        "    files = [f for f in os.listdir(cutout_dir) if rx.match(f)]\n",
        "\n",
        "    # Sort numerically if filenames look like '00012.png'\n",
        "    def _num_key(name):\n",
        "        stem = os.path.splitext(name)[0]\n",
        "        try:\n",
        "            return int(stem)\n",
        "        except ValueError:\n",
        "            return stem\n",
        "    files = sorted(files, key=_num_key)\n",
        "\n",
        "    if not files:\n",
        "        print(f\"[show_cutout_grid] No files matching pattern {pattern} in {cutout_dir}\")\n",
        "        return None, None, [], []\n",
        "\n",
        "    # Load color images (BGR) and convert to RGB\n",
        "    files = files[:max_images]\n",
        "    images_rgb, used = [], []\n",
        "    for f in files:\n",
        "        bgr = cv2.imread(os.path.join(cutout_dir, f), cv2.IMREAD_COLOR)\n",
        "        if bgr is None:\n",
        "            continue\n",
        "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "        images_rgb.append(rgb)\n",
        "        used.append(f)\n",
        "\n",
        "    if not images_rgb:\n",
        "        print(\"[show_cutout_grid] No images could be read.\")\n",
        "        return None, None, [], []\n",
        "\n",
        "    num_images = len(images_rgb)\n",
        "    cols = max(1, cols)\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "\n",
        "    fig = plt.figure(figsize=(cols * figsize_cell, rows * figsize_cell))\n",
        "    grid = ImageGrid(fig, 111, nrows_ncols=(rows, cols), axes_pad=0.0, share_all=True)\n",
        "\n",
        "    for ax, img in zip(grid, images_rgb):\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Hide any extra axes\n",
        "    for ax in grid[len(images_rgb):]:\n",
        "        ax.axis('off')\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "    return fig, grid, used, images_rgb\n"
      ],
      "metadata": {
        "id": "102xwmjxt0Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Optional, Dict, Any\n",
        "\n",
        "def estimate_poses_for_masks(\n",
        "    pose_est,\n",
        "    mask_files: List[str],\n",
        "    local_path: str,\n",
        "    *,\n",
        "    N: Optional[int] = None,\n",
        "    iterations: int = 200,\n",
        "    learning_rate: float = 0.05,\n",
        "    tol: float = 1.0,\n",
        "    show_overlay: bool = True,\n",
        "    print_results: bool = True,\n",
        "    stop_on_error: bool = False,\n",
        "    output_size: tuple,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run pose estimation over a list of mask files.\n",
        "\n",
        "    Args:\n",
        "        pose_est: Your pose estimator object (e.g., pose_est01) exposing:\n",
        "                  set_number_of_iterations, set_learning_rate, set_reference_mask,\n",
        "                  init_model, run_optimization, display_current_and_reference_as_overlay,\n",
        "                  printout_results, and attribute .model.camera_position (Tensor).\n",
        "        mask_files: List of mask file paths (relative to local_path or absolute).\n",
        "        local_path: Base directory containing the mask files (e.g., \"/content/\").\n",
        "        N: If provided, process only the first N files; otherwise process all.\n",
        "        iterations: Number of optimization iterations to set on the estimator.\n",
        "        learning_rate: Learning rate to set and to pass to init_model.\n",
        "        tol: Tolerance value passed to run_optimization(tol=...).\n",
        "        show_overlay: If True, calls display_current_and_reference_as_overlay() per file.\n",
        "        print_results: If True, calls printout_results() per file.\n",
        "        stop_on_error: If True, raises on first file error; otherwise continues.\n",
        "\n",
        "    Returns:\n",
        "        A list of dicts, one per processed file:\n",
        "        [\n",
        "          {\n",
        "            \"file\": \"<filename>\",\n",
        "            \"path\": \"<full_path>\",\n",
        "            \"R\": Tensor (detached clone),\n",
        "            \"T\": Tensor (detached clone),\n",
        "            \"camera_position\": Tensor (detached clone)\n",
        "          },\n",
        "          ...\n",
        "        ]\n",
        "    \"\"\"\n",
        "\n",
        "    selected = mask_files[:N] if (N is not None) else mask_files\n",
        "    results = []\n",
        "\n",
        "    for mask_file in selected:\n",
        "        try:\n",
        "            print(f\"\\nProcessing: {os.path.basename(mask_file)}...\")\n",
        "            image_path = os.path.join(local_path, mask_file)\n",
        "\n",
        "            # Set the current reference mask\n",
        "            # pose_est.set_reference_mask(image_path)\n",
        "            _ = pose_est.read_rgb_cutout_black_bg_from_file(image_path, output_size=output_size)\n",
        "\n",
        "\n",
        "            # Use current camera position as eye_override (batchify)\n",
        "            camera_pos = pose_est.model.camera_position.detach().unsqueeze(0)\n",
        "\n",
        "            # Initialize model for this image\n",
        "            pose_est.init_model(learning_rate=learning_rate, eye_override=camera_pos)\n",
        "\n",
        "            # Optimize\n",
        "            R, T = pose_est.run_optimization(tol=tol)\n",
        "\n",
        "            # Show overlay & print per-image results\n",
        "            if show_overlay:\n",
        "                pose_est.display_current_and_reference_as_overlay()\n",
        "            if print_results:\n",
        "                pose_est.printout_results()\n",
        "\n",
        "            # Snapshot final camera position\n",
        "            camera_position = pose_est.model.camera_position.detach().clone()\n",
        "\n",
        "            results.append({\n",
        "                \"file\": os.path.basename(mask_file),\n",
        "                \"path\": image_path,\n",
        "                \"R\": R.detach().clone(),\n",
        "                \"T\": T.detach().clone(),\n",
        "                \"camera_position\": camera_position\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            msg = f\"[estimate_poses_for_masks] Error processing {mask_file}: {e}\"\n",
        "            if stop_on_error:\n",
        "                raise RuntimeError(msg) from e\n",
        "            else:\n",
        "                print(msg)\n",
        "                continue\n",
        "\n",
        "    print(\"\\nDone. Collected results for\", len(results), \"file(s).\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "Q7aoj0OcxzBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_pose_gif(\n",
        "    pose_est,\n",
        "    results,\n",
        "    out_path: str = \"./result.gif\",\n",
        "    duration: float = 0.5,          # seconds per frame\n",
        "    apply_alpha: bool = True,\n",
        "    preview: bool = False,\n",
        "    verbose: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Create an animated GIF by re-rendering frames at the camera positions in `results`.\n",
        "\n",
        "    Args:\n",
        "        pose_est: your estimator (e.g., pose_est01) exposing render_sse_image(eye_override=pos)\n",
        "        results: list of tuples (R, T, pos) OR dicts with key \"camera_position\" (or \"pos\")\n",
        "        out_path: where to save the GIF\n",
        "        duration: per-frame duration in seconds\n",
        "        apply_alpha: if True and frames are RGBA, multiply RGB by alpha\n",
        "        preview: if True, show each frame with matplotlib (slow)\n",
        "        verbose: log progress\n",
        "\n",
        "    Returns:\n",
        "        (out_path, num_frames)\n",
        "    \"\"\"\n",
        "\n",
        "    def _get_pos(entry):\n",
        "        # tuple (R, T, pos)\n",
        "        if isinstance(entry, (tuple, list)) and len(entry) >= 3:\n",
        "            return entry[2]\n",
        "        # dict with camera position\n",
        "        if isinstance(entry, dict):\n",
        "            if \"camera_position\" in entry:\n",
        "                return entry[\"camera_position\"]\n",
        "            if \"pos\" in entry:\n",
        "                return entry[\"pos\"]\n",
        "        raise ValueError(\"Unrecognized results entry format. Expected (R,T,pos) or dict with 'camera_position'/'pos'.\")\n",
        "\n",
        "    def _to_numpy(arr_or_tensor):\n",
        "        # torch tensor -> numpy\n",
        "        try:\n",
        "            import torch\n",
        "            if isinstance(arr_or_tensor, torch.Tensor):\n",
        "                return arr_or_tensor.detach().cpu().numpy()\n",
        "        except Exception:\n",
        "            pass\n",
        "        return np.asarray(arr_or_tensor)\n",
        "\n",
        "    def _ensure_ch_last(img):\n",
        "        # Move (C,H,W) -> (H,W,C)\n",
        "        if img.ndim == 3 and img.shape[0] in (1,3,4) and img.shape[-1] not in (1,3,4):\n",
        "            img = np.transpose(img, (1,2,0))\n",
        "        return img\n",
        "\n",
        "    def _rgba_to_rgb_uint8(img_rgba):\n",
        "        rgb = img_rgba[..., :3]\n",
        "        if not apply_alpha or img_rgba.shape[-1] < 4:\n",
        "            return _to_uint8(rgb)\n",
        "        alpha = img_rgba[..., 3]\n",
        "        # normalize alpha if needed\n",
        "        if alpha.max() > 1.0 + 1e-6:\n",
        "            alpha = alpha / 255.0\n",
        "        rgb = rgb * alpha[..., None]\n",
        "        return _to_uint8(rgb)\n",
        "\n",
        "    def _to_uint8(img):\n",
        "        img = np.asarray(img)\n",
        "        if np.issubdtype(img.dtype, np.floating):\n",
        "            if img.max() <= 1.0 + 1e-6 and img.min() >= -1e-6:\n",
        "                img = (np.clip(img, 0.0, 1.0) * 255.0).round()\n",
        "            else:\n",
        "                img = np.clip(img, 0.0, 255.0).round()\n",
        "            img = img.astype(np.uint8)\n",
        "        elif img.dtype != np.uint8:\n",
        "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "        return img\n",
        "\n",
        "    os.makedirs(os.path.dirname(os.path.abspath(out_path)) or \".\", exist_ok=True)\n",
        "    frame_count = 0\n",
        "\n",
        "    with imageio.get_writer(out_path, mode=\"I\", duration=duration, loop=0) as writer:\n",
        "        for i, entry in enumerate(results):\n",
        "            pos = _get_pos(entry)\n",
        "\n",
        "            # Many setups store pos as torch tensor; allow shape (3,) or (1,3)\n",
        "            try:\n",
        "                import torch\n",
        "                if isinstance(pos, torch.Tensor):\n",
        "                    pos_np = pos.detach().cpu()\n",
        "                    # Leave shape as provided; your renderer already accepts (1,3) or (3,)\n",
        "                    eye_override = pos_np\n",
        "                else:\n",
        "                    eye_override = pos\n",
        "            except Exception:\n",
        "                eye_override = pos\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"[INFO] Rendering frame {i}...\")\n",
        "\n",
        "            # Render with the given camera position\n",
        "            frame = pose_est.render_sse_image(eye_override=eye_override)\n",
        "\n",
        "            # Convert to numpy, channel-last\n",
        "            frame = _ensure_ch_last(_to_numpy(frame))\n",
        "\n",
        "            # If RGBA, apply alpha; otherwise just convert\n",
        "            if frame.ndim == 3 and frame.shape[-1] == 4:\n",
        "                rgb_uint8 = _rgba_to_rgb_uint8(frame)\n",
        "            else:\n",
        "                rgb_uint8 = _to_uint8(frame)\n",
        "\n",
        "            writer.append_data(rgb_uint8)\n",
        "            frame_count += 1\n",
        "\n",
        "            if preview:\n",
        "                plt.imshow(rgb_uint8)\n",
        "                plt.axis('off')\n",
        "                plt.title(f\"Frame {i}\")\n",
        "                plt.show()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[INFO] GIF saved to {out_path} with {frame_count} frame(s).\")\n",
        "    return out_path, frame_count\n"
      ],
      "metadata": {
        "id": "xmX7x72E0VRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plBJwEslrQdt"
      },
      "source": [
        "### Set up a basic model\n",
        "\n",
        "The model class and initialize a parameter for the camera position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBbP1-EDrQdu"
      },
      "outputs": [],
      "source": [
        "# class Model(nn.Module):\n",
        "#     def __init__(self, meshes, renderer, image_ref, initial_position):\n",
        "#         super().__init__()\n",
        "#         self.meshes = meshes\n",
        "#         self.device = meshes.device\n",
        "#         self.renderer = renderer\n",
        "#         self.initial_position = initial_position\n",
        "\n",
        "#         # Get the silhouette of the reference RGB image by finding all non-white pixel values.\n",
        "#         image_ref = torch.from_numpy((image_ref[..., :3].max(-1) != 1).astype(np.float32))\n",
        "#         # image_ref = (image_ref[..., :3].amax(dim=-1) != 1.0).float()\n",
        "\n",
        "#         self.register_buffer('image_ref', image_ref)\n",
        "\n",
        "#         # Create an optimizable parameter for the x, y, z position of the camera.\n",
        "#         # This is the (x,y,z)_world camera translation vector\n",
        "#         self.camera_position = nn.Parameter(\n",
        "#             torch.from_numpy(initial_position).to(meshes.device)\n",
        "#             )\n",
        "\n",
        "#     def forward(self):\n",
        "\n",
        "#         # Render the image using the updated camera position. Based on the new position of the\n",
        "#         # camera we calculate the rotation and translation matrices.\n",
        "#         R = look_at_rotation(self.camera_position[None, :], device=self.device)  # (1, 3, 3)\n",
        "#         T = -torch.bmm(R.transpose(1, 2), self.camera_position[None, :, None])[:, :, 0]   # (1, 3)\n",
        "\n",
        "#         image = self.renderer(meshes_world=self.meshes.clone(), R=R, T=T)\n",
        "\n",
        "#         # Convert image to silhouette (just a mask here)\n",
        "#         image = image[..., 3]\n",
        "\n",
        "#         # Loss function\n",
        "#         # loss = torch.sum((image - self.image_ref) ** 2)\n",
        "\n",
        "#         loss1 = torch.sum((image - self.image_ref) ** 2)\n",
        "\n",
        "#         loss2 = self.dice_loss(image, self.image_ref)\n",
        "\n",
        "#         loss = 0.5 *loss1 + 0.5 * loss2\n",
        "\n",
        "#         return loss, image, R, T\n",
        "\n",
        "\n",
        "#     def dice_loss(self, pred, target, eps=1e-6):\n",
        "#         pred_bin = (pred > 0.5).float()\n",
        "#         target_bin = (target > 0.5).float()\n",
        "\n",
        "#         intersection = (pred_bin * target_bin).sum()\n",
        "#         union = pred_bin.sum() + target_bin.sum()\n",
        "\n",
        "#         dice = (2. * intersection + eps) / (union + eps)\n",
        "#         return 1. - dice\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New model with roll angle"
      ],
      "metadata": {
        "id": "Do-dpbEFpodu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from pytorch3d.renderer.cameras import look_at_rotation\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        meshes,\n",
        "        renderer,\n",
        "        image_ref,              # numpy or torch; HxW (mask) or HxWx3/4 (RGB/A)\n",
        "        initial_position,       # (3,)\n",
        "        optimize_roll: bool = True,\n",
        "        roll_init_deg: float = 0.0,\n",
        "        white_thresh: float = 0.99,   # for RGB refs: non-white => foreground\n",
        "        auto_resize_ref: bool = True, # resize ref mask to render size\n",
        "        local_path: str = \"/content/\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.meshes = meshes\n",
        "        self.device = meshes.device\n",
        "        self.renderer = renderer\n",
        "        self.white_thresh = white_thresh\n",
        "        self.auto_resize_ref = auto_resize_ref\n",
        "\n",
        "        self.util = SSE_Util(local_path)\n",
        "\n",
        "        # ---- Build a 2D silhouette reference (H, W) in [0,1]\n",
        "        ref = torch.as_tensor(image_ref).to(torch.float32)\n",
        "        if ref.max() > 1.0 + 1e-6:\n",
        "            ref = ref / 255.0\n",
        "        # If a batch dim sneaked in, squeeze it (e.g., (1,H,W,4))\n",
        "        if ref.ndim == 4 and ref.shape[0] == 1:\n",
        "            ref = ref[0]\n",
        "        # If CHW, permute to HWC\n",
        "        if ref.ndim == 3 and ref.shape[0] in (1, 3, 4) and ref.shape[-1] not in (1, 3, 4):\n",
        "            ref = ref.permute(1, 2, 0)\n",
        "\n",
        "        if ref.ndim == 2:\n",
        "            sil_ref = ref.clamp(0, 1)\n",
        "        elif ref.ndim == 3 and ref.shape[-1] == 4:\n",
        "            sil_ref = ref[..., 3].clamp(0, 1)                             # use alpha\n",
        "        elif ref.ndim == 3 and ref.shape[-1] == 3:\n",
        "            sil_ref = (ref.max(dim=-1).values < self.white_thresh).float() # non-white\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported image_ref shape {tuple(ref.shape)}\")\n",
        "\n",
        "\n",
        "\n",
        "        self.register_buffer(\"image_ref\", sil_ref.to(self.device))   # (H,W) float\n",
        "\n",
        "\n",
        "        # ---- Learnable camera translation C = (x,y,z)\n",
        "        C0 = torch.as_tensor(initial_position, dtype=torch.float32, device=self.device)\n",
        "        if C0.ndim == 2 and C0.shape[0] == 1:\n",
        "            C0 = C0[0]\n",
        "        self.camera_position = nn.Parameter(C0)\n",
        "\n",
        "        # ---- Learnable camera roll (deg) around camera Z\n",
        "        roll0 = torch.tensor(roll_init_deg, dtype=torch.float32, device=self.device)\n",
        "        self.roll_deg = nn.Parameter(roll0) if optimize_roll else None\n",
        "\n",
        "\n",
        "\n",
        "    def _roll_Rz_cam(self, deg: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"(1,3,3) rotation about camera Z by 'deg' degrees (pre-multiplies world->cam).\"\"\"\n",
        "        theta = torch.deg2rad(deg.view(1))\n",
        "        c, s = torch.cos(theta), torch.sin(theta)\n",
        "        z = torch.zeros_like(c); o = torch.ones_like(c)\n",
        "        Rz = torch.stack([\n",
        "            torch.stack([ c, -s, z], dim=-1),\n",
        "            torch.stack([ s,  c, z], dim=-1),\n",
        "            torch.stack([ z,  z,  o], dim=-1),\n",
        "        ], dim=1)  # (1,3,3)\n",
        "        return Rz\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        C = self.camera_position\n",
        "\n",
        "        # Get Rotation and Translation from camera position\n",
        "        R, T = self.util.get_camera_pose(eye_override=C, device=self.device)\n",
        "\n",
        "\n",
        "        # Add roll (try mode=\"camera\" first; if no visible spin, try mode=\"world\")\n",
        "        R, T = self.util.add_camera_roll_to_RT(R, T, roll_deg=self.roll_deg, device=self.device, mode=\"world\")\n",
        "\n",
        "\n",
        "\n",
        "        # R0 = look_at_rotation(C[None, :], device=self.device)  # (1,3,3)\n",
        "\n",
        "        # # Camera roll in camera coords: R = R_roll @ R0\n",
        "        # if self.roll_deg is not None:\n",
        "        #     Rroll = self._roll_Rz_cam(self.roll_deg)\n",
        "        #     R = torch.bmm(Rroll, R0)\n",
        "        # else:\n",
        "        #     R = R0\n",
        "\n",
        "        # # T = -R^T * C\n",
        "        # T = -torch.bmm(R.transpose(1, 2), C[None, :, None])[:, :, 0]  # (1,3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Render -> RGBA, take alpha as silhouette\n",
        "        rgba = self.renderer(meshes_world=self.meshes.clone(), R=R, T=T)\n",
        "        if rgba.ndim == 4:\n",
        "            rgba = rgba[0]                 # (H,W,4)\n",
        "        pred = rgba[..., 3].clamp(0, 1)    # (H,W)\n",
        "\n",
        "        # Match ref size if needed\n",
        "        ref = self.image_ref\n",
        "        if self.auto_resize_ref and ref.shape != pred.shape:\n",
        "            ref = F.interpolate(ref[None, None, ...], size=pred.shape, mode=\"nearest\")[0, 0]\n",
        "\n",
        "        # Loss: MSE + (thresholded) Dice, same as your original intent\n",
        "        loss1 = F.mse_loss(pred, ref)\n",
        "        loss2 = self.dice_loss(pred, ref)\n",
        "        loss = 0.5 * loss1 + 0.5 * loss2\n",
        "\n",
        "        return loss, pred, R, T\n",
        "\n",
        "    ## @staticmethod\n",
        "    # def dice_loss(pred, target, eps=1e-6):\n",
        "    #     pred_bin = (pred > 0.5).float()\n",
        "    #     target_bin = (target > 0.5).float()\n",
        "    #     inter = (pred_bin * target_bin).sum()\n",
        "    #     denom = pred_bin.sum() + target_bin.sum()\n",
        "    #     return 1.0 - (2.0 * inter + eps) / (denom + eps)\n",
        "\n",
        "    def dice_loss(self, pred, target, eps=1e-6):\n",
        "        pred_bin = (pred > 0.5).float()\n",
        "        target_bin = (target > 0.5).float()\n",
        "\n",
        "        intersection = (pred_bin * target_bin).sum()\n",
        "        union = pred_bin.sum() + target_bin.sum()\n",
        "\n",
        "        dice = (2. * intersection + eps) / (union + eps)\n",
        "        return 1. - dice\n",
        "\n"
      ],
      "metadata": {
        "id": "dOoYYvkupnlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JHWcJQe84FL"
      },
      "source": [
        "### Pose-Estimator Class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from pytorch3d.structures import Meshes\n",
        "\n",
        "class PoseEstimator:\n",
        "    def __init__(self, local_path, use_light_model=True, image_size=(480, 720), scale_factor=1, num_iter=50):\n",
        "\n",
        "        # Local path of server or local computer (e.g., /content/ for Colab)\n",
        "        self.local_path = local_path\n",
        "\n",
        "        # True if using the light-weight model\n",
        "        self.use_light_model = use_light_model\n",
        "\n",
        "        # Scale factor to resize image and intrinsics calibration matrix\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "        # (Re-scaled) image size (probably it will eventually become a square image, e.g., 256x256)\n",
        "        self.im_size = (int(image_size[0] * scale_factor), int(image_size[1] * scale_factor))\n",
        "\n",
        "        # Maximum number of iterations to perform\n",
        "        self.num_iter = num_iter\n",
        "\n",
        "        # PyTorch3D mesh from CAD model. Set later by _load_cad_model()\n",
        "        self.mesh = None\n",
        "\n",
        "        # Model (Will be set later)\n",
        "        self.model = None\n",
        "\n",
        "        # Optimizer (will be set later)\n",
        "        self.optimizer = None\n",
        "\n",
        "        # These are the PyTorch3D renderers used to create images of the CAD model given a camera.\n",
        "        # Set later by _setup_camera_and_renderer()\n",
        "        self.silhouette_renderer = None\n",
        "        self.phong_renderer = None\n",
        "\n",
        "        # Reference silhouette.\n",
        "        self.silhouette_ref = None\n",
        "\n",
        "        # Reference image\n",
        "        self.image_ref = None\n",
        "\n",
        "        # CAD file path\n",
        "        self.cad_file = None\n",
        "\n",
        "        # Camera intrinsics\n",
        "        self.K = None\n",
        "\n",
        "        # Camera position\n",
        "        self.camera_pos = None\n",
        "\n",
        "        # Reference camera pose\n",
        "        self.R_ref = None\n",
        "        self.T_ref = None\n",
        "\n",
        "        # Roll angle\n",
        "        self.roll_deg = None\n",
        "\n",
        "\n",
        "        #-------------------------------------------------\n",
        "\n",
        "        # Declare the utilities object\n",
        "        self.util = SSE_Util(local_path)\n",
        "\n",
        "        # Set the device (i.e., cuda, cpu, and eventually mps)\n",
        "        self.device = self.util.get_device()\n",
        "\n",
        "        # Private method: Load cad models into memory\n",
        "        self._load_cad_model()\n",
        "\n",
        "        # Change center of cad model to its centroid (volume centroid)\n",
        "        self._center_model_to_centroid()\n",
        "\n",
        "        # Private method: Set up camera and some renderers (i.e., Phong, Silhouette)\n",
        "        self._setup_camera_and_renderer()\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def recenter_meshes_to_com(self, meshes: Meshes, method: str = \"surface\"):\n",
        "        \"\"\"\n",
        "        Translate each mesh in a batched PyTorch3D Meshes so its center-of-mass is at the origin.\n",
        "\n",
        "        Args:\n",
        "            meshes: PyTorch3D Meshes (can be a batch).\n",
        "            method: 'vertex' | 'surface' | 'volume'\n",
        "\n",
        "        Returns:\n",
        "            centered_meshes: new Meshes with verts translated so COM == (0,0,0)\n",
        "            translations: (N, 3) tensor with the per-mesh translation that was SUBTRACTED\n",
        "                          (i.e., original_verts - translations -> centered_verts)\n",
        "        \"\"\"\n",
        "        if method not in {\"vertex\", \"surface\", \"volume\"}:\n",
        "            raise ValueError(\"method must be 'vertex', 'surface', or 'volume'\")\n",
        "\n",
        "        verts_list = meshes.verts_list()\n",
        "        faces_list = meshes.faces_list()\n",
        "        device = verts_list[0].device\n",
        "\n",
        "        translations = []\n",
        "        new_verts_list = []\n",
        "\n",
        "        for V, F in zip(verts_list, faces_list):\n",
        "            # V: (Vn, 3) float; F: (Fn, 3) long\n",
        "            if method == \"vertex\":\n",
        "                com = V.mean(0)\n",
        "\n",
        "            else:\n",
        "                tri = V[F]                    # (F, 3, 3): triangle vertices\n",
        "                a, b, c = tri[:, 0, :], tri[:, 1, :], tri[:, 2, :]\n",
        "\n",
        "                if method == \"surface\":\n",
        "                    # Triangle areas and centroids\n",
        "                    ab = b - a\n",
        "                    ac = c - a\n",
        "                    areas = 0.5 * torch.linalg.norm(torch.cross(ab, ac, dim=1), dim=1)  # (F,)\n",
        "                    centroids = (a + b + c) / 3.0                                       # (F, 3)\n",
        "                    A = areas.sum()\n",
        "                    if A.abs() < 1e-12:\n",
        "                        # Degenerate surface -> fall back\n",
        "                        com = V.mean(0)\n",
        "                    else:\n",
        "                        com = (areas[:, None] * centroids).sum(0) / A\n",
        "\n",
        "                elif method == \"volume\":\n",
        "                    # Volume of tetrahedra (0, a, b, c): V = dot(a, cross(b, c)) / 6\n",
        "                    vols = torch.einsum('ij,ij->i', a, torch.cross(b, c, dim=1)) / 6.0   # signed (F,)\n",
        "                    # Tetra centroid = (0 + a + b + c)/4 = (a+b+c)/4\n",
        "                    tet_centroids = (a + b + c) / 4.0                                    # (F, 3)\n",
        "                    Vtot = vols.sum()\n",
        "                    if Vtot.abs() < 1e-12:\n",
        "                        # Likely not closed or inconsistent winding; fall back\n",
        "                        com = V.mean(0)\n",
        "                    else:\n",
        "                        com = (vols[:, None] * tet_centroids).sum(0) / Vtot\n",
        "\n",
        "            translations.append(com)\n",
        "            new_verts_list.append(V - com)   # shift so COM -> origin\n",
        "\n",
        "        translations = torch.stack(translations, dim=0).to(device)\n",
        "\n",
        "        # Rebuild a Meshes; keep textures/materials if present\n",
        "        centered = Meshes(verts=new_verts_list, faces=faces_list, textures=meshes.textures)\n",
        "        return centered, translations\n",
        "\n",
        "\n",
        "    def _center_model_to_centroid(self):\n",
        "        # meshes: a PyTorch3D Meshes (batched or single)\n",
        "        centered_meshes, com = self.recenter_meshes_to_com(self.mesh, method=\"vertex\")\n",
        "        print(\"Translation applied (per mesh):\", com)  # original COM positions\n",
        "\n",
        "        # If you ever want to move it back later:\n",
        "        # original_verts = centered_meshes.verts_list()[i] + com[i]\n",
        "\n",
        "        self.mesh = centered_meshes\n",
        "\n",
        "\n",
        "    def _load_cad_model(self):\n",
        "        \"\"\"\n",
        "        Loads the cad model into memory.\n",
        "        Here, we can select the light model or the detailed model.\n",
        "        \"\"\"\n",
        "        if self.use_light_model:\n",
        "            self.cad_file = self.local_path + \"assets/final_model.obj\"\n",
        "        else:\n",
        "            # cad_file = self.local_path + \"assets/obj_000001.ply\"\n",
        "            self.cad_file = self.local_path + \"assets/sse_only.obj\"\n",
        "            # self.cad_file = self.local_path + \"assets/sse_only_real_texture.obj\"\n",
        "\n",
        "        # self.mesh = self.util.load_cad_mesh(self.cad_file, device=self.device)\n",
        "        self.mesh = self.util.load_cad_mesh_with_texture(self.cad_file, device=self.device)\n",
        "\n",
        "\n",
        "    def set_number_of_iterations(self, num_iter):\n",
        "        \"\"\"\n",
        "        Sets the number of iterations for the optimization.\n",
        "        \"\"\"\n",
        "        self.num_iter = num_iter\n",
        "        print(f\"[INFO]: Number of iterations set to {self.num_iter}\")\n",
        "\n",
        "    def _setup_camera_and_renderer(self):\n",
        "        \"\"\"\n",
        "        Sets up the camera and some renderers (i.e., Phong, Silhouette).\n",
        "        \"\"\"\n",
        "\n",
        "        # Get the intrinsic matrix to be used for rendering.\n",
        "        self.K = self.util.get_camera_intrinsics(scale_factor=self.scale_factor)\n",
        "\n",
        "        # Create the PyTorch3D perspective camera based on the K matrix.\n",
        "        self.cameras = self.util.create_perspective_camera(self.K, image_size=(self.im_size,), device=self.device)\n",
        "\n",
        "        # These are the PyTorch3D renderers used to create images of the CAD model given a camera.\n",
        "        self.silhouette_renderer = self.util.create_silhouette_renderer(self.cameras, self.im_size, self.device)\n",
        "        self.phong_renderer = self.util.create_phong_renderer(self.cameras, self.im_size, self.device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def add_camera_roll(self, R, T, roll_deg, device=None):\n",
        "        \"\"\"\n",
        "        Add a Z-axis camera roll (degrees) to an existing (R,T) in PyTorch3D convention.\n",
        "        Returns R_new, T_new with the same rank as inputs.\n",
        "        \"\"\"\n",
        "        # to tensors\n",
        "        R = torch.as_tensor(R)\n",
        "        T = torch.as_tensor(T)\n",
        "        dev   = device if device is not None else R.device\n",
        "        dtype = R.dtype if R.is_floating_point() else torch.float32\n",
        "        R = R.to(dev, dtype)\n",
        "        T = T.to(dev, dtype)\n",
        "\n",
        "        # ensure batch\n",
        "        unbatched = (R.ndim == 2)  # (3,3) vs (1,3,3)\n",
        "        if unbatched:\n",
        "            R = R[None, ...]   # (1,3,3)\n",
        "            T = T[None, ...]   # (1,3)\n",
        "\n",
        "        # recover camera center C from (R, T):  T = -R^T C  =>  C = -R T\n",
        "        C = -torch.matmul(R, T[..., None]).squeeze(-1)   # (B,3)\n",
        "\n",
        "        # build Rz(roll) about camera Z\n",
        "        theta = torch.tensor(float(roll_deg), dtype=dtype, device=dev)\n",
        "        c, s = torch.cos(torch.deg2rad(theta)), torch.sin(torch.deg2rad(theta))\n",
        "        z = torch.tensor(0.0, dtype=dtype, device=dev)\n",
        "        o = torch.tensor(1.0, dtype=dtype, device=dev)\n",
        "        Rz = torch.stack([\n",
        "            torch.stack([ c, -s, z]),   # [ [c,-s,0],\n",
        "            torch.stack([ s,  c, z]),   #   [s, c,0],\n",
        "            torch.stack([ z,  z,  o])   #   [0, 0,1] ]\n",
        "        ], dim=0)                        # (3,3)\n",
        "        Rz = Rz.expand(R.shape[0], -1, -1)  # (B,3,3)\n",
        "\n",
        "        # compose and recompute T to keep C fixed\n",
        "        R_new = torch.matmul(Rz, R)                                  # (B,3,3)\n",
        "        T_new = -torch.matmul(R_new.transpose(1, 2), C[..., None]).squeeze(-1)  # (B,3)\n",
        "\n",
        "        if unbatched:\n",
        "            R_new = R_new[0]\n",
        "            T_new = T_new[0]\n",
        "        return R_new, T_new\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def create_test_reference_image(self, distance=3, elevation=50, azimuth=-40, roll=0):\n",
        "        \"\"\"\n",
        "        Renders a reference image for testing\n",
        "        \"\"\"\n",
        "\n",
        "        # Get Rotation and Translation from camera position\n",
        "        R, T = self.util.get_camera_pose(distance, elevation, azimuth, device=self.device)\n",
        "\n",
        "\n",
        "        # Add roll (try mode=\"camera\" first; if no visible spin, try mode=\"world\")\n",
        "        R, T = self.util.add_camera_roll_to_RT(R, T, roll_deg=roll, device=self.device, mode=\"world\")\n",
        "\n",
        "\n",
        "        self.R_ref, self.T_ref = R, T\n",
        "\n",
        "        # Render a silhouette and an image of the CAD model to use as reference for tests.\n",
        "        self.silhouette_ref, image_ref = self.util.render_mesh(\n",
        "            self.mesh,\n",
        "            self.silhouette_renderer,\n",
        "            self.phong_renderer,\n",
        "            R, T\n",
        "        )\n",
        "        self.image_ref = image_ref.cpu().numpy()\n",
        "\n",
        "        # Check dimensions\n",
        "        print(self.image_ref.shape)\n",
        "        print(self.silhouette_ref.shape)\n",
        "\n",
        "\n",
        "        return self.image_ref\n",
        "\n",
        "\n",
        "    def create_reference_image_from_cutout(\n",
        "        self,\n",
        "        cutout_path: str,\n",
        "        *,\n",
        "        use_alpha_if_present: bool = True,\n",
        "        white_thresh: float = 0.9,   # used only if we need to infer a mask from RGB\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Load a PNG cutout and prepare:\n",
        "          - self.image_ref: (1, H, W, 4) float32 NumPy in [0,1], composited on WHITE (alpha=1)\n",
        "          - self.silhouette_ref: (1, H, W, 4) float32 Torch on self.device, white FG, alpha=mask\n",
        "\n",
        "        No resizing is performed here.\n",
        "        \"\"\"\n",
        "        import numpy as np\n",
        "        import torch\n",
        "        from PIL import Image\n",
        "\n",
        "        # Load as RGBA\n",
        "        img = Image.open(cutout_path).convert(\"RGBA\")\n",
        "        rgba = np.asarray(img).astype(np.float32) / 255.0          # (H, W, 4)\n",
        "        rgb   = rgba[..., :3]\n",
        "        alpha = rgba[..., 3]\n",
        "\n",
        "        # Build mask (sil) in [0,1]\n",
        "        if use_alpha_if_present and (alpha.max() - alpha.min()) > 1e-6:\n",
        "            sil = alpha.astype(np.float32)\n",
        "        else:\n",
        "            # Infer mask from RGB: treat near-white as foreground\n",
        "            # (If your object isnâ€™t white, swap this to a \"non-black\" test or provide a mask.)\n",
        "            sil = (rgb.max(axis=-1) > white_thresh).astype(np.float32)\n",
        "\n",
        "        # -------- image_ref: composite over WHITE, alpha = 1\n",
        "        rgb_on_white = rgb * sil[..., None] + (1.0 - sil[..., None]) * 1.0\n",
        "        rgba_white = np.concatenate(\n",
        "            [rgb_on_white, np.ones_like(sil, dtype=np.float32)[..., None]], axis=-1\n",
        "        )  # (H, W, 4), alpha=1\n",
        "\n",
        "        self.image_ref = rgba_white[None, ...].astype(np.float32)  # (1, H, W, 4) NumPy\n",
        "\n",
        "        # -------- silhouette_ref: white FG (1,1,1), transparent BG (alpha = mask)\n",
        "        sil_rgb = np.repeat(sil[..., None], 3, axis=-1)            # (H, W, 3)\n",
        "        sil_rgba = np.concatenate([sil_rgb, sil[..., None]], axis=-1).astype(np.float32)  # (H, W, 4)\n",
        "\n",
        "\n",
        "        self.silhouette_ref = torch.from_numpy(sil_rgba[None, ...]).to(self.device)      # (1, H, W, 4)\n",
        "\n",
        "        # self.silhouette_ref = sil_rgba\n",
        "\n",
        "        return self.image_ref\n",
        "\n",
        "\n",
        "    def create_reference_image_from_cutout_old(\n",
        "        self,\n",
        "        cutout_path: str,\n",
        "        *,\n",
        "        use_alpha_if_present: bool = True,\n",
        "        white_thresh: float = 0.9,   # used only if we need to infer a mask from RGB\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Load a PNG cutout and prepare:\n",
        "          - self.image_ref: (1, H, W, 4) float32 in [0,1], with a WHITE background\n",
        "          - self.silhouette_ref: (H, W) float32 torch tensor in [0,1]\n",
        "\n",
        "        No resizing is performed here.\n",
        "        \"\"\"\n",
        "        import numpy as np\n",
        "        import torch\n",
        "        from PIL import Image\n",
        "\n",
        "        # Load RGBA (preserve alpha if present)\n",
        "        img = Image.open(cutout_path).convert(\"RGBA\")\n",
        "        rgba = np.asarray(img).astype(np.float32) / 255.0      # (H, W, 4)\n",
        "        rgb   = rgba[..., :3]\n",
        "        alpha = rgba[..., 3]\n",
        "\n",
        "        # Build silhouette (H, W) in [0,1]\n",
        "        # Prefer alpha if it's informative; otherwise infer: white (foreground) vs black (background)\n",
        "        if use_alpha_if_present and (alpha.max() - alpha.min()) > 1e-6:\n",
        "            sil = alpha.astype(np.float32)\n",
        "        else:\n",
        "            # If there's no useful alpha, assume 'white foreground on black background'\n",
        "            # Foreground where any channel is near white\n",
        "            sil = (rgb.max(axis=-1) > white_thresh).astype(np.float32)\n",
        "\n",
        "        # Compose onto a WHITE background (so background is visibly white even if input was black)\n",
        "        # rgb_out = sil * rgb + (1 - sil) * white\n",
        "        rgb_out = rgb * sil[..., None] + (1.0 - sil[..., None]) * 1.0  # white = 1.0\n",
        "\n",
        "        # Keep an alpha channel; using the silhouette as alpha is often convenient downstream\n",
        "        alpha_out = sil\n",
        "\n",
        "        rgba_out = np.concatenate([rgb_out, alpha_out[..., None]], axis=-1).astype(np.float32)  # (H, W, 4)\n",
        "\n",
        "        # Store with a batch dimension (1, H, W, 4) as requested\n",
        "        self.image_ref = rgba_out[None, ...]\n",
        "        self.silhouette_ref = torch.from_numpy(sil).to(self.device)\n",
        "\n",
        "        return self.image_ref\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def display_test_reference_image(self):\n",
        "        \"\"\"\n",
        "        Displays the reference image for testing\n",
        "        \"\"\"\n",
        "\n",
        "        plt.figure(figsize=(5, 5))\n",
        "\n",
        "\n",
        "        # Reference image is numpy (1, 480, 720, 4). Look into keeping Ref image as a torch tensor\n",
        "        # until we need to plot it.\n",
        "        image_ref_np = self.image_ref.squeeze()\n",
        "        # Plot reference image\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(image_ref_np)\n",
        "        plt.title(\"Phong Rendering\")\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Silhouette is torch tensor torch.Size([1, 480, 720, 4])\n",
        "        silhouette_np = self.silhouette_ref.cpu().numpy().squeeze()[..., 3]\n",
        "        # Plot Silhouette\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(silhouette_np, cmap=\"gray\")  # Alpha channel\n",
        "        plt.title(\"Silhouette (alpha)\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def display_initial_and_reference(self):\n",
        "\n",
        "        plt.figure()\n",
        "\n",
        "        # At the moment, the image returned by model() is just a mask, not the\n",
        "        # rendering. To get the rendering, we need to call the Phong renderer\n",
        "        # explicitly. Maybe, we should not return any image or mask and simply\n",
        "        # recalculate them from R, t, Camera whenever needed.\n",
        "        _, image_init, R, T = self.model()\n",
        "\n",
        "        # Plot initial image\n",
        "        plt.subplot(1, 2, 1)\n",
        "        initial_pose_img = image_init.detach().squeeze().cpu().numpy()\n",
        "        plt.imshow(initial_pose_img, cmap=\"gray\")\n",
        "        plt.grid(False)\n",
        "        plt.title(\"Starting position\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Silhouette is torch tensor torch.Size([1, 480, 720, 4])\n",
        "        silhouette_np = self.silhouette_ref.cpu().numpy().squeeze()[..., 3]\n",
        "        # Plot Silhouette\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(silhouette_np, cmap=\"gray\")  # Alpha channel\n",
        "        plt.title(\"Reference mask\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def display_current_image(self):\n",
        "\n",
        "        plt.figure(figsize=(5, 5))\n",
        "\n",
        "        # At the moment, the image returned by model() is just a mask, not the\n",
        "        # rendering. To get the rendering, we need to call the Phong renderer\n",
        "        # explicitly. Maybe, we should not return any image or mask and simply\n",
        "        # recalculate them from R, t, Camera whenever needed.\n",
        "        _, image_init, R, T = self.model()\n",
        "\n",
        "\n",
        "        # R, T = self.compute_camera_pose(model.camera_position, model.device)\n",
        "\n",
        "        rendered = self.phong_renderer(\n",
        "            meshes_world=self.model.meshes.clone(),\n",
        "            R=R,\n",
        "            T=T\n",
        "        )[0]  # (H, W, 4)\n",
        "\n",
        "\n",
        "        rgb_rendered = rendered[..., :3].detach().cpu().numpy()\n",
        "\n",
        "\n",
        "        # Plot reference image\n",
        "        # plt.subplot(1, 2, 1)\n",
        "        plt.imshow(rgb_rendered)\n",
        "        plt.title(\"Phong Rendering\")\n",
        "        plt.axis('off')\n",
        "        # plt.tight_layout()\n",
        "\n",
        "\n",
        "    def display_image(self, distance=2, elevation=50, azimuth=45, eye_override=None):\n",
        "\n",
        "        plt.figure(figsize=(5, 5))\n",
        "\n",
        "        # At the moment, the image returned by model() is just a mask, not the\n",
        "        # rendering. To get the rendering, we need to call the Phong renderer\n",
        "        # explicitly. Maybe, we should not return any image or mask and simply\n",
        "        # recalculate them from R, t, Camera whenever needed.\n",
        "        # _, image_init, R, T = self.model()\n",
        "\n",
        "\n",
        "        if eye_override is not None:\n",
        "            # Get Rotation and Translation from camera position\n",
        "            R, T = self.util.get_camera_pose(eye_override=eye_override, device=self.device)\n",
        "        else:\n",
        "            # Get Rotation and Translation from camera position\n",
        "            R, T = self.util.get_camera_pose(distance, elevation, azimuth, device=self.device)\n",
        "\n",
        "\n",
        "        rendered = self.phong_renderer(\n",
        "            meshes_world=self.mesh,\n",
        "            R=R,\n",
        "            T=T\n",
        "        )[0]  # (H, W, 4)\n",
        "\n",
        "\n",
        "        rgb_rendered = rendered[..., :3].detach().cpu().numpy()\n",
        "\n",
        "\n",
        "        # Plot reference image\n",
        "        # plt.subplot(1, 2, 1)\n",
        "        plt.imshow(rgb_rendered)\n",
        "        plt.title(\"Phong Rendering\")\n",
        "        plt.axis('off')\n",
        "        # plt.tight_layout()\n",
        "\n",
        "\n",
        "    def render_sse_image(self, distance=2, elevation=0, azimuth=0, eye_override=None):\n",
        "\n",
        "        if eye_override is not None:\n",
        "            # Get Rotation and Translation from camera position\n",
        "            R, T = self.util.get_camera_pose(eye_override=eye_override, device=self.device)\n",
        "        else:\n",
        "            # Get Rotation and Translation from camera position\n",
        "            R, T = self.util.get_camera_pose(distance, elevation, azimuth, device=self.device)\n",
        "\n",
        "\n",
        "        rendered = self.phong_renderer(\n",
        "            meshes_world=self.mesh,\n",
        "            R=R,\n",
        "            T=T\n",
        "        )[0]  # (H, W, 4)\n",
        "\n",
        "\n",
        "        rgb_rendered = rendered[..., :3].detach().cpu().numpy()\n",
        "\n",
        "\n",
        "        return rgb_rendered\n",
        "\n",
        "\n",
        "\n",
        "    def display_grid_of_images(self):\n",
        "\n",
        "        # Set batch size - this is the number of different viewpoints from which we want to render the mesh.\n",
        "        batch_size = 20\n",
        "\n",
        "        # Create a batch of meshes by repeating the cow mesh and associated textures.\n",
        "        # Meshes has a useful `extend` method which allows us do this very easily.\n",
        "        # This also extends the textures.\n",
        "        meshes = self.mesh.extend(batch_size)\n",
        "\n",
        "        # Get a batch of viewing angles.\n",
        "        elev = torch.linspace(0, 180, batch_size)\n",
        "        azim = torch.linspace(-180, 180, batch_size)\n",
        "\n",
        "        # All the cameras helper methods support mixed type inputs and broadcasting. So we can\n",
        "        # view the camera from the same distance and specify dist=2.7 as a float,\n",
        "        # and then specify elevation and azimuth angles for each viewpoint as tensors.\n",
        "        R, T = look_at_view_transform(dist=1.0, elev=elev, azim=azim)\n",
        "\n",
        "\n",
        "\n",
        "        # cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
        "\n",
        "        # Place a point light in front of the object\n",
        "        # lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
        "\n",
        "        # Move the light back in front of the cow which is facing the -z direction.\n",
        "        # lights.location = torch.tensor([[0.0, 0.0, -3.0]], device=device)\n",
        "\n",
        "        # We can pass arbitrary keyword arguments to the rasterizer/shader via the renderer\n",
        "        # so the renderer does not need to be reinitialized if any of the settings change.\n",
        "        # images = renderer(meshes, cameras=cameras, lights=lights)\n",
        "        images = self.phong_renderer(\n",
        "            meshes_world=meshes,\n",
        "            R=R,\n",
        "            T=T\n",
        "        )  # (H, W, 4)\n",
        "\n",
        "        print(images.shape)\n",
        "\n",
        "        # Display grid of images\n",
        "        image_grid(images.cpu().numpy(), rows=4, cols=5, rgb=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def display_current_and_reference_as_overlay(self, show_mask = False):\n",
        "\n",
        "        if show_mask:\n",
        "            sil2 = (self.image_ref[..., :3].max(-1) != 1).astype(np.float32).squeeze()\n",
        "            bg_img = self.util.create_background_from_mask(sil2)\n",
        "            image_uint8 = self.util.render_overlay(self.model, self.phong_renderer, bg_img)\n",
        "        else:\n",
        "            image_uint8 = self.render_current_and_reference_as_overlay()\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(image_uint8)\n",
        "        plt.show()\n",
        "\n",
        "    def get_reference_image(self):\n",
        "        return self.image_ref\n",
        "\n",
        "\n",
        "    def get_reference_silhouette(self):\n",
        "        return self.silhouette_ref\n",
        "\n",
        "\n",
        "    def render_current_and_reference_as_overlay(self):\n",
        "\n",
        "        # sil2 = (self.image_ref[..., :3].max(-1) != 1).astype(np.float32).squeeze()\n",
        "        # bg_img = self.util.create_background_from_mask(sil2)\n",
        "\n",
        "        bg_img = self.image_ref[..., 0:3].squeeze()\n",
        "\n",
        "        image_uint8 = self.util.render_overlay(self.model, self.phong_renderer, bg_img)\n",
        "\n",
        "        return image_uint8\n",
        "\n",
        "\n",
        "\n",
        "    def set_reference_image_from_file(self,\n",
        "                                      image_path,\n",
        "                                      output_size: Optional[Tuple[int, int]] = None,  # (W_out, H_out). If None, keep original canvas size.\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Sets the reference image for the pose alignment (reads the image from a file).\n",
        "        \"\"\"\n",
        "\n",
        "        # No resize or centering\n",
        "        _ = self.create_reference_image_from_cutout(\n",
        "          image_path,\n",
        "          use_alpha_if_present=True,\n",
        "          white_thresh = 0.0)\n",
        "\n",
        "        # This output is in the range [0,1] RGB but no alpha\n",
        "        cutout = self.image_ref\n",
        "\n",
        "        _, H, W, _ = cutout.shape\n",
        "        if output_size is None:\n",
        "            out_w, out_h = W, H\n",
        "        else:\n",
        "            out_w, out_h = int(output_size[0]), int(output_size[1])\n",
        "\n",
        "        # cutout: (1,480,720,4) or (480,720,4) uint8 / float\n",
        "\n",
        "        print(\"ref image from inside read cutout function:\", cutout.shape)\n",
        "\n",
        "        # # centering and resize\n",
        "        # centered_rgba, info = self.util.center_cutout_keep_scale(\n",
        "        #     cutout_rgba=cutout,\n",
        "        #     output_size=(out_w, out_h),   # (width,height) keep same size; or change to e.g. (800, 600)\n",
        "        #     pad=20,\n",
        "        #     overflow=\"shrink\",         # or \"crop\"/\"error\"\n",
        "        #     return_transform=True\n",
        "        # )\n",
        "\n",
        "\n",
        "        # centered_rgba, centered_mask = self.util.center_cutout_keep_scale_2(\n",
        "        #     cutout_rgba=cutout,\n",
        "        #     output_size=(out_w, out_h),\n",
        "        #     pad=20,\n",
        "        #     overflow=\"shrink\",\n",
        "        #     return_transform=True)\n",
        "\n",
        "\n",
        "\n",
        "        self.image_ref = cutout\n",
        "        self.silhouette_ref = cutout\n",
        "\n",
        "\n",
        "\n",
        "    def read_rgb_cutout_black_bg_from_file(self, image_path: str,\n",
        "                                          output_size: tuple[int, int]) -> np.ndarray:\n",
        "\n",
        "        # Read cutout\n",
        "        cutout = self.util.read_rgb_cutout_black_bg(image_path)  # (H,W,3) float32 in [0,1]\n",
        "\n",
        "        # Centers the cutout\n",
        "        centered_cutout = self.util.center_cutout_rgb_uint8(cutout)  # -> (H, W, 3) uint8\n",
        "\n",
        "        # Crop the centered cutout preserving the scale\n",
        "        crop_cutout = self.util.crop_center_to_size_uint8(centered_cutout, out_size=output_size)\n",
        "\n",
        "        # cropped is (H, W, 3) uint8\n",
        "        rgba = self.util.add_alpha_from_black_bg_uint8(crop_cutout, black_thresh=0, mode=\"binary\")  # -> (H, W, 4) uint8\n",
        "\n",
        "        # img: (H,W,3) or (H,W,4), any numeric dtype\n",
        "        batched_rgba = self.util.to_float_batched_rgba_white_bg_preserve_alpha(rgba)  # -> (1,H,W,4) float in [0,1]\n",
        "\n",
        "        # Set ref_image\n",
        "        self.image_ref = batched_rgba\n",
        "\n",
        "\n",
        "        return batched_rgba\n",
        "\n",
        "\n",
        "\n",
        "    def set_reference_mask(self, image_path):\n",
        "        \"\"\"\n",
        "        Sets the reference mask for the pose alignment (reads the mask from a file).\n",
        "        \"\"\"\n",
        "\n",
        "        # Load mask image\n",
        "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            raise IOError(f\"Failed to load image: {image_path}\")\n",
        "\n",
        "        # Threshold mask to make into (0,255) range\n",
        "        _, binary_mask = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        self.prepare_mask_from_numpy(binary_mask)\n",
        "\n",
        "\n",
        "    def prepare_mask_from_numpy(self, mask_np):\n",
        "\n",
        "        from PIL import Image\n",
        "        import numpy as np\n",
        "\n",
        "\n",
        "        # mask_centered = self.util.center_mask_in_image_from_array(mask_np)[1]\n",
        "\n",
        "        mask_centered = self.util.center_mask_preserve_ratio(mask_np, output_size=(256,256))[1]\n",
        "\n",
        "        # _, mask_centered = self.util.center_mask_keep_scale(mask_np, output_size=(256, 256), overflow=\"crop\", pad=8)\n",
        "\n",
        "\n",
        "        mask_resized = cv2.resize(mask_centered, self.im_size[::-1], interpolation=cv2.INTER_LANCZOS4)\n",
        "        mask_rgb = np.stack([mask_resized]*4, axis=-1)[None, ...]  # (1, H, W, 4)\n",
        "        mask_rgb = 255 - mask_rgb\n",
        "        self.image_ref = mask_rgb.astype(np.float32) / 255.0\n",
        "\n",
        "        self.silhouette_ref = mask_rgb.astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "    def init_model(self, distance=2, elevation=110, azimuth=50, roll_deg=0.0, learning_rate=0.05, eye_override=None):\n",
        "        \"\"\"\n",
        "        Initializes the model with a given camera position.\n",
        "        \"\"\"\n",
        "\n",
        "        if eye_override is not None:\n",
        "            # Set the camera's 3-D location from the input location (3-D position)\n",
        "            self.camera_pos = eye_override\n",
        "            print(f\"  Camera position (x,y,z): = {self.camera_pos.cpu().numpy()[0]}\")\n",
        "        else:\n",
        "            # Set the camera's 3-D location from the input spherical angles\n",
        "            self.camera_pos = self.util.get_camera_position(distance, elevation, azimuth, device=self.device)\n",
        "            print(f\"  distance = {distance}, elevation = {elevation}, azimuth = {azimuth}\")\n",
        "\n",
        "\n",
        "        # Initialize the model\n",
        "        self.model = Model(\n",
        "            meshes=self.mesh,\n",
        "            renderer=self.silhouette_renderer,\n",
        "            image_ref=self.image_ref,\n",
        "            initial_position=self.camera_pos.cpu().numpy()[0],\n",
        "            roll_init_deg = roll_deg\n",
        "            # camera_pos=self.camera_pos.detach().cpu().numpy()[0],\n",
        "            # device=self.device\n",
        "        ).to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Set the optimizer\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate\n",
        "            )\n",
        "\n",
        "\n",
        "    def set_learning_rate(self, new_lr):\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = new_lr\n",
        "        print(f\"[INFO]: Learning rate set to {new_lr}\")\n",
        "\n",
        "\n",
        "    def run_optimization(self, tol = 0.1, show_mask=False):\n",
        "\n",
        "\n",
        "        # print(\"Shape image_ref before conversion to silhouette:\", self.image_ref.shape)\n",
        "\n",
        "        if show_mask:\n",
        "            # Make a mask out of image_ref\n",
        "            sil2 = (self.image_ref[..., :3].max(-1) != 1).astype(np.float32).squeeze()\n",
        "            bg_img = self.util.create_background_from_mask(sil2)\n",
        "        else:\n",
        "            bg_img = self.image_ref[..., 0:3].squeeze()\n",
        "\n",
        "        # print(\"Shape image_ref after conversion to silhouette:\", bg_img.shape)\n",
        "        # print(\"This is the bg_mask that is passed to util.optimize_camera_pose_best_loss()\",)\n",
        "        # plt.figure()\n",
        "        # plt.imshow(bg_img)\n",
        "        # plt.show()\n",
        "\n",
        "\n",
        "\n",
        "        self.writer = imageio.get_writer(\"./optimization_sequence.gif\", mode='I', duration=0.5)\n",
        "\n",
        "        # self.R, self.T = self.util.optimize_camera_pose(\n",
        "        self.R, self.T = self.util.optimize_camera_pose_best_loss(\n",
        "            model=self.model,\n",
        "            optimizer=self.optimizer,\n",
        "            mask_background=bg_img,\n",
        "            phong_renderer=self.phong_renderer,\n",
        "            writer=self.writer,\n",
        "            num_iter=self.num_iter,\n",
        "            log_every=10,\n",
        "            patience=10\n",
        "            )\n",
        "        return self.R, self.T\n",
        "\n",
        "    def get_results(self):\n",
        "        return self.R, self.T, self.model.camera_position\n",
        "\n",
        "\n",
        "    def printout_results(self):\n",
        "        print(\"\\nCam position from model =\\n:\", self.model.camera_position.detach().cpu().numpy())\n",
        "\n",
        "        # Convert to NumPy\n",
        "        R = self.R.detach().cpu().numpy()\n",
        "        T = self.T.detach().cpu().numpy()\n",
        "\n",
        "        # Print results\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print(\"\\nEstimated Rotation Matrix (R):\")\n",
        "        print(R)\n",
        "\n",
        "        print(\"\\nEstimated Translation Vector (T):\")\n",
        "        print(T.reshape(-1))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9QLY_y8eYG-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradio UI function for pose selection\n",
        "Call this function whenever manual pose selection is needed."
      ],
      "metadata": {
        "id": "V0lXNs_VG4mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def gradio_UI_select_pose(pose_estimator):\n",
        "\n",
        "#     import gradio as gr\n",
        "#     from PIL import Image\n",
        "#     import numpy as np\n",
        "#     import threading\n",
        "\n",
        "\n",
        "#     def render_pose_image(distance, azimuth, elevation, roll):\n",
        "#         try:\n",
        "#             # Update the model with new pose\n",
        "#             pose_estimator.init_model(distance=distance, elevation=elevation, azimuth=azimuth, roll_deg=float(roll), learning_rate=0.0)\n",
        "\n",
        "#             # Display the new overlay image\n",
        "#             overlay_img = pose_estimator.render_current_and_reference_as_overlay()\n",
        "\n",
        "#             # Convert to PIL if needed\n",
        "#             if isinstance(overlay_img, np.ndarray):\n",
        "#                 if overlay_img.dtype in [np.float32, np.float64]:\n",
        "#                     overlay_img = (overlay_img * 255).clip(0, 255).astype(np.uint8)\n",
        "#                 image_pil = Image.fromarray(overlay_img)\n",
        "#                 return image_pil\n",
        "#             else:\n",
        "#                 return overlay_img  # If already PIL Image\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error rendering overlay: {e}\")\n",
        "#             return None\n",
        "\n",
        "\n",
        "#     app = None  # launch handle is stored here\n",
        "\n",
        "#     def on_done():\n",
        "#         def _close():\n",
        "#             try:\n",
        "#                 app.close()\n",
        "#             except Exception as e:\n",
        "#                 print(\"Close error:\", e)\n",
        "#         threading.Thread(target=_close, daemon=True).start()\n",
        "#         return \"âœ… Closing the interfaceâ€¦\"\n",
        "\n",
        "#     # Gradio UI\n",
        "\n",
        "\n",
        "#     with gr.Blocks(title=\"CAD Pose Renderer\") as demo:\n",
        "#         gr.Markdown(\"Adjust the camera pose to render the CAD model. Click **Done** to stop the app.\")\n",
        "#         with gr.Row():\n",
        "#             distance  = gr.Slider(minimum=0.5, maximum=20.0, step=0.1, value=6, label=\"Distance\")\n",
        "#             elevation = gr.Slider(minimum=-89.0, maximum=89.0, step=1, value=0, label=\"Elevation\")\n",
        "#             azimuth   = gr.Slider(minimum=-179.0, maximum=180.0, step=1, value=0, label=\"Azimuth\")\n",
        "#             roll      = gr.Slider(minimum=-179.0, maximum=180.0, step=1, value=0, label=\"Roll\")\n",
        "#             # done_btn = gr.Button(\"âœ… Done\", variant=\"primary\")\n",
        "#         out_img = gr.Image(type=\"pil\", label=\"Rendered Image\")\n",
        "#         status  = gr.Markdown(\"\")\n",
        "\n",
        "#         # Update image whenever a slider changes (use a button instead if you prefer manual updates)\n",
        "#         distance.change(render_pose_image, [distance, azimuth, elevation, roll], out_img)\n",
        "#         azimuth.change(render_pose_image,  [distance, azimuth, elevation, roll], out_img)\n",
        "#         elevation.change(render_pose_image,[distance, azimuth, elevation, roll], out_img)\n",
        "#         roll.change(render_pose_image,     [distance, azimuth, elevation, roll], out_img)\n",
        "\n",
        "#         # Initial render on load\n",
        "#         demo.load(render_pose_image, [distance, azimuth, elevation, roll], out_img)\n",
        "\n",
        "#         # Done -> close server\n",
        "#         # done_btn.click(on_done, outputs=status)\n",
        "\n",
        "\n",
        "#     app = demo.launch(inline=True, prevent_thread_lock=True, debug=False)\n",
        "\n",
        "#     return app\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3aMyMJ43G_Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio interface for selecting pose"
      ],
      "metadata": {
        "id": "8u0lbUf_GBrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Old) Not capturing values"
      ],
      "metadata": {
        "id": "j72LB8InzWpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def gradio_UI_select_pose(pose_estimator):\n",
        "#     import gradio as gr\n",
        "#     from PIL import Image\n",
        "#     import numpy as np\n",
        "#     import threading\n",
        "#     import torch\n",
        "\n",
        "#     # ---- helpers ------------------------------------------------------------\n",
        "#     def camera_center_to_dist_elev_azim(C: torch.Tensor):\n",
        "#         \"\"\"PyTorch3D conv: Y-up; azim=0 -> +Z; +azim toward +X.\"\"\"\n",
        "#         if C.ndim == 2 and C.shape[0] == 1:\n",
        "#             C = C[0]\n",
        "#         x, y, z = C[0], C[1], C[2]\n",
        "#         dist = torch.linalg.norm(C)\n",
        "#         rho  = torch.sqrt(torch.clamp(x*x + z*z, min=1e-12))\n",
        "#         elev = torch.rad2deg(torch.atan2(y, rho))   # [-90, 90]\n",
        "#         azim = torch.rad2deg(torch.atan2(x, z))     # (-180, 180]\n",
        "#         return float(dist.detach().cpu()), float(elev.detach().cpu()), float(azim.detach().cpu())\n",
        "\n",
        "#     def get_initial_slider_values_from_model(pose_estimator):\n",
        "#         \"\"\"Return (distance, elevation, azimuth, roll) as floats for slider init.\"\"\"\n",
        "#         C = pose_estimator.model.camera_position.detach()\n",
        "#         d, e, a = camera_center_to_dist_elev_azim(C)\n",
        "#         roll_param = getattr(pose_estimator.model, \"roll_deg\", None)\n",
        "#         r = float(roll_param.detach().cpu()) if roll_param is not None else 0.0\n",
        "#         # Clamp/wrap into slider ranges\n",
        "#         e = max(-89.0, min(89.0, e))\n",
        "#         a = ((a + 180.0) % 360.0) - 180.0\n",
        "#         r = ((r + 180.0) % 360.0) - 180.0\n",
        "#         return d, e, a, r\n",
        "\n",
        "#     # Keep the argument order consistent: (distance, elevation, azimuth, roll)\n",
        "#     def render_pose_image(distance, elevation, azimuth, roll):\n",
        "#         try:\n",
        "#             pose_estimator.init_model(\n",
        "#                 distance=distance,\n",
        "#                 elevation=elevation,\n",
        "#                 azimuth=azimuth,\n",
        "#                 roll_deg=float(roll),\n",
        "#                 learning_rate=0.0\n",
        "#             )\n",
        "#             overlay_img = pose_estimator.render_current_and_reference_as_overlay()\n",
        "#             if isinstance(overlay_img, np.ndarray):\n",
        "#                 if overlay_img.dtype in (np.float32, np.float64):\n",
        "#                     overlay_img = (overlay_img * 255).clip(0, 255).astype(np.uint8)\n",
        "#                 return Image.fromarray(overlay_img)\n",
        "#             return overlay_img\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error rendering overlay: {e}\")\n",
        "#             return None\n",
        "\n",
        "#     # Do both: set slider values and render once\n",
        "#     def init_and_render():\n",
        "#         d, e, a, r = get_initial_slider_values_from_model(pose_estimator)\n",
        "#         img = render_pose_image(d, e, a, r)\n",
        "#         return d, e, a, r, img\n",
        "\n",
        "#     app = None\n",
        "#     def on_done():\n",
        "#         def _close():\n",
        "#             try:\n",
        "#                 app.close()\n",
        "#             except Exception as e:\n",
        "#                 print(\"Close error:\", e)\n",
        "#         threading.Thread(target=_close, daemon=True).start()\n",
        "#         return \"âœ… Closing the interfaceâ€¦\"\n",
        "\n",
        "#     # ---- UI -----------------------------------------------------------------\n",
        "#     with gr.Blocks(title=\"CAD Pose Renderer\") as demo:\n",
        "#         gr.Markdown(\"Adjust the camera pose to render the CAD model. Click **Done** to stop the app.\")\n",
        "#         with gr.Row():\n",
        "#             distance  = gr.Slider(minimum=0.5,  maximum=20.0,  step=0.1, value=2.0, label=\"Distance\")\n",
        "#             elevation = gr.Slider(minimum=-89.0, maximum=89.0,  step=1,   value=0.0, label=\"Elevation\")\n",
        "#             azimuth   = gr.Slider(minimum=-180.0, maximum=180.0, step=1,   value=0.0, label=\"Azimuth\")\n",
        "#             roll      = gr.Slider(minimum=-180.0, maximum=180.0, step=1,   value=0.0, label=\"Roll\")\n",
        "#             # done_btn = gr.Button(\"âœ… Done\", variant=\"primary\")\n",
        "\n",
        "#         out_img = gr.Image(type=\"pil\", label=\"Rendered Image\")\n",
        "#         status  = gr.Markdown(\"\")\n",
        "\n",
        "#         # Initialize sliders and render immediately on load (single callback)\n",
        "#         demo.load(\n",
        "#             init_and_render,\n",
        "#             inputs=None,\n",
        "#             outputs=[distance, elevation, azimuth, roll, out_img]\n",
        "#         )\n",
        "\n",
        "#         # Live updates when sliders move (match arg order!)\n",
        "#         distance.change( render_pose_image, [distance, elevation, azimuth, roll], out_img)\n",
        "#         elevation.change(render_pose_image, [distance, elevation, azimuth, roll], out_img)\n",
        "#         azimuth.change(  render_pose_image, [distance, elevation, azimuth, roll], out_img)\n",
        "#         roll.change(     render_pose_image, [distance, elevation, azimuth, roll], out_img)\n",
        "\n",
        "#         # done_btn.click(on_done, outputs=status)\n",
        "\n",
        "#     app = demo.launch(inline=True, prevent_thread_lock=True, debug=False)\n",
        "#     return app\n"
      ],
      "metadata": {
        "id": "a0aycAcYZqp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (New) Capturing values"
      ],
      "metadata": {
        "id": "H1Rillfxzitv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_UI_select_pose(pose_estimator):\n",
        "    import gradio as gr\n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "    import threading\n",
        "    import torch\n",
        "    import json\n",
        "\n",
        "    # ---- helpers ------------------------------------------------------------\n",
        "    def camera_center_to_dist_elev_azim(C: torch.Tensor):\n",
        "        if C.ndim == 2 and C.shape[0] == 1:\n",
        "            C = C[0]\n",
        "        x, y, z = C[0], C[1], C[2]\n",
        "        dist = torch.linalg.norm(C)\n",
        "        rho  = torch.sqrt(torch.clamp(x*x + z*z, min=1e-12))\n",
        "        elev = torch.rad2deg(torch.atan2(y, rho))   # [-90, 90]\n",
        "        azim = torch.rad2deg(torch.atan2(x, z))     # (-180, 180]\n",
        "        return float(dist.detach().cpu()), float(elev.detach().cpu()), float(azim.detach().cpu())\n",
        "\n",
        "    def get_initial_slider_values_from_model(pose_estimator):\n",
        "        C = pose_estimator.model.camera_position.detach()\n",
        "        d, e, a = camera_center_to_dist_elev_azim(C)\n",
        "        roll_param = getattr(pose_estimator.model, \"roll_deg\", None)\n",
        "        r = float(roll_param.detach().cpu()) if roll_param is not None else 0.0\n",
        "        # Clamp/wrap into slider ranges\n",
        "        e = max(-89.0, min(89.0, e))\n",
        "        a = ((a + 180.0) % 360.0) - 180.0\n",
        "        r = ((r + 180.0) % 360.0) - 180.0\n",
        "        return d, e, a, r\n",
        "\n",
        "    # Keep arg order: (distance, elevation, azimuth, roll)\n",
        "    def render_pose_image(distance, elevation, azimuth, roll):\n",
        "        try:\n",
        "            pose_estimator.init_model(\n",
        "                distance=distance,\n",
        "                elevation=elevation,\n",
        "                azimuth=azimuth,\n",
        "                roll_deg=float(roll),\n",
        "                learning_rate=0.0\n",
        "            )\n",
        "            overlay_img = pose_estimator.render_current_and_reference_as_overlay()\n",
        "            if isinstance(overlay_img, np.ndarray):\n",
        "                if overlay_img.dtype in (np.float32, np.float64):\n",
        "                    overlay_img = (overlay_img * 255).clip(0, 255).astype(np.uint8)\n",
        "                return Image.fromarray(overlay_img)\n",
        "            return overlay_img\n",
        "        except Exception as e:\n",
        "            print(f\"Error rendering overlay: {e}\")\n",
        "            return None\n",
        "\n",
        "    # init on load: set sliders + render once\n",
        "    def init_and_render():\n",
        "        d, e, a, r = get_initial_slider_values_from_model(pose_estimator)\n",
        "        img = render_pose_image(d, e, a, r)\n",
        "        return d, e, a, r, img\n",
        "\n",
        "    app = None\n",
        "\n",
        "    # Save current values to notebook vars and close UI\n",
        "    def save_pose_and_close(distance, elevation, azimuth, roll):\n",
        "        vals = {\n",
        "            \"distance\": float(distance),\n",
        "            \"elevation\": float(elevation),\n",
        "            \"azimuth\": float(azimuth),\n",
        "            \"roll_deg\": float(roll),\n",
        "        }\n",
        "        # Store into the notebook's namespace\n",
        "        try:\n",
        "            ip = get_ipython()\n",
        "            if ip is not None:\n",
        "                ip.user_ns[\"selected_pose\"] = vals\n",
        "                ip.user_ns[\"selected_distance\"]  = vals[\"distance\"]\n",
        "                ip.user_ns[\"selected_elevation\"] = vals[\"elevation\"]\n",
        "                ip.user_ns[\"selected_azimuth\"]   = vals[\"azimuth\"]\n",
        "                ip.user_ns[\"selected_roll\"]      = vals[\"roll_deg\"]\n",
        "        except Exception as e:\n",
        "            print(\"Failed to save pose to notebook vars:\", e)\n",
        "\n",
        "        # Close the app\n",
        "        def _close():\n",
        "            try:\n",
        "                app.close()\n",
        "            except Exception as e:\n",
        "                print(\"Close error:\", e)\n",
        "        threading.Thread(target=_close, daemon=True).start()\n",
        "\n",
        "        return f\"âœ… Saved to variables: selected_pose (and individual scalars). Values: {json.dumps(vals)}\"\n",
        "\n",
        "    # ---- UI -----------------------------------------------------------------\n",
        "    with gr.Blocks(title=\"CAD Pose Renderer\") as demo:\n",
        "        gr.Markdown(\"Adjust the camera pose to render the CAD model, then click **Done** to save the values to the notebook.\")\n",
        "        with gr.Row():\n",
        "            distance  = gr.Slider(minimum=0.5,  maximum=20.0,  step=0.1, value=2.0, label=\"Distance\")\n",
        "            elevation = gr.Slider(minimum=-89.0, maximum=89.0,  step=1,   value=0.0, label=\"Elevation\")\n",
        "            azimuth   = gr.Slider(minimum=-180.0, maximum=180.0, step=1,   value=0.0, label=\"Azimuth\")\n",
        "            roll      = gr.Slider(minimum=-180.0, maximum=180.0, step=1,   value=0.0, label=\"Roll\")\n",
        "        out_img = gr.Image(type=\"pil\", label=\"Rendered Image\")\n",
        "        with gr.Row():\n",
        "            done_btn = gr.Button(\"âœ… Done\", variant=\"primary\")\n",
        "        status  = gr.Markdown(\"\")\n",
        "\n",
        "        # Initialize sliders and render immediately on load\n",
        "        demo.load(init_and_render, inputs=None, outputs=[distance, elevation, azimuth, roll, out_img])\n",
        "\n",
        "        # Live updates when sliders move (match arg order!)\n",
        "        distance.change( render_pose_image, [distance, elevation, azimuth, roll], out_img)\n",
        "        elevation.change(render_pose_image, [distance, elevation, azimuth, roll], out_img)\n",
        "        azimuth.change(  render_pose_image, [distance, elevation, azimuth, roll], out_img)\n",
        "        roll.change(     render_pose_image, [distance, elevation, azimuth, roll], out_img)\n",
        "\n",
        "        # Save + close\n",
        "        done_btn.click(save_pose_and_close, inputs=[distance, elevation, azimuth, roll], outputs=status)\n",
        "\n",
        "    app = demo.launch(inline=True, prevent_thread_lock=True, debug=False)\n",
        "    return app\n"
      ],
      "metadata": {
        "id": "t71B340tziG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download CAD files and textures"
      ],
      "metadata": {
        "id": "hxBLed_6M5m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare the utilities object\n",
        "util = SSE_Util(local_path)\n",
        "\n",
        "# Download cad models and texture\n",
        "util.get_cad_model_files()"
      ],
      "metadata": {
        "id": "uZApq_-uNK0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Test 1: Estimate pose using a synthetic silhouette\n",
        "\n",
        "In this example, we create an image of the  silhouette of the object to use as a reference image for the pose estimator."
      ],
      "metadata": {
        "id": "YCTEY3tI5W9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a pose estimator"
      ],
      "metadata": {
        "id": "EGszaVYSNzvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pose estimator\n",
        "pose_est = PoseEstimator(\n",
        "    local_path=local_path,\n",
        "    use_light_model = False,\n",
        "    image_size=(256, 256),\n",
        "    scale_factor=1,\n",
        "    num_iter=200\n",
        "    )"
      ],
      "metadata": {
        "id": "tgGdFJ-f5V9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the model to create a test reference image"
      ],
      "metadata": {
        "id": "Qrj_eM4XvPU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a test reference image\n",
        "#\n",
        "# elevation = [-89, 89] degrees.\n",
        "# azimuth   = [0,360] or [-180,180]\n",
        "#\n",
        "pose_est.create_test_reference_image(\n",
        "    distance=6.6,\n",
        "    elevation=0.0,\n",
        "    azimuth=0.0,\n",
        "    roll = 30.0\n",
        "    )\n",
        "\n",
        "# Display the reference image and its silhouette\n",
        "pose_est.display_test_reference_image()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SgozHl51GVoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize pose-estimation model\n",
        "pose_est.init_model(distance=6.6, elevation=0, azimuth=0, roll_deg=-30.0, learning_rate=0.15)\n",
        "\n",
        "\n",
        "overlay_img = pose_est.render_current_and_reference_as_overlay()\n",
        "\n",
        "print(overlay_img.shape)\n",
        "# Plot reference image mask\n",
        "plt.figure()\n",
        "plt.imshow(overlay_img, cmap='gray')\n",
        "plt.title(\"silhouette_ref (Generated from alpha) - hard silhouette\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "plI-vixO5OJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select an initial pose using the UI\n"
      ],
      "metadata": {
        "id": "aiAwJnDhzi8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = gradio_UI_select_pose(pose_est)   # launches inline, non-blocking\n",
        "# Close app programmatically\n",
        "# app.close()"
      ],
      "metadata": {
        "id": "N-RRpjGxHU0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_pose         # dict with keys: distance, elevation, azimuth, roll\n",
        "selected_distance\n",
        "selected_elevation\n",
        "selected_azimuth\n",
        "selected_roll\n",
        "\n",
        "print(selected_pose)\n",
        "pose_est.init_model(**selected_pose, learning_rate=0.05)"
      ],
      "metadata": {
        "id": "sPAhF5N_0H1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the selected pose as the initial pose of the optmization step"
      ],
      "metadata": {
        "id": "n_9ffRCkOhdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display an overlay with the current pose and reference mask\n",
        "pose_est.display_current_and_reference_as_overlay()"
      ],
      "metadata": {
        "id": "rSXDlJf4Qhjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the optimization and display the resulting pose"
      ],
      "metadata": {
        "id": "vurFSLdWQmn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of iterations\n",
        "pose_est.set_number_of_iterations(400)\n",
        "\n",
        "# Estimate pose\n",
        "R, T = pose_est.run_optimization(tol=0.001)\n",
        "\n",
        "# Display an overlay with the current pose and reference mask\n",
        "pose_est.display_current_and_reference_as_overlay()"
      ],
      "metadata": {
        "id": "dafmLYlZEG0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref_mask = pose_est.image_ref[..., :3]\n",
        "print(pose_est.image_ref.shape)\n",
        "print(pose_est.silhouette_ref.shape)\n",
        "\n",
        "m = pose_est.image_ref[:,:,:,0:3].squeeze()\n",
        "# m = 255 - m\n",
        "\n",
        "ref_mask = ref_mask.squeeze()\n",
        "# Plot reference image\n",
        "plt.figure()\n",
        "plt.imshow(m, cmap='gray')\n",
        "plt.plot(128, 128, 'o', markersize=8, color='red')\n",
        "plt.title(\"Reference Image\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4PZQ8j4nSgpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 2: Estimate pose using an actual sillhouette"
      ],
      "metadata": {
        "id": "Vo_nV9c8vWs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pose estimator\n",
        "pose_est_mask = PoseEstimator(\n",
        "    local_path=local_path,\n",
        "    use_light_model = False,\n",
        "    image_size=(256, 256),\n",
        "    scale_factor=1,\n",
        "    num_iter=200\n",
        "    )"
      ],
      "metadata": {
        "id": "-WB71HqEaoBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the reference image\n",
        "This is an actual mask from the segmented object."
      ],
      "metadata": {
        "id": "blFY7mxkR4iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = local_path + \"assets/00002.png\"\n",
        "image_path = local_path + \"assets/00006.png\"\n",
        "\n",
        "\n",
        "_ = pose_est_mask.read_rgb_cutout_black_bg_from_file(image_path, output_size=(256, 256))\n",
        "\n",
        "\n",
        "# Plot reference image\n",
        "plt.figure()\n",
        "plt.imshow(pose_est_mask.image_ref.squeeze(), cmap='gray')\n",
        "plt.title(\"cutout\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Plot reference image\n",
        "plt.figure()\n",
        "plt.imshow(pose_est_mask.image_ref.squeeze()[:,:,3], cmap='gray')\n",
        "plt.title(\"cutout\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w0oTpQ7wfCRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pose_est_mask.init_model(distance=10, elevation=110, azimuth=50, roll_deg=0.0, learning_rate=0.15)\n",
        "\n",
        "overlay_img = pose_est_mask.render_current_and_reference_as_overlay()\n",
        "\n",
        "print(overlay_img.shape)\n",
        "# Plot reference image mask\n",
        "plt.figure()\n",
        "plt.imshow(overlay_img, cmap='gray')\n",
        "plt.title(\"silhouette_ref (Generated from alpha) - hard silhouette\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uAMw_bJOJ2aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose the initial pose\n",
        "Use the UI to select the initial pose."
      ],
      "metadata": {
        "id": "NFR-zdY_4NUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set this position as the initial position for the optimization step.\n",
        "# Also set the learning rate.\n",
        "\n",
        "\n",
        "\n",
        "app = gradio_UI_select_pose(pose_est_mask)   # launches inline, non-blocking\n",
        "# Close app programmatically\n",
        "# app.close()"
      ],
      "metadata": {
        "id": "JaHpkmgcTDfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_pose         # dict with keys: distance, elevation, azimuth, roll\n",
        "selected_distance\n",
        "selected_elevation\n",
        "selected_azimuth\n",
        "selected_roll\n",
        "\n",
        "print(selected_pose)\n",
        "pose_est.init_model(**selected_pose, learning_rate=0.05)"
      ],
      "metadata": {
        "id": "xNbF6tGm1L_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the initial pose"
      ],
      "metadata": {
        "id": "1_VgzEUUTyET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the camera position\n",
        "camera_pos = pose_est_mask.model.camera_position.detach()\n",
        "camera_pos = camera_pos.unsqueeze(0)\n",
        "\n",
        "# Set this position as the initial position for the optimization step.\n",
        "# Also set the learning rate.\n",
        "pose_est_mask.init_model(learning_rate=0.05, eye_override=camera_pos)\n",
        "\n",
        "# Display an overlay with the current pose and reference mask\n",
        "pose_est_mask.display_current_and_reference_as_overlay()"
      ],
      "metadata": {
        "id": "duPtj4se4F6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the optimization and display the resulting pose\n"
      ],
      "metadata": {
        "id": "_9_V--biUISC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pose_est_mask.init_model(distance=8, elevation=15, azimuth=-28, roll_deg=25.0, learning_rate=0.15)\n",
        "\n",
        "\n",
        "# Set the number of iterations\n",
        "pose_est_mask.set_number_of_iterations(300)\n",
        "\n",
        "# Estimate pose\n",
        "R, T = pose_est_mask.run_optimization(tol=0.01)\n",
        "\n",
        "\n",
        "# Display an overlay with the current pose and reference mask\n",
        "pose_est_mask.display_current_and_reference_as_overlay()\n",
        "\n",
        "# Display the model with the current pose\n",
        "pose_est_mask.display_current_image()\n",
        "\n",
        "print(\"\\nCam position from model =\\n:\", pose_est_mask.model.camera_position)\n",
        "\n",
        "cam_pos = pose_est_mask.model.camera_position.detach()\n",
        "cam_pos = cam_pos.unsqueeze(0)\n",
        "\n",
        "print(\"\\nCam position from model numpy =\\n:\", cam_pos.cpu().numpy())\n"
      ],
      "metadata": {
        "id": "oKHFlh-bTs4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Test 3: Process a sequence of masks from a video"
      ],
      "metadata": {
        "id": "5kpwkalKFPA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create directory to hold video data"
      ],
      "metadata": {
        "id": "a90CD0YS5xfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing video: `sse_subset`\n",
        "Mostly in-plane translation across the image."
      ],
      "metadata": {
        "id": "m0wk5UmTzJKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# video_name = \"sse_subset\"\n",
        "# res = prepare_video_data(local_path, video_name, delete_zips=True, overwrite=True, verbose=True)\n",
        "# print(res)"
      ],
      "metadata": {
        "id": "CqiqCVPCmkQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing video: `test14_node_small`\n",
        "Spinning about the vertical direction\n"
      ],
      "metadata": {
        "id": "9DUmVkpNo5df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_name = \"test14_node_small\"\n",
        "res = prepare_video_data(local_path, video_name, delete_zips=True, overwrite=True, verbose=True)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "3G6GyaLcpEyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing video: `node2_turning`\n",
        "Some in-plane rotation\n"
      ],
      "metadata": {
        "id": "X6l_twMep0Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# video_name = \"node2_turning\"\n",
        "# res = prepare_video_data(local_path, video_name, delete_zips=True, overwrite=True, verbose=True)\n",
        "# print(res)"
      ],
      "metadata": {
        "id": "9hufmo5uUXLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make a list of all mask files"
      ],
      "metadata": {
        "id": "he2_eskdFK1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process all masks\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "\n",
        "# Directory with mask files\n",
        "mask_dir = \"video_data/cutouts/\"\n",
        "\n",
        "# Regex pattern for exactly 5-digit filenames ending in .png\n",
        "pattern = re.compile(r\"^\\d{5}\\.png$\")\n",
        "\n",
        "# Get all .png files\n",
        "all_files = glob.glob(os.path.join(mask_dir, \"*.png\"))\n",
        "\n",
        "# Filter using regex\n",
        "mask_files = sorted([\n",
        "    f for f in all_files if pattern.match(os.path.basename(f))\n",
        "])\n",
        "\n",
        "print(mask_files)"
      ],
      "metadata": {
        "id": "7Ke6NYC9FIsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display masks"
      ],
      "metadata": {
        "id": "0MorRNK5Fhlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, grid, names, masks = show_mask_grid(local_path=local_path, max_images=20, cols=5)\n",
        "print(\"Shown files:\", names[:5], \"...\")"
      ],
      "metadata": {
        "id": "tVtsW-hBtH36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display cutouts"
      ],
      "metadata": {
        "id": "-zNQLx4wo00p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, grid, names, imgs = show_cutout_grid(local_path=local_path, max_images=20, cols=5)\n",
        "print(\"Shown files:\", names[:5], \"...\")\n"
      ],
      "metadata": {
        "id": "yj9OcbZYt5sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the pose estimation on the mask sequence"
      ],
      "metadata": {
        "id": "TLFw33niF7Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize pose estimator"
      ],
      "metadata": {
        "id": "I9O6lBbuGhUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pose estimator\n",
        "pose_est01 = PoseEstimator(\n",
        "    local_path=local_path,\n",
        "    use_light_model = False,\n",
        "    image_size=(256, 256),\n",
        "    scale_factor=1,\n",
        "    num_iter=200\n",
        "    )"
      ],
      "metadata": {
        "id": "YyyJPvncGqJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set the initial pose for the first frame"
      ],
      "metadata": {
        "id": "StZ78Er0HAt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask to use to set the initial pose\n",
        "image_path = local_path + \"video_data/masks/00001.png\"\n",
        "image_path = local_path + \"video_data/cutouts/00001.png\"\n",
        "\n",
        "# pose_est01.set_reference_mask(image_path)\n",
        "\n",
        "\n",
        "_ = pose_est01.read_rgb_cutout_black_bg_from_file(image_path, output_size=(256, 256))\n",
        "\n",
        "# Plot reference image\n",
        "plt.figure()\n",
        "plt.imshow(pose_est01.image_ref.squeeze(), cmap='gray')\n",
        "plt.title(\"cutout\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Plot reference image\n",
        "plt.figure()\n",
        "plt.imshow(pose_est01.image_ref.squeeze()[:,:,3], cmap='gray')\n",
        "plt.title(\"mask\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "FXHj2-asHTqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GRADIO: Choose initial pose\n",
        "\n",
        "Use the UI to select the initial pose"
      ],
      "metadata": {
        "id": "C_78Zob8psr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = gradio_UI_select_pose(pose_est01)   # launches inline, non-blocking\n",
        "# Close app programmatically\n",
        "# app.close()"
      ],
      "metadata": {
        "id": "ditWfidg55Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_pose         # dict with keys: distance, elevation, azimuth, roll\n",
        "selected_distance\n",
        "selected_elevation\n",
        "selected_azimuth\n",
        "selected_roll\n",
        "\n",
        "print(selected_pose)\n",
        "pose_est01.init_model(**selected_pose, learning_rate=0.05)"
      ],
      "metadata": {
        "id": "rZjM4NDzfTid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pose_est01.init_model(distance=8, elevation=15, azimuth=-28, roll_deg=25.0, learning_rate=0.15)\n",
        "\n",
        "\n",
        "\n",
        "# Get the camera position\n",
        "camera_pos = pose_est01.model.camera_position.detach()\n",
        "camera_pos = camera_pos.unsqueeze(0)\n",
        "\n",
        "# Set this position as the initial position for the optimization step.\n",
        "# Also set the learning rate.\n",
        "pose_est01.init_model(learning_rate=0.05, eye_override=camera_pos)\n",
        "\n",
        "# Display an overlay with the current pose and reference mask\n",
        "pose_est01.display_current_and_reference_as_overlay()"
      ],
      "metadata": {
        "id": "BKjCvDdRfho8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit initial pose\n",
        "Estimate pose and display the overlay"
      ],
      "metadata": {
        "id": "872_RrylImZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pose_est01.set_number_of_iterations(400)\n",
        "\n",
        "R, T = pose_est01.run_optimization(tol=1)\n",
        "\n",
        "# Display an overlay with the current pose and reference mask\n",
        "pose_est01.display_current_and_reference_as_overlay()\n",
        "\n",
        "pose_est01.printout_results()"
      ],
      "metadata": {
        "id": "VJTovssIItws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, estimate pose for all images"
      ],
      "metadata": {
        "id": "brQOrJJcKRHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pose_est01.set_number_of_iterations(200)\n",
        "pose_est01.set_learning_rate(0.05)\n",
        "\n",
        "results = estimate_poses_for_masks(\n",
        "    pose_est=pose_est01,\n",
        "    mask_files=mask_files,\n",
        "    local_path=local_path,\n",
        "    N=None,                 # None or an int like 3\n",
        "    iterations=200,\n",
        "    learning_rate=0.05,\n",
        "    tol=1.0,\n",
        "    show_overlay=False,\n",
        "    print_results=False,\n",
        "    stop_on_error=False,\n",
        "    output_size = (256, 256)\n",
        ")\n",
        "\n",
        "# Access a result\n",
        "if results:\n",
        "    r0 = results[0]\n",
        "    print(\"First file:\", r0[\"file\"])\n",
        "    print(\"R shape:\", tuple(r0[\"R\"].shape), \"T shape:\", tuple(r0[\"T\"].shape))\n"
      ],
      "metadata": {
        "id": "1pKX82FMwxP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gif_path, n = make_pose_gif(pose_est01, results, out_path=\"./result.gif\", duration=0.5)\n",
        "print(gif_path, n)"
      ],
      "metadata": {
        "id": "ryi9TpTOzW-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anp_metadata": {
      "path": "fbsource/fbcode/vision/fair/pytorch3d/docs/tutorials/camera_position_optimization_with_differentiable_rendering.ipynb"
    },
    "bento_stylesheets": {
      "bento/extensions/flow/main.css": true,
      "bento/extensions/kernel_selector/main.css": true,
      "bento/extensions/kernel_ui/main.css": true,
      "bento/extensions/new_kernel/main.css": true,
      "bento/extensions/system_usage/main.css": true,
      "bento/extensions/theme/main.css": true
    },
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "disseminate_notebook_info": {
      "backup_notebook_id": "1062179640844868"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}